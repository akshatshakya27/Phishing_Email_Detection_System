{
    "epoch_logs": [
        {
            "loss": 0.7013,
            "grad_norm": 2.128396987915039,
            "learning_rate": 4.9977919095566153e-05,
            "epoch": 0.0013248542660307366,
            "step": 20
        },
        {
            "loss": 0.6816,
            "grad_norm": 2.4856324195861816,
            "learning_rate": 4.995583819113231e-05,
            "epoch": 0.002649708532061473,
            "step": 40
        },
        {
            "loss": 0.6733,
            "grad_norm": 1.9110475778579712,
            "learning_rate": 4.993375728669847e-05,
            "epoch": 0.00397456279809221,
            "step": 60
        },
        {
            "loss": 0.6812,
            "grad_norm": 2.1398138999938965,
            "learning_rate": 4.991167638226462e-05,
            "epoch": 0.005299417064122946,
            "step": 80
        },
        {
            "loss": 0.6712,
            "grad_norm": 2.644500494003296,
            "learning_rate": 4.988959547783078e-05,
            "epoch": 0.006624271330153683,
            "step": 100
        },
        {
            "loss": 0.6588,
            "grad_norm": 1.7231391668319702,
            "learning_rate": 4.986751457339693e-05,
            "epoch": 0.00794912559618442,
            "step": 120
        },
        {
            "loss": 0.6513,
            "grad_norm": 3.699573040008545,
            "learning_rate": 4.984543366896308e-05,
            "epoch": 0.009273979862215157,
            "step": 140
        },
        {
            "loss": 0.6213,
            "grad_norm": 3.4227583408355713,
            "learning_rate": 4.982335276452924e-05,
            "epoch": 0.010598834128245893,
            "step": 160
        },
        {
            "loss": 0.6157,
            "grad_norm": 4.88714075088501,
            "learning_rate": 4.9801271860095396e-05,
            "epoch": 0.01192368839427663,
            "step": 180
        },
        {
            "loss": 0.622,
            "grad_norm": 3.8525772094726562,
            "learning_rate": 4.977919095566155e-05,
            "epoch": 0.013248542660307366,
            "step": 200
        },
        {
            "loss": 0.5976,
            "grad_norm": 3.989912748336792,
            "learning_rate": 4.97571100512277e-05,
            "epoch": 0.014573396926338102,
            "step": 220
        },
        {
            "loss": 0.5938,
            "grad_norm": 4.420867443084717,
            "learning_rate": 4.9735029146793856e-05,
            "epoch": 0.01589825119236884,
            "step": 240
        },
        {
            "loss": 0.5655,
            "grad_norm": 3.3875627517700195,
            "learning_rate": 4.971294824236001e-05,
            "epoch": 0.017223105458399578,
            "step": 260
        },
        {
            "loss": 0.6296,
            "grad_norm": 3.7195703983306885,
            "learning_rate": 4.9690867337926165e-05,
            "epoch": 0.018547959724430314,
            "step": 280
        },
        {
            "loss": 0.5585,
            "grad_norm": 1.8329873085021973,
            "learning_rate": 4.966878643349232e-05,
            "epoch": 0.01987281399046105,
            "step": 300
        },
        {
            "loss": 0.5827,
            "grad_norm": 6.912115573883057,
            "learning_rate": 4.9646705529058473e-05,
            "epoch": 0.021197668256491786,
            "step": 320
        },
        {
            "loss": 0.5247,
            "grad_norm": 3.6940114498138428,
            "learning_rate": 4.9624624624624625e-05,
            "epoch": 0.02252252252252252,
            "step": 340
        },
        {
            "loss": 0.5252,
            "grad_norm": 8.104856491088867,
            "learning_rate": 4.960254372019078e-05,
            "epoch": 0.02384737678855326,
            "step": 360
        },
        {
            "loss": 0.5036,
            "grad_norm": 3.5613412857055664,
            "learning_rate": 4.9580462815756933e-05,
            "epoch": 0.025172231054583997,
            "step": 380
        },
        {
            "loss": 0.5189,
            "grad_norm": 2.157111167907715,
            "learning_rate": 4.955838191132309e-05,
            "epoch": 0.026497085320614733,
            "step": 400
        },
        {
            "loss": 0.5191,
            "grad_norm": 8.500993728637695,
            "learning_rate": 4.953630100688925e-05,
            "epoch": 0.02782193958664547,
            "step": 420
        },
        {
            "loss": 0.5187,
            "grad_norm": 4.484507083892822,
            "learning_rate": 4.95142201024554e-05,
            "epoch": 0.029146793852676205,
            "step": 440
        },
        {
            "loss": 0.5407,
            "grad_norm": 6.710977077484131,
            "learning_rate": 4.949213919802155e-05,
            "epoch": 0.03047164811870694,
            "step": 460
        },
        {
            "loss": 0.5125,
            "grad_norm": 3.2074227333068848,
            "learning_rate": 4.947005829358771e-05,
            "epoch": 0.03179650238473768,
            "step": 480
        },
        {
            "loss": 0.4964,
            "grad_norm": 6.03197717666626,
            "learning_rate": 4.944797738915386e-05,
            "epoch": 0.033121356650768416,
            "step": 500
        },
        {
            "loss": 0.5094,
            "grad_norm": 9.476265907287598,
            "learning_rate": 4.942589648472002e-05,
            "epoch": 0.034446210916799155,
            "step": 520
        },
        {
            "loss": 0.5374,
            "grad_norm": 5.538206100463867,
            "learning_rate": 4.9403815580286176e-05,
            "epoch": 0.03577106518282989,
            "step": 540
        },
        {
            "loss": 0.4944,
            "grad_norm": 7.718759059906006,
            "learning_rate": 4.938173467585233e-05,
            "epoch": 0.03709591944886063,
            "step": 560
        },
        {
            "loss": 0.4923,
            "grad_norm": 1.9609566926956177,
            "learning_rate": 4.935965377141848e-05,
            "epoch": 0.03842077371489136,
            "step": 580
        },
        {
            "loss": 0.5018,
            "grad_norm": 13.87255859375,
            "learning_rate": 4.9337572866984636e-05,
            "epoch": 0.0397456279809221,
            "step": 600
        },
        {
            "loss": 0.4329,
            "grad_norm": 9.254399299621582,
            "learning_rate": 4.931549196255079e-05,
            "epoch": 0.04107048224695284,
            "step": 620
        },
        {
            "loss": 0.4818,
            "grad_norm": 2.5485870838165283,
            "learning_rate": 4.9293411058116945e-05,
            "epoch": 0.04239533651298357,
            "step": 640
        },
        {
            "loss": 0.4811,
            "grad_norm": 10.786905288696289,
            "learning_rate": 4.9271330153683096e-05,
            "epoch": 0.04372019077901431,
            "step": 660
        },
        {
            "loss": 0.494,
            "grad_norm": 1.6231918334960938,
            "learning_rate": 4.9249249249249253e-05,
            "epoch": 0.04504504504504504,
            "step": 680
        },
        {
            "loss": 0.4278,
            "grad_norm": 4.106787204742432,
            "learning_rate": 4.9227168344815404e-05,
            "epoch": 0.04636989931107578,
            "step": 700
        },
        {
            "loss": 0.4697,
            "grad_norm": 2.5857934951782227,
            "learning_rate": 4.9205087440381556e-05,
            "epoch": 0.04769475357710652,
            "step": 720
        },
        {
            "loss": 0.5282,
            "grad_norm": 11.612424850463867,
            "learning_rate": 4.918300653594771e-05,
            "epoch": 0.049019607843137254,
            "step": 740
        },
        {
            "loss": 0.4432,
            "grad_norm": 18.118732452392578,
            "learning_rate": 4.916092563151387e-05,
            "epoch": 0.050344462109167994,
            "step": 760
        },
        {
            "loss": 0.4679,
            "grad_norm": 3.7416601181030273,
            "learning_rate": 4.913884472708002e-05,
            "epoch": 0.051669316375198726,
            "step": 780
        },
        {
            "loss": 0.485,
            "grad_norm": 1.9274448156356812,
            "learning_rate": 4.911676382264618e-05,
            "epoch": 0.052994170641229466,
            "step": 800
        },
        {
            "loss": 0.4416,
            "grad_norm": 3.077291965484619,
            "learning_rate": 4.909468291821233e-05,
            "epoch": 0.0543190249072602,
            "step": 820
        },
        {
            "loss": 0.4279,
            "grad_norm": 3.6535048484802246,
            "learning_rate": 4.907260201377848e-05,
            "epoch": 0.05564387917329094,
            "step": 840
        },
        {
            "loss": 0.4017,
            "grad_norm": 2.9167020320892334,
            "learning_rate": 4.905052110934465e-05,
            "epoch": 0.05696873343932168,
            "step": 860
        },
        {
            "loss": 0.4618,
            "grad_norm": 24.868976593017578,
            "learning_rate": 4.90284402049108e-05,
            "epoch": 0.05829358770535241,
            "step": 880
        },
        {
            "loss": 0.4113,
            "grad_norm": 2.8400683403015137,
            "learning_rate": 4.900635930047695e-05,
            "epoch": 0.05961844197138315,
            "step": 900
        },
        {
            "loss": 0.4633,
            "grad_norm": 9.009074211120605,
            "learning_rate": 4.898427839604311e-05,
            "epoch": 0.06094329623741388,
            "step": 920
        },
        {
            "loss": 0.5037,
            "grad_norm": 1.304991364479065,
            "learning_rate": 4.896219749160926e-05,
            "epoch": 0.06226815050344462,
            "step": 940
        },
        {
            "loss": 0.4999,
            "grad_norm": 4.283069133758545,
            "learning_rate": 4.894011658717541e-05,
            "epoch": 0.06359300476947535,
            "step": 960
        },
        {
            "loss": 0.421,
            "grad_norm": 2.6116199493408203,
            "learning_rate": 4.891803568274157e-05,
            "epoch": 0.06491785903550609,
            "step": 980
        },
        {
            "loss": 0.4437,
            "grad_norm": 1.305297613143921,
            "learning_rate": 4.8895954778307725e-05,
            "epoch": 0.06624271330153683,
            "step": 1000
        },
        {
            "loss": 0.4499,
            "grad_norm": 2.27736234664917,
            "learning_rate": 4.8873873873873876e-05,
            "epoch": 0.06756756756756757,
            "step": 1020
        },
        {
            "loss": 0.4926,
            "grad_norm": 4.570565223693848,
            "learning_rate": 4.8851792969440033e-05,
            "epoch": 0.06889242183359831,
            "step": 1040
        },
        {
            "loss": 0.3788,
            "grad_norm": 14.074797630310059,
            "learning_rate": 4.8829712065006184e-05,
            "epoch": 0.07021727609962904,
            "step": 1060
        },
        {
            "loss": 0.4152,
            "grad_norm": 1.7167153358459473,
            "learning_rate": 4.8807631160572336e-05,
            "epoch": 0.07154213036565978,
            "step": 1080
        },
        {
            "loss": 0.3996,
            "grad_norm": 11.753791809082031,
            "learning_rate": 4.878555025613849e-05,
            "epoch": 0.07286698463169052,
            "step": 1100
        },
        {
            "loss": 0.3754,
            "grad_norm": 1.8849835395812988,
            "learning_rate": 4.876346935170465e-05,
            "epoch": 0.07419183889772125,
            "step": 1120
        },
        {
            "loss": 0.4706,
            "grad_norm": 3.233734607696533,
            "learning_rate": 4.87413884472708e-05,
            "epoch": 0.075516693163752,
            "step": 1140
        },
        {
            "loss": 0.4205,
            "grad_norm": 39.44409942626953,
            "learning_rate": 4.871930754283695e-05,
            "epoch": 0.07684154742978272,
            "step": 1160
        },
        {
            "loss": 0.4346,
            "grad_norm": 2.9097940921783447,
            "learning_rate": 4.869722663840311e-05,
            "epoch": 0.07816640169581346,
            "step": 1180
        },
        {
            "loss": 0.5015,
            "grad_norm": 1.9934959411621094,
            "learning_rate": 4.867514573396927e-05,
            "epoch": 0.0794912559618442,
            "step": 1200
        },
        {
            "loss": 0.4655,
            "grad_norm": 2.0522677898406982,
            "learning_rate": 4.865306482953542e-05,
            "epoch": 0.08081611022787494,
            "step": 1220
        },
        {
            "loss": 0.4394,
            "grad_norm": 35.56560134887695,
            "learning_rate": 4.863098392510158e-05,
            "epoch": 0.08214096449390568,
            "step": 1240
        },
        {
            "loss": 0.3617,
            "grad_norm": 3.744788646697998,
            "learning_rate": 4.860890302066773e-05,
            "epoch": 0.0834658187599364,
            "step": 1260
        },
        {
            "loss": 0.5234,
            "grad_norm": 1.9592771530151367,
            "learning_rate": 4.858682211623388e-05,
            "epoch": 0.08479067302596714,
            "step": 1280
        },
        {
            "loss": 0.4101,
            "grad_norm": 1.4845781326293945,
            "learning_rate": 4.856474121180004e-05,
            "epoch": 0.08611552729199788,
            "step": 1300
        },
        {
            "loss": 0.4568,
            "grad_norm": 1.1031898260116577,
            "learning_rate": 4.8542660307366196e-05,
            "epoch": 0.08744038155802862,
            "step": 1320
        },
        {
            "loss": 0.391,
            "grad_norm": 7.627500057220459,
            "learning_rate": 4.852057940293235e-05,
            "epoch": 0.08876523582405936,
            "step": 1340
        },
        {
            "loss": 0.4304,
            "grad_norm": 4.28452730178833,
            "learning_rate": 4.8498498498498504e-05,
            "epoch": 0.09009009009009009,
            "step": 1360
        },
        {
            "loss": 0.4542,
            "grad_norm": 1.907442569732666,
            "learning_rate": 4.8476417594064656e-05,
            "epoch": 0.09141494435612083,
            "step": 1380
        },
        {
            "loss": 0.4909,
            "grad_norm": 2.1938257217407227,
            "learning_rate": 4.8454336689630807e-05,
            "epoch": 0.09273979862215156,
            "step": 1400
        },
        {
            "loss": 0.4775,
            "grad_norm": 1.543732762336731,
            "learning_rate": 4.8432255785196964e-05,
            "epoch": 0.0940646528881823,
            "step": 1420
        },
        {
            "loss": 0.4721,
            "grad_norm": 26.493993759155273,
            "learning_rate": 4.841017488076312e-05,
            "epoch": 0.09538950715421304,
            "step": 1440
        },
        {
            "loss": 0.4795,
            "grad_norm": 1.6335734128952026,
            "learning_rate": 4.838809397632927e-05,
            "epoch": 0.09671436142024377,
            "step": 1460
        },
        {
            "loss": 0.4358,
            "grad_norm": 3.7441437244415283,
            "learning_rate": 4.8366013071895424e-05,
            "epoch": 0.09803921568627451,
            "step": 1480
        },
        {
            "loss": 0.425,
            "grad_norm": 3.057894229888916,
            "learning_rate": 4.834393216746158e-05,
            "epoch": 0.09936406995230525,
            "step": 1500
        },
        {
            "loss": 0.4679,
            "grad_norm": 12.225703239440918,
            "learning_rate": 4.832185126302773e-05,
            "epoch": 0.10068892421833599,
            "step": 1520
        },
        {
            "loss": 0.4419,
            "grad_norm": 42.47072219848633,
            "learning_rate": 4.8299770358593884e-05,
            "epoch": 0.10201377848436671,
            "step": 1540
        },
        {
            "loss": 0.531,
            "grad_norm": 1.52092444896698,
            "learning_rate": 4.827768945416005e-05,
            "epoch": 0.10333863275039745,
            "step": 1560
        },
        {
            "loss": 0.4244,
            "grad_norm": 1.7932629585266113,
            "learning_rate": 4.82556085497262e-05,
            "epoch": 0.10466348701642819,
            "step": 1580
        },
        {
            "loss": 0.4339,
            "grad_norm": 0.7897335886955261,
            "learning_rate": 4.823352764529235e-05,
            "epoch": 0.10598834128245893,
            "step": 1600
        },
        {
            "loss": 0.4884,
            "grad_norm": 3.5560638904571533,
            "learning_rate": 4.821144674085851e-05,
            "epoch": 0.10731319554848967,
            "step": 1620
        },
        {
            "loss": 0.4282,
            "grad_norm": 1.3223652839660645,
            "learning_rate": 4.818936583642466e-05,
            "epoch": 0.1086380498145204,
            "step": 1640
        },
        {
            "loss": 0.4733,
            "grad_norm": 1.444156527519226,
            "learning_rate": 4.816728493199082e-05,
            "epoch": 0.10996290408055114,
            "step": 1660
        },
        {
            "loss": 0.3934,
            "grad_norm": 15.795255661010742,
            "learning_rate": 4.8145204027556976e-05,
            "epoch": 0.11128775834658187,
            "step": 1680
        },
        {
            "loss": 0.4441,
            "grad_norm": 0.9428908228874207,
            "learning_rate": 4.812312312312313e-05,
            "epoch": 0.11261261261261261,
            "step": 1700
        },
        {
            "loss": 0.3673,
            "grad_norm": 1.1074798107147217,
            "learning_rate": 4.810104221868928e-05,
            "epoch": 0.11393746687864335,
            "step": 1720
        },
        {
            "loss": 0.4371,
            "grad_norm": 3.448604106903076,
            "learning_rate": 4.8078961314255436e-05,
            "epoch": 0.11526232114467408,
            "step": 1740
        },
        {
            "loss": 0.4664,
            "grad_norm": 2.3988037109375,
            "learning_rate": 4.8056880409821587e-05,
            "epoch": 0.11658717541070482,
            "step": 1760
        },
        {
            "loss": 0.4263,
            "grad_norm": 3.484619617462158,
            "learning_rate": 4.8034799505387744e-05,
            "epoch": 0.11791202967673556,
            "step": 1780
        },
        {
            "loss": 0.4657,
            "grad_norm": 5.566167831420898,
            "learning_rate": 4.8012718600953895e-05,
            "epoch": 0.1192368839427663,
            "step": 1800
        },
        {
            "loss": 0.4082,
            "grad_norm": 1.2931387424468994,
            "learning_rate": 4.799063769652005e-05,
            "epoch": 0.12056173820879704,
            "step": 1820
        },
        {
            "loss": 0.4845,
            "grad_norm": 4.6374688148498535,
            "learning_rate": 4.7968556792086204e-05,
            "epoch": 0.12188659247482776,
            "step": 1840
        },
        {
            "loss": 0.4542,
            "grad_norm": 24.729524612426758,
            "learning_rate": 4.794647588765236e-05,
            "epoch": 0.1232114467408585,
            "step": 1860
        },
        {
            "loss": 0.4389,
            "grad_norm": 0.9551827907562256,
            "learning_rate": 4.792439498321851e-05,
            "epoch": 0.12453630100688924,
            "step": 1880
        },
        {
            "loss": 0.5157,
            "grad_norm": 3.326871156692505,
            "learning_rate": 4.790231407878467e-05,
            "epoch": 0.12586115527291997,
            "step": 1900
        },
        {
            "loss": 0.4619,
            "grad_norm": 16.431838989257812,
            "learning_rate": 4.788023317435082e-05,
            "epoch": 0.1271860095389507,
            "step": 1920
        },
        {
            "loss": 0.4026,
            "grad_norm": 1.6370151042938232,
            "learning_rate": 4.785815226991698e-05,
            "epoch": 0.12851086380498145,
            "step": 1940
        },
        {
            "loss": 0.4458,
            "grad_norm": 2.4571518898010254,
            "learning_rate": 4.783607136548313e-05,
            "epoch": 0.12983571807101218,
            "step": 1960
        },
        {
            "loss": 0.4057,
            "grad_norm": 34.31187438964844,
            "learning_rate": 4.781399046104928e-05,
            "epoch": 0.13116057233704292,
            "step": 1980
        },
        {
            "loss": 0.4599,
            "grad_norm": 1.2778828144073486,
            "learning_rate": 4.779190955661545e-05,
            "epoch": 0.13248542660307366,
            "step": 2000
        },
        {
            "loss": 0.4671,
            "grad_norm": 2.9451112747192383,
            "learning_rate": 4.77698286521816e-05,
            "epoch": 0.1338102808691044,
            "step": 2020
        },
        {
            "loss": 0.3671,
            "grad_norm": 1.417762041091919,
            "learning_rate": 4.774774774774775e-05,
            "epoch": 0.13513513513513514,
            "step": 2040
        },
        {
            "loss": 0.4749,
            "grad_norm": 1.1866915225982666,
            "learning_rate": 4.7725666843313907e-05,
            "epoch": 0.13645998940116588,
            "step": 2060
        },
        {
            "loss": 0.4187,
            "grad_norm": 3.480889320373535,
            "learning_rate": 4.770358593888006e-05,
            "epoch": 0.13778484366719662,
            "step": 2080
        },
        {
            "loss": 0.4461,
            "grad_norm": 1.7887617349624634,
            "learning_rate": 4.768150503444621e-05,
            "epoch": 0.13910969793322733,
            "step": 2100
        },
        {
            "loss": 0.4094,
            "grad_norm": 6.444519519805908,
            "learning_rate": 4.765942413001237e-05,
            "epoch": 0.14043455219925807,
            "step": 2120
        },
        {
            "loss": 0.5001,
            "grad_norm": 3.050095796585083,
            "learning_rate": 4.7637343225578524e-05,
            "epoch": 0.1417594064652888,
            "step": 2140
        },
        {
            "loss": 0.4842,
            "grad_norm": 1.1293245553970337,
            "learning_rate": 4.7615262321144675e-05,
            "epoch": 0.14308426073131955,
            "step": 2160
        },
        {
            "loss": 0.5326,
            "grad_norm": 2.3847267627716064,
            "learning_rate": 4.759318141671083e-05,
            "epoch": 0.1444091149973503,
            "step": 2180
        },
        {
            "loss": 0.4296,
            "grad_norm": 2.9781618118286133,
            "learning_rate": 4.7571100512276984e-05,
            "epoch": 0.14573396926338103,
            "step": 2200
        },
        {
            "loss": 0.4569,
            "grad_norm": 2.283176898956299,
            "learning_rate": 4.7549019607843135e-05,
            "epoch": 0.14705882352941177,
            "step": 2220
        },
        {
            "loss": 0.4425,
            "grad_norm": 1.9315237998962402,
            "learning_rate": 4.752693870340929e-05,
            "epoch": 0.1483836777954425,
            "step": 2240
        },
        {
            "loss": 0.4419,
            "grad_norm": 1.355686068534851,
            "learning_rate": 4.750485779897545e-05,
            "epoch": 0.14970853206147325,
            "step": 2260
        },
        {
            "loss": 0.4635,
            "grad_norm": 2.6858811378479004,
            "learning_rate": 4.74827768945416e-05,
            "epoch": 0.151033386327504,
            "step": 2280
        },
        {
            "loss": 0.4664,
            "grad_norm": 2.2978293895721436,
            "learning_rate": 4.746069599010775e-05,
            "epoch": 0.1523582405935347,
            "step": 2300
        },
        {
            "loss": 0.4601,
            "grad_norm": 0.9931445717811584,
            "learning_rate": 4.743861508567391e-05,
            "epoch": 0.15368309485956544,
            "step": 2320
        },
        {
            "loss": 0.3992,
            "grad_norm": 2.08410382270813,
            "learning_rate": 4.741653418124006e-05,
            "epoch": 0.15500794912559618,
            "step": 2340
        },
        {
            "loss": 0.4333,
            "grad_norm": 15.609195709228516,
            "learning_rate": 4.739445327680622e-05,
            "epoch": 0.15633280339162692,
            "step": 2360
        },
        {
            "loss": 0.441,
            "grad_norm": 3.894503593444824,
            "learning_rate": 4.737237237237238e-05,
            "epoch": 0.15765765765765766,
            "step": 2380
        },
        {
            "loss": 0.4728,
            "grad_norm": 1.9440068006515503,
            "learning_rate": 4.735029146793853e-05,
            "epoch": 0.1589825119236884,
            "step": 2400
        },
        {
            "loss": 0.5018,
            "grad_norm": 2.1364729404449463,
            "learning_rate": 4.732821056350468e-05,
            "epoch": 0.16030736618971914,
            "step": 2420
        },
        {
            "loss": 0.4263,
            "grad_norm": 12.868901252746582,
            "learning_rate": 4.730612965907084e-05,
            "epoch": 0.16163222045574988,
            "step": 2440
        },
        {
            "loss": 0.4279,
            "grad_norm": 5.03653621673584,
            "learning_rate": 4.7284048754636995e-05,
            "epoch": 0.16295707472178061,
            "step": 2460
        },
        {
            "loss": 0.4258,
            "grad_norm": 2.2897682189941406,
            "learning_rate": 4.7261967850203146e-05,
            "epoch": 0.16428192898781135,
            "step": 2480
        },
        {
            "loss": 0.3958,
            "grad_norm": 1.7236331701278687,
            "learning_rate": 4.7239886945769304e-05,
            "epoch": 0.16560678325384207,
            "step": 2500
        },
        {
            "loss": 0.4302,
            "grad_norm": 1.7441012859344482,
            "learning_rate": 4.7217806041335455e-05,
            "epoch": 0.1669316375198728,
            "step": 2520
        },
        {
            "loss": 0.4223,
            "grad_norm": 1.7987626791000366,
            "learning_rate": 4.7195725136901606e-05,
            "epoch": 0.16825649178590354,
            "step": 2540
        },
        {
            "loss": 0.4398,
            "grad_norm": 2.756711721420288,
            "learning_rate": 4.7173644232467764e-05,
            "epoch": 0.16958134605193428,
            "step": 2560
        },
        {
            "loss": 0.4678,
            "grad_norm": 1.281987190246582,
            "learning_rate": 4.715156332803392e-05,
            "epoch": 0.17090620031796502,
            "step": 2580
        },
        {
            "loss": 0.3865,
            "grad_norm": 1.1055748462677002,
            "learning_rate": 4.712948242360007e-05,
            "epoch": 0.17223105458399576,
            "step": 2600
        },
        {
            "loss": 0.3647,
            "grad_norm": 1.6524354219436646,
            "learning_rate": 4.710740151916623e-05,
            "epoch": 0.1735559088500265,
            "step": 2620
        },
        {
            "loss": 0.402,
            "grad_norm": 8.558682441711426,
            "learning_rate": 4.708532061473238e-05,
            "epoch": 0.17488076311605724,
            "step": 2640
        },
        {
            "loss": 0.4461,
            "grad_norm": 3.5466790199279785,
            "learning_rate": 4.706323971029853e-05,
            "epoch": 0.17620561738208798,
            "step": 2660
        },
        {
            "loss": 0.4384,
            "grad_norm": 2.259577512741089,
            "learning_rate": 4.704115880586469e-05,
            "epoch": 0.17753047164811872,
            "step": 2680
        },
        {
            "loss": 0.4203,
            "grad_norm": 1.716385006904602,
            "learning_rate": 4.701907790143085e-05,
            "epoch": 0.17885532591414943,
            "step": 2700
        },
        {
            "loss": 0.4713,
            "grad_norm": 1.494203805923462,
            "learning_rate": 4.6996996996997e-05,
            "epoch": 0.18018018018018017,
            "step": 2720
        },
        {
            "loss": 0.4232,
            "grad_norm": 3.751749277114868,
            "learning_rate": 4.697491609256315e-05,
            "epoch": 0.1815050344462109,
            "step": 2740
        },
        {
            "loss": 0.4018,
            "grad_norm": 1.9401992559432983,
            "learning_rate": 4.695283518812931e-05,
            "epoch": 0.18282988871224165,
            "step": 2760
        },
        {
            "loss": 0.4039,
            "grad_norm": 2.1774864196777344,
            "learning_rate": 4.693075428369546e-05,
            "epoch": 0.1841547429782724,
            "step": 2780
        },
        {
            "loss": 0.4279,
            "grad_norm": 1.8189834356307983,
            "learning_rate": 4.690867337926162e-05,
            "epoch": 0.18547959724430313,
            "step": 2800
        },
        {
            "loss": 0.4625,
            "grad_norm": 2.7457401752471924,
            "learning_rate": 4.6886592474827775e-05,
            "epoch": 0.18680445151033387,
            "step": 2820
        },
        {
            "loss": 0.4214,
            "grad_norm": 0.8017414808273315,
            "learning_rate": 4.6864511570393926e-05,
            "epoch": 0.1881293057763646,
            "step": 2840
        },
        {
            "loss": 0.511,
            "grad_norm": 2.581803798675537,
            "learning_rate": 4.684243066596008e-05,
            "epoch": 0.18945416004239535,
            "step": 2860
        },
        {
            "loss": 0.4536,
            "grad_norm": 1.1453619003295898,
            "learning_rate": 4.6820349761526235e-05,
            "epoch": 0.1907790143084261,
            "step": 2880
        },
        {
            "loss": 0.4837,
            "grad_norm": 3.782867431640625,
            "learning_rate": 4.6798268857092386e-05,
            "epoch": 0.1921038685744568,
            "step": 2900
        },
        {
            "loss": 0.3973,
            "grad_norm": 13.438352584838867,
            "learning_rate": 4.6776187952658544e-05,
            "epoch": 0.19342872284048754,
            "step": 2920
        },
        {
            "loss": 0.3866,
            "grad_norm": 0.5802314877510071,
            "learning_rate": 4.67541070482247e-05,
            "epoch": 0.19475357710651828,
            "step": 2940
        },
        {
            "loss": 0.3818,
            "grad_norm": 1.570680856704712,
            "learning_rate": 4.673202614379085e-05,
            "epoch": 0.19607843137254902,
            "step": 2960
        },
        {
            "loss": 0.4008,
            "grad_norm": 7.0524749755859375,
            "learning_rate": 4.6709945239357004e-05,
            "epoch": 0.19740328563857976,
            "step": 2980
        },
        {
            "loss": 0.4119,
            "grad_norm": 0.7472777962684631,
            "learning_rate": 4.668786433492316e-05,
            "epoch": 0.1987281399046105,
            "step": 3000
        },
        {
            "loss": 0.3961,
            "grad_norm": 3.9597067832946777,
            "learning_rate": 4.666578343048931e-05,
            "epoch": 0.20005299417064124,
            "step": 3020
        },
        {
            "loss": 0.4577,
            "grad_norm": 44.76860809326172,
            "learning_rate": 4.664370252605547e-05,
            "epoch": 0.20137784843667197,
            "step": 3040
        },
        {
            "loss": 0.4335,
            "grad_norm": 85.44378662109375,
            "learning_rate": 4.662162162162162e-05,
            "epoch": 0.20270270270270271,
            "step": 3060
        },
        {
            "loss": 0.3917,
            "grad_norm": 2.211822271347046,
            "learning_rate": 4.659954071718778e-05,
            "epoch": 0.20402755696873343,
            "step": 3080
        },
        {
            "loss": 0.3987,
            "grad_norm": 2.439276695251465,
            "learning_rate": 4.657745981275393e-05,
            "epoch": 0.20535241123476417,
            "step": 3100
        },
        {
            "loss": 0.4483,
            "grad_norm": 1.25393807888031,
            "learning_rate": 4.655537890832009e-05,
            "epoch": 0.2066772655007949,
            "step": 3120
        },
        {
            "loss": 0.3461,
            "grad_norm": 0.7329910397529602,
            "learning_rate": 4.653329800388624e-05,
            "epoch": 0.20800211976682564,
            "step": 3140
        },
        {
            "loss": 0.4203,
            "grad_norm": 1.1399548053741455,
            "learning_rate": 4.65112170994524e-05,
            "epoch": 0.20932697403285638,
            "step": 3160
        },
        {
            "loss": 0.5034,
            "grad_norm": 1.0414350032806396,
            "learning_rate": 4.648913619501855e-05,
            "epoch": 0.21065182829888712,
            "step": 3180
        },
        {
            "loss": 0.386,
            "grad_norm": 3.0601913928985596,
            "learning_rate": 4.6467055290584706e-05,
            "epoch": 0.21197668256491786,
            "step": 3200
        },
        {
            "loss": 0.5201,
            "grad_norm": 1.0781826972961426,
            "learning_rate": 4.644497438615086e-05,
            "epoch": 0.2133015368309486,
            "step": 3220
        },
        {
            "loss": 0.4048,
            "grad_norm": 0.9704338908195496,
            "learning_rate": 4.642289348171701e-05,
            "epoch": 0.21462639109697934,
            "step": 3240
        },
        {
            "loss": 0.3578,
            "grad_norm": 2.1346757411956787,
            "learning_rate": 4.640081257728317e-05,
            "epoch": 0.21595124536301008,
            "step": 3260
        },
        {
            "loss": 0.4687,
            "grad_norm": 2.725362539291382,
            "learning_rate": 4.6378731672849324e-05,
            "epoch": 0.2172760996290408,
            "step": 3280
        },
        {
            "loss": 0.4554,
            "grad_norm": 3.3987536430358887,
            "learning_rate": 4.6356650768415475e-05,
            "epoch": 0.21860095389507153,
            "step": 3300
        },
        {
            "loss": 0.5428,
            "grad_norm": 6.888264179229736,
            "learning_rate": 4.633456986398163e-05,
            "epoch": 0.21992580816110227,
            "step": 3320
        },
        {
            "loss": 0.4292,
            "grad_norm": 0.8980467319488525,
            "learning_rate": 4.6312488959547784e-05,
            "epoch": 0.221250662427133,
            "step": 3340
        },
        {
            "loss": 0.3807,
            "grad_norm": 1.2932900190353394,
            "learning_rate": 4.6290408055113935e-05,
            "epoch": 0.22257551669316375,
            "step": 3360
        },
        {
            "loss": 0.3547,
            "grad_norm": 1.452924132347107,
            "learning_rate": 4.626832715068009e-05,
            "epoch": 0.2239003709591945,
            "step": 3380
        },
        {
            "loss": 0.3926,
            "grad_norm": 8.19007682800293,
            "learning_rate": 4.624624624624625e-05,
            "epoch": 0.22522522522522523,
            "step": 3400
        },
        {
            "loss": 0.4095,
            "grad_norm": 1.1188054084777832,
            "learning_rate": 4.62241653418124e-05,
            "epoch": 0.22655007949125597,
            "step": 3420
        },
        {
            "loss": 0.4738,
            "grad_norm": 1.1118574142456055,
            "learning_rate": 4.620208443737856e-05,
            "epoch": 0.2278749337572867,
            "step": 3440
        },
        {
            "loss": 0.4438,
            "grad_norm": 26.919422149658203,
            "learning_rate": 4.618000353294471e-05,
            "epoch": 0.22919978802331745,
            "step": 3460
        },
        {
            "loss": 0.3899,
            "grad_norm": 1.0655289888381958,
            "learning_rate": 4.615792262851086e-05,
            "epoch": 0.23052464228934816,
            "step": 3480
        },
        {
            "loss": 0.4611,
            "grad_norm": 2.800042152404785,
            "learning_rate": 4.613584172407702e-05,
            "epoch": 0.2318494965553789,
            "step": 3500
        },
        {
            "loss": 0.4477,
            "grad_norm": 0.9516223073005676,
            "learning_rate": 4.611376081964318e-05,
            "epoch": 0.23317435082140964,
            "step": 3520
        },
        {
            "loss": 0.3931,
            "grad_norm": 1.0590416193008423,
            "learning_rate": 4.609167991520933e-05,
            "epoch": 0.23449920508744038,
            "step": 3540
        },
        {
            "loss": 0.491,
            "grad_norm": 1.3616946935653687,
            "learning_rate": 4.606959901077548e-05,
            "epoch": 0.23582405935347112,
            "step": 3560
        },
        {
            "loss": 0.43,
            "grad_norm": 10.900555610656738,
            "learning_rate": 4.604751810634164e-05,
            "epoch": 0.23714891361950186,
            "step": 3580
        },
        {
            "loss": 0.3587,
            "grad_norm": 2.0379514694213867,
            "learning_rate": 4.6025437201907795e-05,
            "epoch": 0.2384737678855326,
            "step": 3600
        },
        {
            "loss": 0.4269,
            "grad_norm": 0.9986679553985596,
            "learning_rate": 4.6003356297473946e-05,
            "epoch": 0.23979862215156333,
            "step": 3620
        },
        {
            "loss": 0.4037,
            "grad_norm": 0.8426417112350464,
            "learning_rate": 4.5981275393040104e-05,
            "epoch": 0.24112347641759407,
            "step": 3640
        },
        {
            "loss": 0.3943,
            "grad_norm": 2.781930446624756,
            "learning_rate": 4.5959194488606255e-05,
            "epoch": 0.2424483306836248,
            "step": 3660
        },
        {
            "loss": 0.3746,
            "grad_norm": 1.2037595510482788,
            "learning_rate": 4.5937113584172406e-05,
            "epoch": 0.24377318494965552,
            "step": 3680
        },
        {
            "loss": 0.3342,
            "grad_norm": 3.245483875274658,
            "learning_rate": 4.5915032679738564e-05,
            "epoch": 0.24509803921568626,
            "step": 3700
        },
        {
            "loss": 0.5097,
            "grad_norm": 4.993105888366699,
            "learning_rate": 4.589295177530472e-05,
            "epoch": 0.246422893481717,
            "step": 3720
        },
        {
            "loss": 0.4264,
            "grad_norm": 1.4127990007400513,
            "learning_rate": 4.587087087087087e-05,
            "epoch": 0.24774774774774774,
            "step": 3740
        },
        {
            "loss": 0.4102,
            "grad_norm": 1.207720398902893,
            "learning_rate": 4.584878996643703e-05,
            "epoch": 0.24907260201377848,
            "step": 3760
        },
        {
            "loss": 0.4498,
            "grad_norm": 36.39041519165039,
            "learning_rate": 4.582670906200318e-05,
            "epoch": 0.2503974562798092,
            "step": 3780
        },
        {
            "loss": 0.4038,
            "grad_norm": 3.929914951324463,
            "learning_rate": 4.580462815756933e-05,
            "epoch": 0.25172231054583993,
            "step": 3800
        },
        {
            "loss": 0.4355,
            "grad_norm": 1.772034764289856,
            "learning_rate": 4.578254725313549e-05,
            "epoch": 0.2530471648118707,
            "step": 3820
        },
        {
            "loss": 0.3746,
            "grad_norm": 0.8330400586128235,
            "learning_rate": 4.576046634870165e-05,
            "epoch": 0.2543720190779014,
            "step": 3840
        },
        {
            "loss": 0.4739,
            "grad_norm": 2.5523717403411865,
            "learning_rate": 4.57383854442678e-05,
            "epoch": 0.25569687334393215,
            "step": 3860
        },
        {
            "loss": 0.4649,
            "grad_norm": 6.078444004058838,
            "learning_rate": 4.571630453983395e-05,
            "epoch": 0.2570217276099629,
            "step": 3880
        },
        {
            "loss": 0.391,
            "grad_norm": 1.266847848892212,
            "learning_rate": 4.569422363540011e-05,
            "epoch": 0.25834658187599363,
            "step": 3900
        },
        {
            "loss": 0.419,
            "grad_norm": 0.4973149597644806,
            "learning_rate": 4.567214273096626e-05,
            "epoch": 0.25967143614202437,
            "step": 3920
        },
        {
            "loss": 0.4234,
            "grad_norm": 2.478386163711548,
            "learning_rate": 4.565006182653242e-05,
            "epoch": 0.2609962904080551,
            "step": 3940
        },
        {
            "loss": 0.3997,
            "grad_norm": 2.1593995094299316,
            "learning_rate": 4.5627980922098575e-05,
            "epoch": 0.26232114467408585,
            "step": 3960
        },
        {
            "loss": 0.402,
            "grad_norm": 1.2277923822402954,
            "learning_rate": 4.5605900017664726e-05,
            "epoch": 0.2636459989401166,
            "step": 3980
        },
        {
            "loss": 0.4036,
            "grad_norm": 0.8898026943206787,
            "learning_rate": 4.558381911323088e-05,
            "epoch": 0.2649708532061473,
            "step": 4000
        },
        {
            "loss": 0.4469,
            "grad_norm": 2.37675142288208,
            "learning_rate": 4.5561738208797035e-05,
            "epoch": 0.26629570747217807,
            "step": 4020
        },
        {
            "loss": 0.4122,
            "grad_norm": 1.3274767398834229,
            "learning_rate": 4.5539657304363186e-05,
            "epoch": 0.2676205617382088,
            "step": 4040
        },
        {
            "loss": 0.4939,
            "grad_norm": 1.602898359298706,
            "learning_rate": 4.5517576399929344e-05,
            "epoch": 0.26894541600423955,
            "step": 4060
        },
        {
            "loss": 0.422,
            "grad_norm": 0.9447275996208191,
            "learning_rate": 4.54954954954955e-05,
            "epoch": 0.2702702702702703,
            "step": 4080
        },
        {
            "loss": 0.4557,
            "grad_norm": 0.9090535640716553,
            "learning_rate": 4.547341459106165e-05,
            "epoch": 0.271595124536301,
            "step": 4100
        },
        {
            "loss": 0.428,
            "grad_norm": 23.23253059387207,
            "learning_rate": 4.5451333686627804e-05,
            "epoch": 0.27291997880233176,
            "step": 4120
        },
        {
            "loss": 0.436,
            "grad_norm": 1.33395254611969,
            "learning_rate": 4.542925278219396e-05,
            "epoch": 0.2742448330683625,
            "step": 4140
        },
        {
            "loss": 0.402,
            "grad_norm": 3.484933376312256,
            "learning_rate": 4.540717187776011e-05,
            "epoch": 0.27556968733439324,
            "step": 4160
        },
        {
            "loss": 0.4383,
            "grad_norm": 1.7955957651138306,
            "learning_rate": 4.538509097332627e-05,
            "epoch": 0.2768945416004239,
            "step": 4180
        },
        {
            "loss": 0.4356,
            "grad_norm": 0.9766614437103271,
            "learning_rate": 4.536301006889243e-05,
            "epoch": 0.27821939586645467,
            "step": 4200
        },
        {
            "loss": 0.3643,
            "grad_norm": 0.9589353799819946,
            "learning_rate": 4.534092916445858e-05,
            "epoch": 0.2795442501324854,
            "step": 4220
        },
        {
            "loss": 0.367,
            "grad_norm": 1.1189305782318115,
            "learning_rate": 4.531884826002473e-05,
            "epoch": 0.28086910439851615,
            "step": 4240
        },
        {
            "loss": 0.3784,
            "grad_norm": 1.732356071472168,
            "learning_rate": 4.529676735559089e-05,
            "epoch": 0.2821939586645469,
            "step": 4260
        },
        {
            "loss": 0.3545,
            "grad_norm": 0.8709729909896851,
            "learning_rate": 4.527468645115704e-05,
            "epoch": 0.2835188129305776,
            "step": 4280
        },
        {
            "loss": 0.4412,
            "grad_norm": 2.812835931777954,
            "learning_rate": 4.52526055467232e-05,
            "epoch": 0.28484366719660836,
            "step": 4300
        },
        {
            "loss": 0.4007,
            "grad_norm": 0.9328805208206177,
            "learning_rate": 4.523052464228935e-05,
            "epoch": 0.2861685214626391,
            "step": 4320
        },
        {
            "loss": 0.4291,
            "grad_norm": 1.3460626602172852,
            "learning_rate": 4.5208443737855506e-05,
            "epoch": 0.28749337572866984,
            "step": 4340
        },
        {
            "loss": 0.3941,
            "grad_norm": 2.7217273712158203,
            "learning_rate": 4.518636283342166e-05,
            "epoch": 0.2888182299947006,
            "step": 4360
        },
        {
            "loss": 0.4305,
            "grad_norm": 2.0588669776916504,
            "learning_rate": 4.516428192898781e-05,
            "epoch": 0.2901430842607313,
            "step": 4380
        },
        {
            "loss": 0.4223,
            "grad_norm": 0.7893779277801514,
            "learning_rate": 4.514220102455397e-05,
            "epoch": 0.29146793852676206,
            "step": 4400
        },
        {
            "loss": 0.4011,
            "grad_norm": 1.1542632579803467,
            "learning_rate": 4.5120120120120124e-05,
            "epoch": 0.2927927927927928,
            "step": 4420
        },
        {
            "loss": 0.4378,
            "grad_norm": 2.660565137863159,
            "learning_rate": 4.5098039215686275e-05,
            "epoch": 0.29411764705882354,
            "step": 4440
        },
        {
            "loss": 0.4576,
            "grad_norm": 2.9509360790252686,
            "learning_rate": 4.507595831125243e-05,
            "epoch": 0.2954425013248543,
            "step": 4460
        },
        {
            "loss": 0.3478,
            "grad_norm": 59.83950424194336,
            "learning_rate": 4.5053877406818584e-05,
            "epoch": 0.296767355590885,
            "step": 4480
        },
        {
            "loss": 0.3806,
            "grad_norm": 21.29452133178711,
            "learning_rate": 4.5031796502384735e-05,
            "epoch": 0.29809220985691576,
            "step": 4500
        },
        {
            "loss": 0.3556,
            "grad_norm": 2.5875403881073,
            "learning_rate": 4.50097155979509e-05,
            "epoch": 0.2994170641229465,
            "step": 4520
        },
        {
            "loss": 0.4276,
            "grad_norm": 0.9696388840675354,
            "learning_rate": 4.498763469351705e-05,
            "epoch": 0.30074191838897724,
            "step": 4540
        },
        {
            "loss": 0.3734,
            "grad_norm": 3.215456962585449,
            "learning_rate": 4.49655537890832e-05,
            "epoch": 0.302066772655008,
            "step": 4560
        },
        {
            "loss": 0.4155,
            "grad_norm": 1.0809352397918701,
            "learning_rate": 4.494347288464936e-05,
            "epoch": 0.30339162692103866,
            "step": 4580
        },
        {
            "loss": 0.3018,
            "grad_norm": 1.9140081405639648,
            "learning_rate": 4.492139198021551e-05,
            "epoch": 0.3047164811870694,
            "step": 4600
        },
        {
            "loss": 0.4531,
            "grad_norm": 1.2417969703674316,
            "learning_rate": 4.489931107578166e-05,
            "epoch": 0.30604133545310014,
            "step": 4620
        },
        {
            "loss": 0.3731,
            "grad_norm": 3.772277593612671,
            "learning_rate": 4.487723017134782e-05,
            "epoch": 0.3073661897191309,
            "step": 4640
        },
        {
            "loss": 0.4449,
            "grad_norm": 1.4936484098434448,
            "learning_rate": 4.485514926691398e-05,
            "epoch": 0.3086910439851616,
            "step": 4660
        },
        {
            "loss": 0.4401,
            "grad_norm": 1.4133191108703613,
            "learning_rate": 4.483306836248013e-05,
            "epoch": 0.31001589825119236,
            "step": 4680
        },
        {
            "loss": 0.4467,
            "grad_norm": 2.9627084732055664,
            "learning_rate": 4.4810987458046286e-05,
            "epoch": 0.3113407525172231,
            "step": 4700
        },
        {
            "loss": 0.4725,
            "grad_norm": 1.1472368240356445,
            "learning_rate": 4.478890655361244e-05,
            "epoch": 0.31266560678325384,
            "step": 4720
        },
        {
            "loss": 0.4358,
            "grad_norm": 2.8722312450408936,
            "learning_rate": 4.476682564917859e-05,
            "epoch": 0.3139904610492846,
            "step": 4740
        },
        {
            "loss": 0.3928,
            "grad_norm": 10.2449369430542,
            "learning_rate": 4.4744744744744746e-05,
            "epoch": 0.3153153153153153,
            "step": 4760
        },
        {
            "loss": 0.3959,
            "grad_norm": 1.4186664819717407,
            "learning_rate": 4.4722663840310904e-05,
            "epoch": 0.31664016958134605,
            "step": 4780
        },
        {
            "loss": 0.4386,
            "grad_norm": 28.169343948364258,
            "learning_rate": 4.4700582935877055e-05,
            "epoch": 0.3179650238473768,
            "step": 4800
        },
        {
            "loss": 0.3905,
            "grad_norm": 17.9615535736084,
            "learning_rate": 4.4678502031443206e-05,
            "epoch": 0.31928987811340753,
            "step": 4820
        },
        {
            "loss": 0.3882,
            "grad_norm": 1.9127986431121826,
            "learning_rate": 4.4656421127009364e-05,
            "epoch": 0.32061473237943827,
            "step": 4840
        },
        {
            "loss": 0.3958,
            "grad_norm": 1.2127302885055542,
            "learning_rate": 4.463434022257552e-05,
            "epoch": 0.321939586645469,
            "step": 4860
        },
        {
            "loss": 0.4327,
            "grad_norm": 1.8561724424362183,
            "learning_rate": 4.461225931814167e-05,
            "epoch": 0.32326444091149975,
            "step": 4880
        },
        {
            "loss": 0.4108,
            "grad_norm": 1.0264633893966675,
            "learning_rate": 4.459017841370783e-05,
            "epoch": 0.3245892951775305,
            "step": 4900
        },
        {
            "loss": 0.4424,
            "grad_norm": 1.78843092918396,
            "learning_rate": 4.456809750927398e-05,
            "epoch": 0.32591414944356123,
            "step": 4920
        },
        {
            "loss": 0.4158,
            "grad_norm": 1.646062970161438,
            "learning_rate": 4.454601660484013e-05,
            "epoch": 0.32723900370959197,
            "step": 4940
        },
        {
            "loss": 0.4386,
            "grad_norm": 2.683584451675415,
            "learning_rate": 4.452393570040629e-05,
            "epoch": 0.3285638579756227,
            "step": 4960
        },
        {
            "loss": 0.4238,
            "grad_norm": 0.7956550717353821,
            "learning_rate": 4.450185479597245e-05,
            "epoch": 0.3298887122416534,
            "step": 4980
        },
        {
            "loss": 0.3728,
            "grad_norm": 2.8542697429656982,
            "learning_rate": 4.44797738915386e-05,
            "epoch": 0.33121356650768413,
            "step": 5000
        },
        {
            "loss": 0.4073,
            "grad_norm": 2.9271883964538574,
            "learning_rate": 4.445769298710476e-05,
            "epoch": 0.33253842077371487,
            "step": 5020
        },
        {
            "loss": 0.4246,
            "grad_norm": 1.5673496723175049,
            "learning_rate": 4.443561208267091e-05,
            "epoch": 0.3338632750397456,
            "step": 5040
        },
        {
            "loss": 0.4398,
            "grad_norm": 2.044095754623413,
            "learning_rate": 4.441353117823706e-05,
            "epoch": 0.33518812930577635,
            "step": 5060
        },
        {
            "loss": 0.395,
            "grad_norm": 10.513792991638184,
            "learning_rate": 4.439145027380322e-05,
            "epoch": 0.3365129835718071,
            "step": 5080
        },
        {
            "loss": 0.4532,
            "grad_norm": 2.0194015502929688,
            "learning_rate": 4.4369369369369375e-05,
            "epoch": 0.33783783783783783,
            "step": 5100
        },
        {
            "loss": 0.3696,
            "grad_norm": 2.2083981037139893,
            "learning_rate": 4.4347288464935526e-05,
            "epoch": 0.33916269210386857,
            "step": 5120
        },
        {
            "loss": 0.38,
            "grad_norm": 0.8773791193962097,
            "learning_rate": 4.432520756050168e-05,
            "epoch": 0.3404875463698993,
            "step": 5140
        },
        {
            "loss": 0.4054,
            "grad_norm": 1.0011570453643799,
            "learning_rate": 4.4303126656067835e-05,
            "epoch": 0.34181240063593005,
            "step": 5160
        },
        {
            "loss": 0.3748,
            "grad_norm": 0.5066204071044922,
            "learning_rate": 4.4281045751633986e-05,
            "epoch": 0.3431372549019608,
            "step": 5180
        },
        {
            "loss": 0.4362,
            "grad_norm": 63.447608947753906,
            "learning_rate": 4.4258964847200144e-05,
            "epoch": 0.3444621091679915,
            "step": 5200
        },
        {
            "loss": 0.4309,
            "grad_norm": 1.6325092315673828,
            "learning_rate": 4.42368839427663e-05,
            "epoch": 0.34578696343402227,
            "step": 5220
        },
        {
            "loss": 0.4597,
            "grad_norm": 19.351911544799805,
            "learning_rate": 4.421480303833245e-05,
            "epoch": 0.347111817700053,
            "step": 5240
        },
        {
            "loss": 0.4291,
            "grad_norm": 7.353322505950928,
            "learning_rate": 4.4192722133898604e-05,
            "epoch": 0.34843667196608374,
            "step": 5260
        },
        {
            "loss": 0.4643,
            "grad_norm": 1.4900858402252197,
            "learning_rate": 4.417064122946476e-05,
            "epoch": 0.3497615262321145,
            "step": 5280
        },
        {
            "loss": 0.373,
            "grad_norm": 2.3834261894226074,
            "learning_rate": 4.414856032503091e-05,
            "epoch": 0.3510863804981452,
            "step": 5300
        },
        {
            "loss": 0.4425,
            "grad_norm": 5.837647438049316,
            "learning_rate": 4.412647942059707e-05,
            "epoch": 0.35241123476417596,
            "step": 5320
        },
        {
            "loss": 0.4865,
            "grad_norm": 2.2059898376464844,
            "learning_rate": 4.410439851616323e-05,
            "epoch": 0.3537360890302067,
            "step": 5340
        },
        {
            "loss": 0.4134,
            "grad_norm": 3.468721389770508,
            "learning_rate": 4.408231761172938e-05,
            "epoch": 0.35506094329623744,
            "step": 5360
        },
        {
            "loss": 0.4586,
            "grad_norm": 2.1123387813568115,
            "learning_rate": 4.406023670729553e-05,
            "epoch": 0.3563857975622681,
            "step": 5380
        },
        {
            "loss": 0.4202,
            "grad_norm": 3.6447110176086426,
            "learning_rate": 4.403815580286169e-05,
            "epoch": 0.35771065182829886,
            "step": 5400
        },
        {
            "loss": 0.4632,
            "grad_norm": 73.74510955810547,
            "learning_rate": 4.401607489842784e-05,
            "epoch": 0.3590355060943296,
            "step": 5420
        },
        {
            "loss": 0.4115,
            "grad_norm": 0.9574823975563049,
            "learning_rate": 4.3993993993994e-05,
            "epoch": 0.36036036036036034,
            "step": 5440
        },
        {
            "loss": 0.4017,
            "grad_norm": 1.3010674715042114,
            "learning_rate": 4.3971913089560155e-05,
            "epoch": 0.3616852146263911,
            "step": 5460
        },
        {
            "loss": 0.4391,
            "grad_norm": 36.86811828613281,
            "learning_rate": 4.3949832185126306e-05,
            "epoch": 0.3630100688924218,
            "step": 5480
        },
        {
            "loss": 0.4733,
            "grad_norm": 1.357970952987671,
            "learning_rate": 4.392775128069246e-05,
            "epoch": 0.36433492315845256,
            "step": 5500
        },
        {
            "loss": 0.4686,
            "grad_norm": 2.509345293045044,
            "learning_rate": 4.3905670376258615e-05,
            "epoch": 0.3656597774244833,
            "step": 5520
        },
        {
            "loss": 0.3447,
            "grad_norm": 0.890937328338623,
            "learning_rate": 4.3883589471824766e-05,
            "epoch": 0.36698463169051404,
            "step": 5540
        },
        {
            "loss": 0.463,
            "grad_norm": 1.0271861553192139,
            "learning_rate": 4.3861508567390924e-05,
            "epoch": 0.3683094859565448,
            "step": 5560
        },
        {
            "loss": 0.4181,
            "grad_norm": 1.7845611572265625,
            "learning_rate": 4.3839427662957075e-05,
            "epoch": 0.3696343402225755,
            "step": 5580
        },
        {
            "loss": 0.4745,
            "grad_norm": 0.6908029913902283,
            "learning_rate": 4.381734675852323e-05,
            "epoch": 0.37095919448860626,
            "step": 5600
        },
        {
            "loss": 0.3755,
            "grad_norm": 2.0952646732330322,
            "learning_rate": 4.3795265854089384e-05,
            "epoch": 0.372284048754637,
            "step": 5620
        },
        {
            "loss": 0.3833,
            "grad_norm": 2.0287013053894043,
            "learning_rate": 4.3773184949655535e-05,
            "epoch": 0.37360890302066774,
            "step": 5640
        },
        {
            "loss": 0.404,
            "grad_norm": 1.7276854515075684,
            "learning_rate": 4.37511040452217e-05,
            "epoch": 0.3749337572866985,
            "step": 5660
        },
        {
            "loss": 0.4422,
            "grad_norm": 4.179422378540039,
            "learning_rate": 4.372902314078785e-05,
            "epoch": 0.3762586115527292,
            "step": 5680
        },
        {
            "loss": 0.4562,
            "grad_norm": 1.2768399715423584,
            "learning_rate": 4.3706942236354e-05,
            "epoch": 0.37758346581875996,
            "step": 5700
        },
        {
            "loss": 0.3969,
            "grad_norm": 1.1427556276321411,
            "learning_rate": 4.368486133192016e-05,
            "epoch": 0.3789083200847907,
            "step": 5720
        },
        {
            "loss": 0.4571,
            "grad_norm": 3.521160840988159,
            "learning_rate": 4.366278042748631e-05,
            "epoch": 0.38023317435082143,
            "step": 5740
        },
        {
            "loss": 0.4176,
            "grad_norm": 1.5541903972625732,
            "learning_rate": 4.364069952305246e-05,
            "epoch": 0.3815580286168522,
            "step": 5760
        },
        {
            "loss": 0.4003,
            "grad_norm": 1.0141116380691528,
            "learning_rate": 4.3618618618618626e-05,
            "epoch": 0.38288288288288286,
            "step": 5780
        },
        {
            "loss": 0.385,
            "grad_norm": 1.1968950033187866,
            "learning_rate": 4.359653771418478e-05,
            "epoch": 0.3842077371489136,
            "step": 5800
        },
        {
            "loss": 0.4331,
            "grad_norm": 1.4728479385375977,
            "learning_rate": 4.357445680975093e-05,
            "epoch": 0.38553259141494434,
            "step": 5820
        },
        {
            "loss": 0.4194,
            "grad_norm": 1.185603380203247,
            "learning_rate": 4.3552375905317086e-05,
            "epoch": 0.3868574456809751,
            "step": 5840
        },
        {
            "loss": 0.4804,
            "grad_norm": 3.6152381896972656,
            "learning_rate": 4.353029500088324e-05,
            "epoch": 0.3881822999470058,
            "step": 5860
        },
        {
            "loss": 0.4419,
            "grad_norm": 6.147012710571289,
            "learning_rate": 4.350821409644939e-05,
            "epoch": 0.38950715421303655,
            "step": 5880
        },
        {
            "loss": 0.3612,
            "grad_norm": 0.7400503158569336,
            "learning_rate": 4.3486133192015546e-05,
            "epoch": 0.3908320084790673,
            "step": 5900
        },
        {
            "loss": 0.4615,
            "grad_norm": 1.314895749092102,
            "learning_rate": 4.3464052287581704e-05,
            "epoch": 0.39215686274509803,
            "step": 5920
        },
        {
            "loss": 0.3995,
            "grad_norm": 0.8391532301902771,
            "learning_rate": 4.3441971383147855e-05,
            "epoch": 0.3934817170111288,
            "step": 5940
        },
        {
            "loss": 0.3754,
            "grad_norm": 1.261327862739563,
            "learning_rate": 4.3419890478714006e-05,
            "epoch": 0.3948065712771595,
            "step": 5960
        },
        {
            "loss": 0.4267,
            "grad_norm": 0.7676388621330261,
            "learning_rate": 4.3397809574280164e-05,
            "epoch": 0.39613142554319025,
            "step": 5980
        },
        {
            "loss": 0.4645,
            "grad_norm": 3.1555233001708984,
            "learning_rate": 4.337572866984632e-05,
            "epoch": 0.397456279809221,
            "step": 6000
        },
        {
            "loss": 0.428,
            "grad_norm": 2.5834758281707764,
            "learning_rate": 4.335364776541247e-05,
            "epoch": 0.39878113407525173,
            "step": 6020
        },
        {
            "loss": 0.3993,
            "grad_norm": 2.459329605102539,
            "learning_rate": 4.333156686097863e-05,
            "epoch": 0.40010598834128247,
            "step": 6040
        },
        {
            "loss": 0.5084,
            "grad_norm": 65.75983428955078,
            "learning_rate": 4.330948595654478e-05,
            "epoch": 0.4014308426073132,
            "step": 6060
        },
        {
            "loss": 0.3622,
            "grad_norm": 2.9041831493377686,
            "learning_rate": 4.328740505211093e-05,
            "epoch": 0.40275569687334395,
            "step": 6080
        },
        {
            "loss": 0.3538,
            "grad_norm": 3.046308755874634,
            "learning_rate": 4.326532414767709e-05,
            "epoch": 0.4040805511393747,
            "step": 6100
        },
        {
            "loss": 0.4651,
            "grad_norm": 3.5363211631774902,
            "learning_rate": 4.324324324324325e-05,
            "epoch": 0.40540540540540543,
            "step": 6120
        },
        {
            "loss": 0.4173,
            "grad_norm": 12.56222152709961,
            "learning_rate": 4.32211623388094e-05,
            "epoch": 0.40673025967143617,
            "step": 6140
        },
        {
            "loss": 0.4075,
            "grad_norm": 1.2942616939544678,
            "learning_rate": 4.319908143437556e-05,
            "epoch": 0.40805511393746685,
            "step": 6160
        },
        {
            "loss": 0.4602,
            "grad_norm": 1.7197608947753906,
            "learning_rate": 4.317700052994171e-05,
            "epoch": 0.4093799682034976,
            "step": 6180
        },
        {
            "loss": 0.3747,
            "grad_norm": 4.733574390411377,
            "learning_rate": 4.315491962550786e-05,
            "epoch": 0.41070482246952833,
            "step": 6200
        },
        {
            "loss": 0.3751,
            "grad_norm": 0.7315953373908997,
            "learning_rate": 4.313283872107402e-05,
            "epoch": 0.41202967673555907,
            "step": 6220
        },
        {
            "loss": 0.4819,
            "grad_norm": 1.2130995988845825,
            "learning_rate": 4.3110757816640175e-05,
            "epoch": 0.4133545310015898,
            "step": 6240
        },
        {
            "loss": 0.4113,
            "grad_norm": 1.7929314374923706,
            "learning_rate": 4.3088676912206326e-05,
            "epoch": 0.41467938526762055,
            "step": 6260
        },
        {
            "loss": 0.4449,
            "grad_norm": 30.648080825805664,
            "learning_rate": 4.3066596007772484e-05,
            "epoch": 0.4160042395336513,
            "step": 6280
        },
        {
            "loss": 0.5083,
            "grad_norm": 3.080158233642578,
            "learning_rate": 4.3044515103338635e-05,
            "epoch": 0.417329093799682,
            "step": 6300
        },
        {
            "loss": 0.4208,
            "grad_norm": 0.892865002155304,
            "learning_rate": 4.3022434198904786e-05,
            "epoch": 0.41865394806571277,
            "step": 6320
        },
        {
            "loss": 0.4335,
            "grad_norm": 2.8839800357818604,
            "learning_rate": 4.3000353294470944e-05,
            "epoch": 0.4199788023317435,
            "step": 6340
        },
        {
            "loss": 0.4438,
            "grad_norm": 2.191767454147339,
            "learning_rate": 4.29782723900371e-05,
            "epoch": 0.42130365659777425,
            "step": 6360
        },
        {
            "loss": 0.4183,
            "grad_norm": 2.973823308944702,
            "learning_rate": 4.295619148560325e-05,
            "epoch": 0.422628510863805,
            "step": 6380
        },
        {
            "loss": 0.3869,
            "grad_norm": 0.8116955161094666,
            "learning_rate": 4.2934110581169404e-05,
            "epoch": 0.4239533651298357,
            "step": 6400
        },
        {
            "loss": 0.3977,
            "grad_norm": 1.523215651512146,
            "learning_rate": 4.291202967673556e-05,
            "epoch": 0.42527821939586646,
            "step": 6420
        },
        {
            "loss": 0.4669,
            "grad_norm": 60.58918380737305,
            "learning_rate": 4.288994877230171e-05,
            "epoch": 0.4266030736618972,
            "step": 6440
        },
        {
            "loss": 0.3922,
            "grad_norm": 2.3557679653167725,
            "learning_rate": 4.286786786786787e-05,
            "epoch": 0.42792792792792794,
            "step": 6460
        },
        {
            "loss": 0.4908,
            "grad_norm": 113.52296447753906,
            "learning_rate": 4.284578696343403e-05,
            "epoch": 0.4292527821939587,
            "step": 6480
        },
        {
            "loss": 0.345,
            "grad_norm": 1.1427491903305054,
            "learning_rate": 4.282370605900018e-05,
            "epoch": 0.4305776364599894,
            "step": 6500
        },
        {
            "loss": 0.3972,
            "grad_norm": 10.96420669555664,
            "learning_rate": 4.280162515456633e-05,
            "epoch": 0.43190249072602016,
            "step": 6520
        },
        {
            "loss": 0.3819,
            "grad_norm": 0.8566926121711731,
            "learning_rate": 4.277954425013249e-05,
            "epoch": 0.4332273449920509,
            "step": 6540
        },
        {
            "loss": 0.388,
            "grad_norm": 2.95949125289917,
            "learning_rate": 4.275746334569864e-05,
            "epoch": 0.4345521992580816,
            "step": 6560
        },
        {
            "loss": 0.4126,
            "grad_norm": 0.8950037956237793,
            "learning_rate": 4.27353824412648e-05,
            "epoch": 0.4358770535241123,
            "step": 6580
        },
        {
            "loss": 0.4917,
            "grad_norm": 36.02928924560547,
            "learning_rate": 4.2713301536830955e-05,
            "epoch": 0.43720190779014306,
            "step": 6600
        },
        {
            "loss": 0.3967,
            "grad_norm": 5.938090801239014,
            "learning_rate": 4.2691220632397106e-05,
            "epoch": 0.4385267620561738,
            "step": 6620
        },
        {
            "loss": 0.3763,
            "grad_norm": 3.48968243598938,
            "learning_rate": 4.266913972796326e-05,
            "epoch": 0.43985161632220454,
            "step": 6640
        },
        {
            "loss": 0.4144,
            "grad_norm": 3.909097194671631,
            "learning_rate": 4.2647058823529415e-05,
            "epoch": 0.4411764705882353,
            "step": 6660
        },
        {
            "loss": 0.4431,
            "grad_norm": 2.430708885192871,
            "learning_rate": 4.2624977919095566e-05,
            "epoch": 0.442501324854266,
            "step": 6680
        },
        {
            "loss": 0.3643,
            "grad_norm": 2.900129795074463,
            "learning_rate": 4.2602897014661724e-05,
            "epoch": 0.44382617912029676,
            "step": 6700
        },
        {
            "loss": 0.3855,
            "grad_norm": 1.947550654411316,
            "learning_rate": 4.2580816110227875e-05,
            "epoch": 0.4451510333863275,
            "step": 6720
        },
        {
            "loss": 0.42,
            "grad_norm": 2.066469669342041,
            "learning_rate": 4.255873520579403e-05,
            "epoch": 0.44647588765235824,
            "step": 6740
        },
        {
            "loss": 0.407,
            "grad_norm": 1.0347869396209717,
            "learning_rate": 4.2536654301360184e-05,
            "epoch": 0.447800741918389,
            "step": 6760
        },
        {
            "loss": 0.404,
            "grad_norm": 38.22069549560547,
            "learning_rate": 4.251457339692634e-05,
            "epoch": 0.4491255961844197,
            "step": 6780
        },
        {
            "loss": 0.4525,
            "grad_norm": 2.397515058517456,
            "learning_rate": 4.24924924924925e-05,
            "epoch": 0.45045045045045046,
            "step": 6800
        },
        {
            "loss": 0.3502,
            "grad_norm": 1.0028842687606812,
            "learning_rate": 4.247041158805865e-05,
            "epoch": 0.4517753047164812,
            "step": 6820
        },
        {
            "loss": 0.3726,
            "grad_norm": 1.1209301948547363,
            "learning_rate": 4.24483306836248e-05,
            "epoch": 0.45310015898251194,
            "step": 6840
        },
        {
            "loss": 0.3839,
            "grad_norm": 53.41870880126953,
            "learning_rate": 4.242624977919096e-05,
            "epoch": 0.4544250132485427,
            "step": 6860
        },
        {
            "loss": 0.3803,
            "grad_norm": 71.09962463378906,
            "learning_rate": 4.240416887475711e-05,
            "epoch": 0.4557498675145734,
            "step": 6880
        },
        {
            "loss": 0.4033,
            "grad_norm": 0.7691662311553955,
            "learning_rate": 4.238208797032326e-05,
            "epoch": 0.45707472178060415,
            "step": 6900
        },
        {
            "loss": 0.3566,
            "grad_norm": 1.4552274942398071,
            "learning_rate": 4.2360007065889426e-05,
            "epoch": 0.4583995760466349,
            "step": 6920
        },
        {
            "loss": 0.3786,
            "grad_norm": 4.084726810455322,
            "learning_rate": 4.233792616145558e-05,
            "epoch": 0.45972443031266563,
            "step": 6940
        },
        {
            "loss": 0.3399,
            "grad_norm": 1.0224369764328003,
            "learning_rate": 4.231584525702173e-05,
            "epoch": 0.4610492845786963,
            "step": 6960
        },
        {
            "loss": 0.3742,
            "grad_norm": 3.5916242599487305,
            "learning_rate": 4.2293764352587886e-05,
            "epoch": 0.46237413884472706,
            "step": 6980
        },
        {
            "loss": 0.3692,
            "grad_norm": 2.76640248298645,
            "learning_rate": 4.227168344815404e-05,
            "epoch": 0.4636989931107578,
            "step": 7000
        },
        {
            "loss": 0.4796,
            "grad_norm": 0.7312853932380676,
            "learning_rate": 4.224960254372019e-05,
            "epoch": 0.46502384737678853,
            "step": 7020
        },
        {
            "loss": 0.4369,
            "grad_norm": 1.0024676322937012,
            "learning_rate": 4.222752163928635e-05,
            "epoch": 0.4663487016428193,
            "step": 7040
        },
        {
            "loss": 0.4233,
            "grad_norm": 0.7337478399276733,
            "learning_rate": 4.2205440734852504e-05,
            "epoch": 0.46767355590885,
            "step": 7060
        },
        {
            "loss": 0.4869,
            "grad_norm": 5.064351558685303,
            "learning_rate": 4.2183359830418655e-05,
            "epoch": 0.46899841017488075,
            "step": 7080
        },
        {
            "loss": 0.4596,
            "grad_norm": 0.9539772868156433,
            "learning_rate": 4.216127892598481e-05,
            "epoch": 0.4703232644409115,
            "step": 7100
        },
        {
            "loss": 0.4198,
            "grad_norm": 3.3751790523529053,
            "learning_rate": 4.2139198021550964e-05,
            "epoch": 0.47164811870694223,
            "step": 7120
        },
        {
            "loss": 0.3734,
            "grad_norm": 2.2264883518218994,
            "learning_rate": 4.2117117117117115e-05,
            "epoch": 0.47297297297297297,
            "step": 7140
        },
        {
            "loss": 0.4518,
            "grad_norm": 3.4633142948150635,
            "learning_rate": 4.209503621268327e-05,
            "epoch": 0.4742978272390037,
            "step": 7160
        },
        {
            "loss": 0.5004,
            "grad_norm": 1.2809219360351562,
            "learning_rate": 4.207295530824943e-05,
            "epoch": 0.47562268150503445,
            "step": 7180
        },
        {
            "loss": 0.5201,
            "grad_norm": 1.0435309410095215,
            "learning_rate": 4.205087440381558e-05,
            "epoch": 0.4769475357710652,
            "step": 7200
        },
        {
            "loss": 0.4076,
            "grad_norm": 0.8431523442268372,
            "learning_rate": 4.202879349938173e-05,
            "epoch": 0.47827239003709593,
            "step": 7220
        },
        {
            "loss": 0.3702,
            "grad_norm": 1.872876763343811,
            "learning_rate": 4.200671259494789e-05,
            "epoch": 0.47959724430312667,
            "step": 7240
        },
        {
            "loss": 0.4121,
            "grad_norm": 1.378613829612732,
            "learning_rate": 4.198463169051405e-05,
            "epoch": 0.4809220985691574,
            "step": 7260
        },
        {
            "loss": 0.3929,
            "grad_norm": 1.2406399250030518,
            "learning_rate": 4.19625507860802e-05,
            "epoch": 0.48224695283518815,
            "step": 7280
        },
        {
            "loss": 0.4399,
            "grad_norm": 1.5275527238845825,
            "learning_rate": 4.194046988164636e-05,
            "epoch": 0.4835718071012189,
            "step": 7300
        },
        {
            "loss": 0.3778,
            "grad_norm": 2.1488022804260254,
            "learning_rate": 4.191838897721251e-05,
            "epoch": 0.4848966613672496,
            "step": 7320
        },
        {
            "loss": 0.3443,
            "grad_norm": 76.75370788574219,
            "learning_rate": 4.189630807277866e-05,
            "epoch": 0.48622151563328037,
            "step": 7340
        },
        {
            "loss": 0.4142,
            "grad_norm": 1.5175505876541138,
            "learning_rate": 4.187422716834482e-05,
            "epoch": 0.48754636989931105,
            "step": 7360
        },
        {
            "loss": 0.4162,
            "grad_norm": 1.8505942821502686,
            "learning_rate": 4.1852146263910975e-05,
            "epoch": 0.4888712241653418,
            "step": 7380
        },
        {
            "loss": 0.4319,
            "grad_norm": 0.8238480091094971,
            "learning_rate": 4.1830065359477126e-05,
            "epoch": 0.49019607843137253,
            "step": 7400
        },
        {
            "loss": 0.4118,
            "grad_norm": 2.463902473449707,
            "learning_rate": 4.1807984455043284e-05,
            "epoch": 0.49152093269740327,
            "step": 7420
        },
        {
            "loss": 0.4051,
            "grad_norm": 1.749190092086792,
            "learning_rate": 4.1785903550609435e-05,
            "epoch": 0.492845786963434,
            "step": 7440
        },
        {
            "loss": 0.3701,
            "grad_norm": 1.6604454517364502,
            "learning_rate": 4.1763822646175586e-05,
            "epoch": 0.49417064122946475,
            "step": 7460
        },
        {
            "loss": 0.4544,
            "grad_norm": 2.485732316970825,
            "learning_rate": 4.1741741741741744e-05,
            "epoch": 0.4954954954954955,
            "step": 7480
        },
        {
            "loss": 0.4328,
            "grad_norm": 110.81462097167969,
            "learning_rate": 4.17196608373079e-05,
            "epoch": 0.4968203497615262,
            "step": 7500
        },
        {
            "loss": 0.4326,
            "grad_norm": 1.4928972721099854,
            "learning_rate": 4.169757993287405e-05,
            "epoch": 0.49814520402755696,
            "step": 7520
        },
        {
            "loss": 0.3513,
            "grad_norm": 0.797951877117157,
            "learning_rate": 4.167549902844021e-05,
            "epoch": 0.4994700582935877,
            "step": 7540
        },
        {
            "loss": 0.4284,
            "grad_norm": 1.172852873802185,
            "learning_rate": 4.165341812400636e-05,
            "epoch": 0.5007949125596184,
            "step": 7560
        },
        {
            "loss": 0.4313,
            "grad_norm": 2.349736213684082,
            "learning_rate": 4.163133721957251e-05,
            "epoch": 0.5021197668256492,
            "step": 7580
        },
        {
            "loss": 0.4416,
            "grad_norm": 1.7288830280303955,
            "learning_rate": 4.160925631513867e-05,
            "epoch": 0.5034446210916799,
            "step": 7600
        },
        {
            "loss": 0.4322,
            "grad_norm": 1.1643178462982178,
            "learning_rate": 4.158717541070483e-05,
            "epoch": 0.5047694753577107,
            "step": 7620
        },
        {
            "loss": 0.4183,
            "grad_norm": 2.7783076763153076,
            "learning_rate": 4.156509450627098e-05,
            "epoch": 0.5060943296237413,
            "step": 7640
        },
        {
            "loss": 0.3623,
            "grad_norm": 0.8718870878219604,
            "learning_rate": 4.154301360183713e-05,
            "epoch": 0.5074191838897721,
            "step": 7660
        },
        {
            "loss": 0.3586,
            "grad_norm": 5.856690883636475,
            "learning_rate": 4.152093269740329e-05,
            "epoch": 0.5087440381558028,
            "step": 7680
        },
        {
            "loss": 0.3653,
            "grad_norm": 1.6076124906539917,
            "learning_rate": 4.149885179296944e-05,
            "epoch": 0.5100688924218336,
            "step": 7700
        },
        {
            "loss": 0.4004,
            "grad_norm": 1.825805425643921,
            "learning_rate": 4.14767708885356e-05,
            "epoch": 0.5113937466878643,
            "step": 7720
        },
        {
            "loss": 0.3854,
            "grad_norm": 3.045722723007202,
            "learning_rate": 4.1454689984101755e-05,
            "epoch": 0.5127186009538951,
            "step": 7740
        },
        {
            "loss": 0.3603,
            "grad_norm": 1.9395793676376343,
            "learning_rate": 4.1432609079667906e-05,
            "epoch": 0.5140434552199258,
            "step": 7760
        },
        {
            "loss": 0.4267,
            "grad_norm": 1.517279028892517,
            "learning_rate": 4.141052817523406e-05,
            "epoch": 0.5153683094859566,
            "step": 7780
        },
        {
            "loss": 0.3854,
            "grad_norm": 2.0096096992492676,
            "learning_rate": 4.1388447270800215e-05,
            "epoch": 0.5166931637519873,
            "step": 7800
        },
        {
            "loss": 0.3539,
            "grad_norm": 2.075011730194092,
            "learning_rate": 4.1366366366366366e-05,
            "epoch": 0.5180180180180181,
            "step": 7820
        },
        {
            "loss": 0.4952,
            "grad_norm": 2.1622402667999268,
            "learning_rate": 4.1344285461932524e-05,
            "epoch": 0.5193428722840487,
            "step": 7840
        },
        {
            "loss": 0.3989,
            "grad_norm": 2.0522725582122803,
            "learning_rate": 4.132220455749868e-05,
            "epoch": 0.5206677265500795,
            "step": 7860
        },
        {
            "loss": 0.409,
            "grad_norm": 1.1892403364181519,
            "learning_rate": 4.130012365306483e-05,
            "epoch": 0.5219925808161102,
            "step": 7880
        },
        {
            "loss": 0.3748,
            "grad_norm": 2.1825177669525146,
            "learning_rate": 4.1278042748630983e-05,
            "epoch": 0.523317435082141,
            "step": 7900
        },
        {
            "loss": 0.5439,
            "grad_norm": 1.0940680503845215,
            "learning_rate": 4.125596184419714e-05,
            "epoch": 0.5246422893481717,
            "step": 7920
        },
        {
            "loss": 0.4385,
            "grad_norm": 2.6532552242279053,
            "learning_rate": 4.123388093976329e-05,
            "epoch": 0.5259671436142025,
            "step": 7940
        },
        {
            "loss": 0.3728,
            "grad_norm": 1.5448369979858398,
            "learning_rate": 4.121180003532945e-05,
            "epoch": 0.5272919978802332,
            "step": 7960
        },
        {
            "loss": 0.4554,
            "grad_norm": 1.7506846189498901,
            "learning_rate": 4.11897191308956e-05,
            "epoch": 0.5286168521462639,
            "step": 7980
        },
        {
            "loss": 0.388,
            "grad_norm": 4.8892130851745605,
            "learning_rate": 4.116763822646176e-05,
            "epoch": 0.5299417064122947,
            "step": 8000
        },
        {
            "loss": 0.4237,
            "grad_norm": 1.991420030593872,
            "learning_rate": 4.114555732202791e-05,
            "epoch": 0.5312665606783253,
            "step": 8020
        },
        {
            "loss": 0.4186,
            "grad_norm": 1.6863977909088135,
            "learning_rate": 4.112347641759406e-05,
            "epoch": 0.5325914149443561,
            "step": 8040
        },
        {
            "loss": 0.4003,
            "grad_norm": 2.7534284591674805,
            "learning_rate": 4.1101395513160226e-05,
            "epoch": 0.5339162692103868,
            "step": 8060
        },
        {
            "loss": 0.4586,
            "grad_norm": 1.5705598592758179,
            "learning_rate": 4.107931460872638e-05,
            "epoch": 0.5352411234764176,
            "step": 8080
        },
        {
            "loss": 0.3818,
            "grad_norm": 0.9639206528663635,
            "learning_rate": 4.105723370429253e-05,
            "epoch": 0.5365659777424483,
            "step": 8100
        },
        {
            "loss": 0.4508,
            "grad_norm": 1.868768572807312,
            "learning_rate": 4.1035152799858686e-05,
            "epoch": 0.5378908320084791,
            "step": 8120
        },
        {
            "loss": 0.3758,
            "grad_norm": 13.448873519897461,
            "learning_rate": 4.101307189542484e-05,
            "epoch": 0.5392156862745098,
            "step": 8140
        },
        {
            "loss": 0.4006,
            "grad_norm": 3.0018672943115234,
            "learning_rate": 4.099099099099099e-05,
            "epoch": 0.5405405405405406,
            "step": 8160
        },
        {
            "loss": 0.3826,
            "grad_norm": 2.916234016418457,
            "learning_rate": 4.096891008655715e-05,
            "epoch": 0.5418653948065713,
            "step": 8180
        },
        {
            "loss": 0.3751,
            "grad_norm": 2.0786309242248535,
            "learning_rate": 4.0946829182123303e-05,
            "epoch": 0.543190249072602,
            "step": 8200
        },
        {
            "loss": 0.4113,
            "grad_norm": 4.432816982269287,
            "learning_rate": 4.0924748277689455e-05,
            "epoch": 0.5445151033386327,
            "step": 8220
        },
        {
            "loss": 0.4493,
            "grad_norm": 23.87368392944336,
            "learning_rate": 4.090266737325561e-05,
            "epoch": 0.5458399576046635,
            "step": 8240
        },
        {
            "loss": 0.4232,
            "grad_norm": 3.573601484298706,
            "learning_rate": 4.0880586468821763e-05,
            "epoch": 0.5471648118706942,
            "step": 8260
        },
        {
            "loss": 0.3868,
            "grad_norm": 1.1508731842041016,
            "learning_rate": 4.0858505564387914e-05,
            "epoch": 0.548489666136725,
            "step": 8280
        },
        {
            "loss": 0.3881,
            "grad_norm": 24.66707992553711,
            "learning_rate": 4.083642465995407e-05,
            "epoch": 0.5498145204027557,
            "step": 8300
        },
        {
            "loss": 0.3794,
            "grad_norm": 5.859675884246826,
            "learning_rate": 4.081434375552023e-05,
            "epoch": 0.5511393746687865,
            "step": 8320
        },
        {
            "loss": 0.3347,
            "grad_norm": 0.4429055154323578,
            "learning_rate": 4.079226285108638e-05,
            "epoch": 0.5524642289348172,
            "step": 8340
        },
        {
            "loss": 0.3673,
            "grad_norm": 4.429868698120117,
            "learning_rate": 4.077018194665254e-05,
            "epoch": 0.5537890832008479,
            "step": 8360
        },
        {
            "loss": 0.3703,
            "grad_norm": 0.754131555557251,
            "learning_rate": 4.074810104221869e-05,
            "epoch": 0.5551139374668786,
            "step": 8380
        },
        {
            "loss": 0.4122,
            "grad_norm": 1.7887905836105347,
            "learning_rate": 4.072602013778485e-05,
            "epoch": 0.5564387917329093,
            "step": 8400
        },
        {
            "loss": 0.3814,
            "grad_norm": 0.8877747654914856,
            "learning_rate": 4.0703939233351e-05,
            "epoch": 0.5577636459989401,
            "step": 8420
        },
        {
            "loss": 0.4425,
            "grad_norm": 2.0784549713134766,
            "learning_rate": 4.068185832891716e-05,
            "epoch": 0.5590885002649708,
            "step": 8440
        },
        {
            "loss": 0.3531,
            "grad_norm": 23.121763229370117,
            "learning_rate": 4.065977742448331e-05,
            "epoch": 0.5604133545310016,
            "step": 8460
        },
        {
            "loss": 0.3817,
            "grad_norm": 1.2636505365371704,
            "learning_rate": 4.063769652004946e-05,
            "epoch": 0.5617382087970323,
            "step": 8480
        },
        {
            "loss": 0.4001,
            "grad_norm": 2.7518422603607178,
            "learning_rate": 4.061561561561562e-05,
            "epoch": 0.5630630630630631,
            "step": 8500
        },
        {
            "loss": 0.4627,
            "grad_norm": 1.3423821926116943,
            "learning_rate": 4.0593534711181775e-05,
            "epoch": 0.5643879173290938,
            "step": 8520
        },
        {
            "loss": 0.5227,
            "grad_norm": 0.9267137050628662,
            "learning_rate": 4.0571453806747926e-05,
            "epoch": 0.5657127715951246,
            "step": 8540
        },
        {
            "loss": 0.3891,
            "grad_norm": 1.8928734064102173,
            "learning_rate": 4.0549372902314083e-05,
            "epoch": 0.5670376258611552,
            "step": 8560
        },
        {
            "loss": 0.4026,
            "grad_norm": 0.7581596970558167,
            "learning_rate": 4.0527291997880235e-05,
            "epoch": 0.568362480127186,
            "step": 8580
        },
        {
            "loss": 0.4787,
            "grad_norm": 2.017172336578369,
            "learning_rate": 4.0505211093446386e-05,
            "epoch": 0.5696873343932167,
            "step": 8600
        },
        {
            "loss": 0.4191,
            "grad_norm": 1.71153724193573,
            "learning_rate": 4.0483130189012543e-05,
            "epoch": 0.5710121886592475,
            "step": 8620
        },
        {
            "loss": 0.3681,
            "grad_norm": 0.9243051409721375,
            "learning_rate": 4.04610492845787e-05,
            "epoch": 0.5723370429252782,
            "step": 8640
        },
        {
            "loss": 0.4565,
            "grad_norm": 2.6986966133117676,
            "learning_rate": 4.043896838014485e-05,
            "epoch": 0.573661897191309,
            "step": 8660
        },
        {
            "loss": 0.3755,
            "grad_norm": 0.8469346761703491,
            "learning_rate": 4.041688747571101e-05,
            "epoch": 0.5749867514573397,
            "step": 8680
        },
        {
            "loss": 0.3544,
            "grad_norm": 1.6982024908065796,
            "learning_rate": 4.039480657127716e-05,
            "epoch": 0.5763116057233705,
            "step": 8700
        },
        {
            "loss": 0.4009,
            "grad_norm": 0.6836987137794495,
            "learning_rate": 4.037272566684331e-05,
            "epoch": 0.5776364599894012,
            "step": 8720
        },
        {
            "loss": 0.3767,
            "grad_norm": 1.973398208618164,
            "learning_rate": 4.035064476240947e-05,
            "epoch": 0.5789613142554318,
            "step": 8740
        },
        {
            "loss": 0.3605,
            "grad_norm": 18.432464599609375,
            "learning_rate": 4.032856385797563e-05,
            "epoch": 0.5802861685214626,
            "step": 8760
        },
        {
            "loss": 0.3909,
            "grad_norm": 0.8298428654670715,
            "learning_rate": 4.030648295354178e-05,
            "epoch": 0.5816110227874933,
            "step": 8780
        },
        {
            "loss": 0.4332,
            "grad_norm": 1.9939796924591064,
            "learning_rate": 4.028440204910793e-05,
            "epoch": 0.5829358770535241,
            "step": 8800
        },
        {
            "loss": 0.3998,
            "grad_norm": 59.630950927734375,
            "learning_rate": 4.026232114467409e-05,
            "epoch": 0.5842607313195548,
            "step": 8820
        },
        {
            "loss": 0.3881,
            "grad_norm": 1.2750122547149658,
            "learning_rate": 4.024024024024024e-05,
            "epoch": 0.5855855855855856,
            "step": 8840
        },
        {
            "loss": 0.4181,
            "grad_norm": 4.491105079650879,
            "learning_rate": 4.02181593358064e-05,
            "epoch": 0.5869104398516163,
            "step": 8860
        },
        {
            "loss": 0.3878,
            "grad_norm": 0.9201090335845947,
            "learning_rate": 4.0196078431372555e-05,
            "epoch": 0.5882352941176471,
            "step": 8880
        },
        {
            "loss": 0.3718,
            "grad_norm": 2.0155513286590576,
            "learning_rate": 4.0173997526938706e-05,
            "epoch": 0.5895601483836778,
            "step": 8900
        },
        {
            "loss": 0.418,
            "grad_norm": 34.14763259887695,
            "learning_rate": 4.015191662250486e-05,
            "epoch": 0.5908850026497086,
            "step": 8920
        },
        {
            "loss": 0.403,
            "grad_norm": 1.2068208456039429,
            "learning_rate": 4.0129835718071014e-05,
            "epoch": 0.5922098569157392,
            "step": 8940
        },
        {
            "loss": 0.3988,
            "grad_norm": 66.11067962646484,
            "learning_rate": 4.0107754813637166e-05,
            "epoch": 0.59353471118177,
            "step": 8960
        },
        {
            "loss": 0.3424,
            "grad_norm": 1.164050579071045,
            "learning_rate": 4.008567390920332e-05,
            "epoch": 0.5948595654478007,
            "step": 8980
        },
        {
            "loss": 0.4143,
            "grad_norm": 57.817623138427734,
            "learning_rate": 4.006359300476948e-05,
            "epoch": 0.5961844197138315,
            "step": 9000
        },
        {
            "loss": 0.3806,
            "grad_norm": 2.85414457321167,
            "learning_rate": 4.004151210033563e-05,
            "epoch": 0.5975092739798622,
            "step": 9020
        },
        {
            "loss": 0.3748,
            "grad_norm": 1.8685060739517212,
            "learning_rate": 4.001943119590178e-05,
            "epoch": 0.598834128245893,
            "step": 9040
        },
        {
            "loss": 0.37,
            "grad_norm": 2.831906795501709,
            "learning_rate": 3.999735029146794e-05,
            "epoch": 0.6001589825119237,
            "step": 9060
        },
        {
            "loss": 0.3727,
            "grad_norm": 141.9817657470703,
            "learning_rate": 3.997526938703409e-05,
            "epoch": 0.6014838367779545,
            "step": 9080
        },
        {
            "loss": 0.4383,
            "grad_norm": 8.549249649047852,
            "learning_rate": 3.995318848260025e-05,
            "epoch": 0.6028086910439852,
            "step": 9100
        },
        {
            "loss": 0.4106,
            "grad_norm": 1.2007331848144531,
            "learning_rate": 3.993110757816641e-05,
            "epoch": 0.604133545310016,
            "step": 9120
        },
        {
            "loss": 0.3876,
            "grad_norm": 1.0231820344924927,
            "learning_rate": 3.990902667373256e-05,
            "epoch": 0.6054583995760466,
            "step": 9140
        },
        {
            "loss": 0.3689,
            "grad_norm": 17.826072692871094,
            "learning_rate": 3.988694576929871e-05,
            "epoch": 0.6067832538420773,
            "step": 9160
        },
        {
            "loss": 0.3944,
            "grad_norm": 2.3918557167053223,
            "learning_rate": 3.986486486486487e-05,
            "epoch": 0.6081081081081081,
            "step": 9180
        },
        {
            "loss": 0.3971,
            "grad_norm": 1.016838788986206,
            "learning_rate": 3.9842783960431026e-05,
            "epoch": 0.6094329623741388,
            "step": 9200
        },
        {
            "loss": 0.4789,
            "grad_norm": 1.0226538181304932,
            "learning_rate": 3.982070305599718e-05,
            "epoch": 0.6107578166401696,
            "step": 9220
        },
        {
            "loss": 0.4108,
            "grad_norm": 78.60331726074219,
            "learning_rate": 3.979862215156333e-05,
            "epoch": 0.6120826709062003,
            "step": 9240
        },
        {
            "loss": 0.3973,
            "grad_norm": 5.495431900024414,
            "learning_rate": 3.9776541247129486e-05,
            "epoch": 0.6134075251722311,
            "step": 9260
        },
        {
            "loss": 0.3334,
            "grad_norm": 1.1750904321670532,
            "learning_rate": 3.9754460342695637e-05,
            "epoch": 0.6147323794382618,
            "step": 9280
        },
        {
            "loss": 0.4349,
            "grad_norm": 2.522399663925171,
            "learning_rate": 3.973237943826179e-05,
            "epoch": 0.6160572337042926,
            "step": 9300
        },
        {
            "loss": 0.4056,
            "grad_norm": 2.6706559658050537,
            "learning_rate": 3.971029853382795e-05,
            "epoch": 0.6173820879703232,
            "step": 9320
        },
        {
            "loss": 0.3983,
            "grad_norm": 1.46002995967865,
            "learning_rate": 3.96882176293941e-05,
            "epoch": 0.618706942236354,
            "step": 9340
        },
        {
            "loss": 0.3692,
            "grad_norm": 0.7791833281517029,
            "learning_rate": 3.9666136724960254e-05,
            "epoch": 0.6200317965023847,
            "step": 9360
        },
        {
            "loss": 0.4529,
            "grad_norm": 1.123610019683838,
            "learning_rate": 3.964405582052641e-05,
            "epoch": 0.6213566507684155,
            "step": 9380
        },
        {
            "loss": 0.4164,
            "grad_norm": 1.1123520135879517,
            "learning_rate": 3.962197491609256e-05,
            "epoch": 0.6226815050344462,
            "step": 9400
        },
        {
            "loss": 0.416,
            "grad_norm": 0.43104666471481323,
            "learning_rate": 3.9599894011658714e-05,
            "epoch": 0.624006359300477,
            "step": 9420
        },
        {
            "loss": 0.3769,
            "grad_norm": 0.7289124727249146,
            "learning_rate": 3.957781310722488e-05,
            "epoch": 0.6253312135665077,
            "step": 9440
        },
        {
            "loss": 0.4064,
            "grad_norm": 1.5528734922409058,
            "learning_rate": 3.955573220279103e-05,
            "epoch": 0.6266560678325385,
            "step": 9460
        },
        {
            "loss": 0.4325,
            "grad_norm": 1.7124260663986206,
            "learning_rate": 3.953365129835718e-05,
            "epoch": 0.6279809220985691,
            "step": 9480
        },
        {
            "loss": 0.4282,
            "grad_norm": 0.5169275999069214,
            "learning_rate": 3.951157039392334e-05,
            "epoch": 0.6293057763646,
            "step": 9500
        },
        {
            "loss": 0.3776,
            "grad_norm": 0.6450068950653076,
            "learning_rate": 3.948948948948949e-05,
            "epoch": 0.6306306306306306,
            "step": 9520
        },
        {
            "loss": 0.4295,
            "grad_norm": 2.7620646953582764,
            "learning_rate": 3.946740858505565e-05,
            "epoch": 0.6319554848966613,
            "step": 9540
        },
        {
            "loss": 0.501,
            "grad_norm": 59.64665603637695,
            "learning_rate": 3.94453276806218e-05,
            "epoch": 0.6332803391626921,
            "step": 9560
        },
        {
            "loss": 0.4084,
            "grad_norm": 5.503549098968506,
            "learning_rate": 3.942324677618796e-05,
            "epoch": 0.6346051934287228,
            "step": 9580
        },
        {
            "loss": 0.4546,
            "grad_norm": 1.2591172456741333,
            "learning_rate": 3.940116587175411e-05,
            "epoch": 0.6359300476947536,
            "step": 9600
        },
        {
            "loss": 0.3683,
            "grad_norm": 47.04401397705078,
            "learning_rate": 3.9379084967320266e-05,
            "epoch": 0.6372549019607843,
            "step": 9620
        },
        {
            "loss": 0.3935,
            "grad_norm": 1.2632752656936646,
            "learning_rate": 3.9357004062886417e-05,
            "epoch": 0.6385797562268151,
            "step": 9640
        },
        {
            "loss": 0.3857,
            "grad_norm": 2.7614850997924805,
            "learning_rate": 3.9334923158452574e-05,
            "epoch": 0.6399046104928457,
            "step": 9660
        },
        {
            "loss": 0.4166,
            "grad_norm": 3.6602954864501953,
            "learning_rate": 3.9312842254018725e-05,
            "epoch": 0.6412294647588765,
            "step": 9680
        },
        {
            "loss": 0.4279,
            "grad_norm": 1.9759262800216675,
            "learning_rate": 3.929076134958488e-05,
            "epoch": 0.6425543190249072,
            "step": 9700
        },
        {
            "loss": 0.4194,
            "grad_norm": 1.4427157640457153,
            "learning_rate": 3.9268680445151034e-05,
            "epoch": 0.643879173290938,
            "step": 9720
        },
        {
            "loss": 0.387,
            "grad_norm": 2.9428155422210693,
            "learning_rate": 3.9246599540717185e-05,
            "epoch": 0.6452040275569687,
            "step": 9740
        },
        {
            "loss": 0.3983,
            "grad_norm": 2.017305374145508,
            "learning_rate": 3.922451863628334e-05,
            "epoch": 0.6465288818229995,
            "step": 9760
        },
        {
            "loss": 0.3819,
            "grad_norm": 22.166444778442383,
            "learning_rate": 3.92024377318495e-05,
            "epoch": 0.6478537360890302,
            "step": 9780
        },
        {
            "loss": 0.3745,
            "grad_norm": 2.6944894790649414,
            "learning_rate": 3.918035682741565e-05,
            "epoch": 0.649178590355061,
            "step": 9800
        },
        {
            "loss": 0.3634,
            "grad_norm": 1.3781858682632446,
            "learning_rate": 3.915827592298181e-05,
            "epoch": 0.6505034446210917,
            "step": 9820
        },
        {
            "loss": 0.388,
            "grad_norm": 1.7130203247070312,
            "learning_rate": 3.913619501854796e-05,
            "epoch": 0.6518282988871225,
            "step": 9840
        },
        {
            "loss": 0.3891,
            "grad_norm": 2.6386964321136475,
            "learning_rate": 3.911411411411411e-05,
            "epoch": 0.6531531531531531,
            "step": 9860
        },
        {
            "loss": 0.445,
            "grad_norm": 1.3386303186416626,
            "learning_rate": 3.909203320968027e-05,
            "epoch": 0.6544780074191839,
            "step": 9880
        },
        {
            "loss": 0.4091,
            "grad_norm": 1.0977565050125122,
            "learning_rate": 3.906995230524643e-05,
            "epoch": 0.6558028616852146,
            "step": 9900
        },
        {
            "loss": 0.4444,
            "grad_norm": 2.7474963665008545,
            "learning_rate": 3.904787140081258e-05,
            "epoch": 0.6571277159512454,
            "step": 9920
        },
        {
            "loss": 0.5111,
            "grad_norm": 8.916584968566895,
            "learning_rate": 3.9025790496378737e-05,
            "epoch": 0.6584525702172761,
            "step": 9940
        },
        {
            "loss": 0.3992,
            "grad_norm": 0.7421107888221741,
            "learning_rate": 3.900370959194489e-05,
            "epoch": 0.6597774244833068,
            "step": 9960
        },
        {
            "loss": 0.3689,
            "grad_norm": 1.0062841176986694,
            "learning_rate": 3.898162868751104e-05,
            "epoch": 0.6611022787493376,
            "step": 9980
        },
        {
            "loss": 0.349,
            "grad_norm": 2.7643392086029053,
            "learning_rate": 3.8959547783077197e-05,
            "epoch": 0.6624271330153683,
            "step": 10000
        },
        {
            "loss": 0.4034,
            "grad_norm": 0.731636106967926,
            "learning_rate": 3.8937466878643354e-05,
            "epoch": 0.6637519872813991,
            "step": 10020
        },
        {
            "loss": 0.4093,
            "grad_norm": 54.118831634521484,
            "learning_rate": 3.8915385974209505e-05,
            "epoch": 0.6650768415474297,
            "step": 10040
        },
        {
            "loss": 0.4104,
            "grad_norm": 1.2338067293167114,
            "learning_rate": 3.8893305069775656e-05,
            "epoch": 0.6664016958134605,
            "step": 10060
        },
        {
            "loss": 0.4658,
            "grad_norm": 2.6638312339782715,
            "learning_rate": 3.8871224165341814e-05,
            "epoch": 0.6677265500794912,
            "step": 10080
        },
        {
            "loss": 0.4137,
            "grad_norm": 1.271949052810669,
            "learning_rate": 3.8849143260907965e-05,
            "epoch": 0.669051404345522,
            "step": 10100
        },
        {
            "loss": 0.501,
            "grad_norm": 68.30624389648438,
            "learning_rate": 3.882706235647412e-05,
            "epoch": 0.6703762586115527,
            "step": 10120
        },
        {
            "loss": 0.4108,
            "grad_norm": 1.7508916854858398,
            "learning_rate": 3.880498145204028e-05,
            "epoch": 0.6717011128775835,
            "step": 10140
        },
        {
            "loss": 0.4425,
            "grad_norm": 2.900967597961426,
            "learning_rate": 3.878290054760643e-05,
            "epoch": 0.6730259671436142,
            "step": 10160
        },
        {
            "loss": 0.4408,
            "grad_norm": 0.8630636930465698,
            "learning_rate": 3.876081964317258e-05,
            "epoch": 0.674350821409645,
            "step": 10180
        },
        {
            "loss": 0.4977,
            "grad_norm": 0.9195898175239563,
            "learning_rate": 3.873873873873874e-05,
            "epoch": 0.6756756756756757,
            "step": 10200
        },
        {
            "loss": 0.3822,
            "grad_norm": 4.844733715057373,
            "learning_rate": 3.871665783430489e-05,
            "epoch": 0.6770005299417065,
            "step": 10220
        },
        {
            "loss": 0.426,
            "grad_norm": 3.3429830074310303,
            "learning_rate": 3.869457692987105e-05,
            "epoch": 0.6783253842077371,
            "step": 10240
        },
        {
            "loss": 0.4439,
            "grad_norm": 2.4461026191711426,
            "learning_rate": 3.867249602543721e-05,
            "epoch": 0.6796502384737679,
            "step": 10260
        },
        {
            "loss": 0.4709,
            "grad_norm": 3.579641103744507,
            "learning_rate": 3.865041512100336e-05,
            "epoch": 0.6809750927397986,
            "step": 10280
        },
        {
            "loss": 0.3989,
            "grad_norm": 1.876741647720337,
            "learning_rate": 3.862833421656951e-05,
            "epoch": 0.6822999470058294,
            "step": 10300
        },
        {
            "loss": 0.3731,
            "grad_norm": 0.49568766355514526,
            "learning_rate": 3.860625331213567e-05,
            "epoch": 0.6836248012718601,
            "step": 10320
        },
        {
            "loss": 0.4023,
            "grad_norm": 0.6686891317367554,
            "learning_rate": 3.8584172407701825e-05,
            "epoch": 0.6849496555378908,
            "step": 10340
        },
        {
            "loss": 0.4022,
            "grad_norm": 0.9746384024620056,
            "learning_rate": 3.8562091503267977e-05,
            "epoch": 0.6862745098039216,
            "step": 10360
        },
        {
            "loss": 0.4051,
            "grad_norm": 2.09993839263916,
            "learning_rate": 3.854001059883413e-05,
            "epoch": 0.6875993640699523,
            "step": 10380
        },
        {
            "loss": 0.4684,
            "grad_norm": 2.307396411895752,
            "learning_rate": 3.8517929694400285e-05,
            "epoch": 0.688924218335983,
            "step": 10400
        },
        {
            "loss": 0.4387,
            "grad_norm": 2.0828962326049805,
            "learning_rate": 3.8495848789966436e-05,
            "epoch": 0.6902490726020137,
            "step": 10420
        },
        {
            "loss": 0.3774,
            "grad_norm": 0.41713041067123413,
            "learning_rate": 3.8473767885532594e-05,
            "epoch": 0.6915739268680445,
            "step": 10440
        },
        {
            "loss": 0.3808,
            "grad_norm": 1.2325280904769897,
            "learning_rate": 3.845168698109875e-05,
            "epoch": 0.6928987811340752,
            "step": 10460
        },
        {
            "loss": 0.4274,
            "grad_norm": 1.2076034545898438,
            "learning_rate": 3.84296060766649e-05,
            "epoch": 0.694223635400106,
            "step": 10480
        },
        {
            "loss": 0.425,
            "grad_norm": 2.2332205772399902,
            "learning_rate": 3.8407525172231054e-05,
            "epoch": 0.6955484896661367,
            "step": 10500
        },
        {
            "loss": 0.4226,
            "grad_norm": 1.1514729261398315,
            "learning_rate": 3.838544426779721e-05,
            "epoch": 0.6968733439321675,
            "step": 10520
        },
        {
            "loss": 0.4939,
            "grad_norm": 0.7503042817115784,
            "learning_rate": 3.836336336336336e-05,
            "epoch": 0.6981981981981982,
            "step": 10540
        },
        {
            "loss": 0.3723,
            "grad_norm": 1.4129533767700195,
            "learning_rate": 3.8341282458929514e-05,
            "epoch": 0.699523052464229,
            "step": 10560
        },
        {
            "loss": 0.3777,
            "grad_norm": 2.018700361251831,
            "learning_rate": 3.831920155449568e-05,
            "epoch": 0.7008479067302597,
            "step": 10580
        },
        {
            "loss": 0.3554,
            "grad_norm": 0.902955174446106,
            "learning_rate": 3.829712065006183e-05,
            "epoch": 0.7021727609962904,
            "step": 10600
        },
        {
            "loss": 0.3486,
            "grad_norm": 1.2369532585144043,
            "learning_rate": 3.827503974562798e-05,
            "epoch": 0.7034976152623211,
            "step": 10620
        },
        {
            "loss": 0.368,
            "grad_norm": 1.137014389038086,
            "learning_rate": 3.825295884119414e-05,
            "epoch": 0.7048224695283519,
            "step": 10640
        },
        {
            "loss": 0.3865,
            "grad_norm": 0.5589421391487122,
            "learning_rate": 3.823087793676029e-05,
            "epoch": 0.7061473237943826,
            "step": 10660
        },
        {
            "loss": 0.3899,
            "grad_norm": 0.993915855884552,
            "learning_rate": 3.820879703232644e-05,
            "epoch": 0.7074721780604134,
            "step": 10680
        },
        {
            "loss": 0.4383,
            "grad_norm": 2.6089186668395996,
            "learning_rate": 3.8186716127892605e-05,
            "epoch": 0.7087970323264441,
            "step": 10700
        },
        {
            "loss": 0.4858,
            "grad_norm": 1.314165711402893,
            "learning_rate": 3.8164635223458756e-05,
            "epoch": 0.7101218865924749,
            "step": 10720
        },
        {
            "loss": 0.4088,
            "grad_norm": 1.865285873413086,
            "learning_rate": 3.814255431902491e-05,
            "epoch": 0.7114467408585056,
            "step": 10740
        },
        {
            "loss": 0.43,
            "grad_norm": 3.6119027137756348,
            "learning_rate": 3.8120473414591065e-05,
            "epoch": 0.7127715951245363,
            "step": 10760
        },
        {
            "loss": 0.4003,
            "grad_norm": 0.6542223691940308,
            "learning_rate": 3.8098392510157216e-05,
            "epoch": 0.714096449390567,
            "step": 10780
        },
        {
            "loss": 0.3464,
            "grad_norm": 2.310107946395874,
            "learning_rate": 3.8076311605723374e-05,
            "epoch": 0.7154213036565977,
            "step": 10800
        },
        {
            "loss": 0.5039,
            "grad_norm": 1.274962067604065,
            "learning_rate": 3.8054230701289525e-05,
            "epoch": 0.7167461579226285,
            "step": 10820
        },
        {
            "loss": 0.3621,
            "grad_norm": 3.8203890323638916,
            "learning_rate": 3.803214979685568e-05,
            "epoch": 0.7180710121886592,
            "step": 10840
        },
        {
            "loss": 0.4332,
            "grad_norm": 1.932471513748169,
            "learning_rate": 3.8010068892421834e-05,
            "epoch": 0.71939586645469,
            "step": 10860
        },
        {
            "loss": 0.4107,
            "grad_norm": 0.6392616629600525,
            "learning_rate": 3.7987987987987985e-05,
            "epoch": 0.7207207207207207,
            "step": 10880
        },
        {
            "loss": 0.5053,
            "grad_norm": 52.25007247924805,
            "learning_rate": 3.796590708355414e-05,
            "epoch": 0.7220455749867515,
            "step": 10900
        },
        {
            "loss": 0.405,
            "grad_norm": 0.9926712512969971,
            "learning_rate": 3.79438261791203e-05,
            "epoch": 0.7233704292527822,
            "step": 10920
        },
        {
            "loss": 0.4304,
            "grad_norm": 2.3094701766967773,
            "learning_rate": 3.792174527468645e-05,
            "epoch": 0.724695283518813,
            "step": 10940
        },
        {
            "loss": 0.3864,
            "grad_norm": 1.9361799955368042,
            "learning_rate": 3.789966437025261e-05,
            "epoch": 0.7260201377848436,
            "step": 10960
        },
        {
            "loss": 0.4252,
            "grad_norm": 10.510339736938477,
            "learning_rate": 3.787758346581876e-05,
            "epoch": 0.7273449920508744,
            "step": 10980
        },
        {
            "loss": 0.4603,
            "grad_norm": 1.270772099494934,
            "learning_rate": 3.785550256138491e-05,
            "epoch": 0.7286698463169051,
            "step": 11000
        },
        {
            "loss": 0.3996,
            "grad_norm": 1.145370364189148,
            "learning_rate": 3.783342165695107e-05,
            "epoch": 0.7299947005829359,
            "step": 11020
        },
        {
            "loss": 0.4105,
            "grad_norm": 1.863674283027649,
            "learning_rate": 3.781134075251723e-05,
            "epoch": 0.7313195548489666,
            "step": 11040
        },
        {
            "loss": 0.381,
            "grad_norm": 1.8146824836730957,
            "learning_rate": 3.778925984808338e-05,
            "epoch": 0.7326444091149974,
            "step": 11060
        },
        {
            "loss": 0.4052,
            "grad_norm": 1.0076018571853638,
            "learning_rate": 3.7767178943649536e-05,
            "epoch": 0.7339692633810281,
            "step": 11080
        },
        {
            "loss": 0.3776,
            "grad_norm": 0.9062448143959045,
            "learning_rate": 3.774509803921569e-05,
            "epoch": 0.7352941176470589,
            "step": 11100
        },
        {
            "loss": 0.3493,
            "grad_norm": 1.9957468509674072,
            "learning_rate": 3.772301713478184e-05,
            "epoch": 0.7366189719130896,
            "step": 11120
        },
        {
            "loss": 0.4955,
            "grad_norm": 79.28581237792969,
            "learning_rate": 3.7700936230347996e-05,
            "epoch": 0.7379438261791202,
            "step": 11140
        },
        {
            "loss": 0.3908,
            "grad_norm": 0.6343973875045776,
            "learning_rate": 3.7678855325914154e-05,
            "epoch": 0.739268680445151,
            "step": 11160
        },
        {
            "loss": 0.4075,
            "grad_norm": 2.8119776248931885,
            "learning_rate": 3.7656774421480305e-05,
            "epoch": 0.7405935347111817,
            "step": 11180
        },
        {
            "loss": 0.4573,
            "grad_norm": 1.2931158542633057,
            "learning_rate": 3.763469351704646e-05,
            "epoch": 0.7419183889772125,
            "step": 11200
        },
        {
            "loss": 0.3704,
            "grad_norm": 1.09230637550354,
            "learning_rate": 3.7612612612612614e-05,
            "epoch": 0.7432432432432432,
            "step": 11220
        },
        {
            "loss": 0.3785,
            "grad_norm": 0.8457382917404175,
            "learning_rate": 3.7590531708178765e-05,
            "epoch": 0.744568097509274,
            "step": 11240
        },
        {
            "loss": 0.4385,
            "grad_norm": 0.8626686334609985,
            "learning_rate": 3.756845080374492e-05,
            "epoch": 0.7458929517753047,
            "step": 11260
        },
        {
            "loss": 0.4721,
            "grad_norm": 30.905029296875,
            "learning_rate": 3.754636989931108e-05,
            "epoch": 0.7472178060413355,
            "step": 11280
        },
        {
            "loss": 0.4062,
            "grad_norm": 2.7147462368011475,
            "learning_rate": 3.752428899487723e-05,
            "epoch": 0.7485426603073662,
            "step": 11300
        },
        {
            "loss": 0.4113,
            "grad_norm": 4.315438747406006,
            "learning_rate": 3.750220809044338e-05,
            "epoch": 0.749867514573397,
            "step": 11320
        },
        {
            "loss": 0.3746,
            "grad_norm": 0.97907954454422,
            "learning_rate": 3.748012718600954e-05,
            "epoch": 0.7511923688394276,
            "step": 11340
        },
        {
            "loss": 0.425,
            "grad_norm": 1.64150071144104,
            "learning_rate": 3.745804628157569e-05,
            "epoch": 0.7525172231054584,
            "step": 11360
        },
        {
            "loss": 0.4082,
            "grad_norm": 0.5689888596534729,
            "learning_rate": 3.743596537714185e-05,
            "epoch": 0.7538420773714891,
            "step": 11380
        },
        {
            "loss": 0.4714,
            "grad_norm": 2.7892649173736572,
            "learning_rate": 3.741388447270801e-05,
            "epoch": 0.7551669316375199,
            "step": 11400
        },
        {
            "loss": 0.4091,
            "grad_norm": 71.29450225830078,
            "learning_rate": 3.739180356827416e-05,
            "epoch": 0.7564917859035506,
            "step": 11420
        },
        {
            "loss": 0.3536,
            "grad_norm": 2.69802188873291,
            "learning_rate": 3.736972266384031e-05,
            "epoch": 0.7578166401695814,
            "step": 11440
        },
        {
            "loss": 0.453,
            "grad_norm": 1.2466015815734863,
            "learning_rate": 3.734764175940647e-05,
            "epoch": 0.7591414944356121,
            "step": 11460
        },
        {
            "loss": 0.378,
            "grad_norm": 2.158018112182617,
            "learning_rate": 3.732556085497262e-05,
            "epoch": 0.7604663487016429,
            "step": 11480
        },
        {
            "loss": 0.4065,
            "grad_norm": 0.5845481157302856,
            "learning_rate": 3.7303479950538776e-05,
            "epoch": 0.7617912029676736,
            "step": 11500
        },
        {
            "loss": 0.3551,
            "grad_norm": 0.7356360554695129,
            "learning_rate": 3.7281399046104934e-05,
            "epoch": 0.7631160572337043,
            "step": 11520
        },
        {
            "loss": 0.5156,
            "grad_norm": 12.047039985656738,
            "learning_rate": 3.7259318141671085e-05,
            "epoch": 0.764440911499735,
            "step": 11540
        },
        {
            "loss": 0.4511,
            "grad_norm": 1.1521344184875488,
            "learning_rate": 3.7237237237237236e-05,
            "epoch": 0.7657657657657657,
            "step": 11560
        },
        {
            "loss": 0.3447,
            "grad_norm": 2.6903305053710938,
            "learning_rate": 3.7215156332803394e-05,
            "epoch": 0.7670906200317965,
            "step": 11580
        },
        {
            "loss": 0.3831,
            "grad_norm": 2.978585720062256,
            "learning_rate": 3.719307542836955e-05,
            "epoch": 0.7684154742978272,
            "step": 11600
        },
        {
            "loss": 0.3734,
            "grad_norm": 1.461147427558899,
            "learning_rate": 3.71709945239357e-05,
            "epoch": 0.769740328563858,
            "step": 11620
        },
        {
            "loss": 0.4062,
            "grad_norm": 62.97139358520508,
            "learning_rate": 3.7148913619501854e-05,
            "epoch": 0.7710651828298887,
            "step": 11640
        },
        {
            "loss": 0.4151,
            "grad_norm": 1.0577402114868164,
            "learning_rate": 3.712683271506801e-05,
            "epoch": 0.7723900370959195,
            "step": 11660
        },
        {
            "loss": 0.396,
            "grad_norm": 1.6970322132110596,
            "learning_rate": 3.710475181063416e-05,
            "epoch": 0.7737148913619502,
            "step": 11680
        },
        {
            "loss": 0.39,
            "grad_norm": 1.587213158607483,
            "learning_rate": 3.708267090620032e-05,
            "epoch": 0.775039745627981,
            "step": 11700
        },
        {
            "loss": 0.4198,
            "grad_norm": 1.9651963710784912,
            "learning_rate": 3.706059000176648e-05,
            "epoch": 0.7763645998940116,
            "step": 11720
        },
        {
            "loss": 0.3824,
            "grad_norm": 1.0078476667404175,
            "learning_rate": 3.703850909733263e-05,
            "epoch": 0.7776894541600424,
            "step": 11740
        },
        {
            "loss": 0.4195,
            "grad_norm": 3.8634085655212402,
            "learning_rate": 3.701642819289878e-05,
            "epoch": 0.7790143084260731,
            "step": 11760
        },
        {
            "loss": 0.4183,
            "grad_norm": 1.4186856746673584,
            "learning_rate": 3.699434728846494e-05,
            "epoch": 0.7803391626921039,
            "step": 11780
        },
        {
            "loss": 0.3878,
            "grad_norm": 0.6813074350357056,
            "learning_rate": 3.697226638403109e-05,
            "epoch": 0.7816640169581346,
            "step": 11800
        },
        {
            "loss": 0.3657,
            "grad_norm": 0.5381388068199158,
            "learning_rate": 3.695018547959724e-05,
            "epoch": 0.7829888712241654,
            "step": 11820
        },
        {
            "loss": 0.5073,
            "grad_norm": 1.7719006538391113,
            "learning_rate": 3.6928104575163405e-05,
            "epoch": 0.7843137254901961,
            "step": 11840
        },
        {
            "loss": 0.3775,
            "grad_norm": 0.7071058750152588,
            "learning_rate": 3.6906023670729556e-05,
            "epoch": 0.7856385797562269,
            "step": 11860
        },
        {
            "loss": 0.377,
            "grad_norm": 2.870133876800537,
            "learning_rate": 3.688394276629571e-05,
            "epoch": 0.7869634340222575,
            "step": 11880
        },
        {
            "loss": 0.4131,
            "grad_norm": 0.6325113773345947,
            "learning_rate": 3.6861861861861865e-05,
            "epoch": 0.7882882882882883,
            "step": 11900
        },
        {
            "loss": 0.4348,
            "grad_norm": 0.8615822196006775,
            "learning_rate": 3.6839780957428016e-05,
            "epoch": 0.789613142554319,
            "step": 11920
        },
        {
            "loss": 0.4427,
            "grad_norm": 0.6418417692184448,
            "learning_rate": 3.6817700052994174e-05,
            "epoch": 0.7909379968203497,
            "step": 11940
        },
        {
            "loss": 0.3514,
            "grad_norm": 0.9630394577980042,
            "learning_rate": 3.679561914856033e-05,
            "epoch": 0.7922628510863805,
            "step": 11960
        },
        {
            "loss": 0.3275,
            "grad_norm": 1.9931575059890747,
            "learning_rate": 3.677353824412648e-05,
            "epoch": 0.7935877053524112,
            "step": 11980
        },
        {
            "loss": 0.3882,
            "grad_norm": 2.8501970767974854,
            "learning_rate": 3.6751457339692634e-05,
            "epoch": 0.794912559618442,
            "step": 12000
        },
        {
            "loss": 0.3131,
            "grad_norm": 0.8700041770935059,
            "learning_rate": 3.672937643525879e-05,
            "epoch": 0.7962374138844727,
            "step": 12020
        },
        {
            "loss": 0.4275,
            "grad_norm": 2.990705966949463,
            "learning_rate": 3.670729553082494e-05,
            "epoch": 0.7975622681505035,
            "step": 12040
        },
        {
            "loss": 0.4129,
            "grad_norm": 2.4816315174102783,
            "learning_rate": 3.66852146263911e-05,
            "epoch": 0.7988871224165341,
            "step": 12060
        },
        {
            "loss": 0.4084,
            "grad_norm": 4.372609615325928,
            "learning_rate": 3.666313372195725e-05,
            "epoch": 0.8002119766825649,
            "step": 12080
        },
        {
            "loss": 0.351,
            "grad_norm": 1.2069165706634521,
            "learning_rate": 3.664105281752341e-05,
            "epoch": 0.8015368309485956,
            "step": 12100
        },
        {
            "loss": 0.3648,
            "grad_norm": 1.7316616773605347,
            "learning_rate": 3.661897191308956e-05,
            "epoch": 0.8028616852146264,
            "step": 12120
        },
        {
            "loss": 0.4077,
            "grad_norm": 1.532650351524353,
            "learning_rate": 3.659689100865571e-05,
            "epoch": 0.8041865394806571,
            "step": 12140
        },
        {
            "loss": 0.3521,
            "grad_norm": 2.2631113529205322,
            "learning_rate": 3.657481010422187e-05,
            "epoch": 0.8055113937466879,
            "step": 12160
        },
        {
            "loss": 0.3689,
            "grad_norm": 0.8526358008384705,
            "learning_rate": 3.655272919978803e-05,
            "epoch": 0.8068362480127186,
            "step": 12180
        },
        {
            "loss": 0.438,
            "grad_norm": 0.9532443881034851,
            "learning_rate": 3.653064829535418e-05,
            "epoch": 0.8081611022787494,
            "step": 12200
        },
        {
            "loss": 0.4214,
            "grad_norm": 0.7213202118873596,
            "learning_rate": 3.6508567390920336e-05,
            "epoch": 0.8094859565447801,
            "step": 12220
        },
        {
            "loss": 0.3993,
            "grad_norm": 1.5590605735778809,
            "learning_rate": 3.648648648648649e-05,
            "epoch": 0.8108108108108109,
            "step": 12240
        },
        {
            "loss": 0.3808,
            "grad_norm": 2.0978879928588867,
            "learning_rate": 3.646440558205264e-05,
            "epoch": 0.8121356650768415,
            "step": 12260
        },
        {
            "loss": 0.4008,
            "grad_norm": 1.8452723026275635,
            "learning_rate": 3.6442324677618796e-05,
            "epoch": 0.8134605193428723,
            "step": 12280
        },
        {
            "loss": 0.3688,
            "grad_norm": 1.0633167028427124,
            "learning_rate": 3.6420243773184954e-05,
            "epoch": 0.814785373608903,
            "step": 12300
        },
        {
            "loss": 0.4781,
            "grad_norm": 0.8776607513427734,
            "learning_rate": 3.6398162868751105e-05,
            "epoch": 0.8161102278749337,
            "step": 12320
        },
        {
            "loss": 0.4462,
            "grad_norm": 9.879651069641113,
            "learning_rate": 3.637608196431726e-05,
            "epoch": 0.8174350821409645,
            "step": 12340
        },
        {
            "loss": 0.4195,
            "grad_norm": 0.691315770149231,
            "learning_rate": 3.6354001059883414e-05,
            "epoch": 0.8187599364069952,
            "step": 12360
        },
        {
            "loss": 0.4144,
            "grad_norm": 0.4188777208328247,
            "learning_rate": 3.6331920155449565e-05,
            "epoch": 0.820084790673026,
            "step": 12380
        },
        {
            "loss": 0.4059,
            "grad_norm": 3.093855619430542,
            "learning_rate": 3.630983925101572e-05,
            "epoch": 0.8214096449390567,
            "step": 12400
        },
        {
            "loss": 0.4012,
            "grad_norm": 1.6436429023742676,
            "learning_rate": 3.628775834658188e-05,
            "epoch": 0.8227344992050875,
            "step": 12420
        },
        {
            "loss": 0.4183,
            "grad_norm": 2.3675074577331543,
            "learning_rate": 3.626567744214803e-05,
            "epoch": 0.8240593534711181,
            "step": 12440
        },
        {
            "loss": 0.3469,
            "grad_norm": 1.468415379524231,
            "learning_rate": 3.624359653771418e-05,
            "epoch": 0.8253842077371489,
            "step": 12460
        },
        {
            "loss": 0.4084,
            "grad_norm": 2.2917215824127197,
            "learning_rate": 3.622151563328034e-05,
            "epoch": 0.8267090620031796,
            "step": 12480
        },
        {
            "loss": 0.3983,
            "grad_norm": 1.6452007293701172,
            "learning_rate": 3.619943472884649e-05,
            "epoch": 0.8280339162692104,
            "step": 12500
        },
        {
            "loss": 0.4827,
            "grad_norm": 0.6735281348228455,
            "learning_rate": 3.617735382441265e-05,
            "epoch": 0.8293587705352411,
            "step": 12520
        },
        {
            "loss": 0.4697,
            "grad_norm": 2.0847079753875732,
            "learning_rate": 3.615527291997881e-05,
            "epoch": 0.8306836248012719,
            "step": 12540
        },
        {
            "loss": 0.3949,
            "grad_norm": 0.6465516686439514,
            "learning_rate": 3.613319201554496e-05,
            "epoch": 0.8320084790673026,
            "step": 12560
        },
        {
            "loss": 0.3812,
            "grad_norm": 0.7169485092163086,
            "learning_rate": 3.611111111111111e-05,
            "epoch": 0.8333333333333334,
            "step": 12580
        },
        {
            "loss": 0.3682,
            "grad_norm": 2.0346338748931885,
            "learning_rate": 3.608903020667727e-05,
            "epoch": 0.834658187599364,
            "step": 12600
        },
        {
            "loss": 0.3638,
            "grad_norm": 2.4366626739501953,
            "learning_rate": 3.606694930224342e-05,
            "epoch": 0.8359830418653948,
            "step": 12620
        },
        {
            "loss": 0.4246,
            "grad_norm": 0.8171335458755493,
            "learning_rate": 3.6044868397809576e-05,
            "epoch": 0.8373078961314255,
            "step": 12640
        },
        {
            "loss": 0.3629,
            "grad_norm": 2.1920416355133057,
            "learning_rate": 3.6022787493375734e-05,
            "epoch": 0.8386327503974563,
            "step": 12660
        },
        {
            "loss": 0.3191,
            "grad_norm": 3.396686315536499,
            "learning_rate": 3.6000706588941885e-05,
            "epoch": 0.839957604663487,
            "step": 12680
        },
        {
            "loss": 0.4706,
            "grad_norm": 2.6623246669769287,
            "learning_rate": 3.5978625684508036e-05,
            "epoch": 0.8412824589295178,
            "step": 12700
        },
        {
            "loss": 0.3728,
            "grad_norm": 54.15888214111328,
            "learning_rate": 3.5956544780074194e-05,
            "epoch": 0.8426073131955485,
            "step": 12720
        },
        {
            "loss": 0.4034,
            "grad_norm": 1.417177677154541,
            "learning_rate": 3.593446387564035e-05,
            "epoch": 0.8439321674615792,
            "step": 12740
        },
        {
            "loss": 0.3939,
            "grad_norm": 1.5123122930526733,
            "learning_rate": 3.59123829712065e-05,
            "epoch": 0.84525702172761,
            "step": 12760
        },
        {
            "loss": 0.465,
            "grad_norm": 1.1228107213974,
            "learning_rate": 3.589030206677266e-05,
            "epoch": 0.8465818759936407,
            "step": 12780
        },
        {
            "loss": 0.3702,
            "grad_norm": 9.208948135375977,
            "learning_rate": 3.586822116233881e-05,
            "epoch": 0.8479067302596714,
            "step": 12800
        },
        {
            "loss": 0.311,
            "grad_norm": 0.8364163637161255,
            "learning_rate": 3.584614025790496e-05,
            "epoch": 0.8492315845257021,
            "step": 12820
        },
        {
            "loss": 0.4237,
            "grad_norm": 0.8326826691627502,
            "learning_rate": 3.582405935347112e-05,
            "epoch": 0.8505564387917329,
            "step": 12840
        },
        {
            "loss": 0.3511,
            "grad_norm": 4.486193656921387,
            "learning_rate": 3.580197844903728e-05,
            "epoch": 0.8518812930577636,
            "step": 12860
        },
        {
            "loss": 0.4222,
            "grad_norm": 51.19399642944336,
            "learning_rate": 3.577989754460343e-05,
            "epoch": 0.8532061473237944,
            "step": 12880
        },
        {
            "loss": 0.4564,
            "grad_norm": 1.6869735717773438,
            "learning_rate": 3.575781664016958e-05,
            "epoch": 0.8545310015898251,
            "step": 12900
        },
        {
            "loss": 0.3615,
            "grad_norm": 3.0998191833496094,
            "learning_rate": 3.573573573573574e-05,
            "epoch": 0.8558558558558559,
            "step": 12920
        },
        {
            "loss": 0.3753,
            "grad_norm": 1.4339478015899658,
            "learning_rate": 3.571365483130189e-05,
            "epoch": 0.8571807101218866,
            "step": 12940
        },
        {
            "loss": 0.415,
            "grad_norm": 0.9803837537765503,
            "learning_rate": 3.569157392686804e-05,
            "epoch": 0.8585055643879174,
            "step": 12960
        },
        {
            "loss": 0.4155,
            "grad_norm": 1.4379485845565796,
            "learning_rate": 3.5669493022434205e-05,
            "epoch": 0.859830418653948,
            "step": 12980
        },
        {
            "loss": 0.3735,
            "grad_norm": 2.2396228313446045,
            "learning_rate": 3.5647412118000356e-05,
            "epoch": 0.8611552729199788,
            "step": 13000
        },
        {
            "loss": 0.3326,
            "grad_norm": 0.8502287864685059,
            "learning_rate": 3.562533121356651e-05,
            "epoch": 0.8624801271860095,
            "step": 13020
        },
        {
            "loss": 0.3781,
            "grad_norm": 2.1364872455596924,
            "learning_rate": 3.5603250309132665e-05,
            "epoch": 0.8638049814520403,
            "step": 13040
        },
        {
            "loss": 0.3527,
            "grad_norm": 29.513376235961914,
            "learning_rate": 3.5581169404698816e-05,
            "epoch": 0.865129835718071,
            "step": 13060
        },
        {
            "loss": 0.4442,
            "grad_norm": 0.76409512758255,
            "learning_rate": 3.555908850026497e-05,
            "epoch": 0.8664546899841018,
            "step": 13080
        },
        {
            "loss": 0.4252,
            "grad_norm": 0.6204342246055603,
            "learning_rate": 3.553700759583113e-05,
            "epoch": 0.8677795442501325,
            "step": 13100
        },
        {
            "loss": 0.39,
            "grad_norm": 1.3631339073181152,
            "learning_rate": 3.551492669139728e-05,
            "epoch": 0.8691043985161632,
            "step": 13120
        },
        {
            "loss": 0.465,
            "grad_norm": 0.7009748220443726,
            "learning_rate": 3.5492845786963434e-05,
            "epoch": 0.870429252782194,
            "step": 13140
        },
        {
            "loss": 0.3636,
            "grad_norm": 2.185532569885254,
            "learning_rate": 3.547076488252959e-05,
            "epoch": 0.8717541070482246,
            "step": 13160
        },
        {
            "loss": 0.3945,
            "grad_norm": 4.524261951446533,
            "learning_rate": 3.544868397809574e-05,
            "epoch": 0.8730789613142554,
            "step": 13180
        },
        {
            "loss": 0.4292,
            "grad_norm": 1.6481841802597046,
            "learning_rate": 3.54266030736619e-05,
            "epoch": 0.8744038155802861,
            "step": 13200
        },
        {
            "loss": 0.4477,
            "grad_norm": 3.6381309032440186,
            "learning_rate": 3.540452216922805e-05,
            "epoch": 0.8757286698463169,
            "step": 13220
        },
        {
            "loss": 0.4447,
            "grad_norm": 1.2537288665771484,
            "learning_rate": 3.538244126479421e-05,
            "epoch": 0.8770535241123476,
            "step": 13240
        },
        {
            "loss": 0.4331,
            "grad_norm": 62.012996673583984,
            "learning_rate": 3.536036036036036e-05,
            "epoch": 0.8783783783783784,
            "step": 13260
        },
        {
            "loss": 0.3759,
            "grad_norm": 3.0037500858306885,
            "learning_rate": 3.533827945592652e-05,
            "epoch": 0.8797032326444091,
            "step": 13280
        },
        {
            "loss": 0.4041,
            "grad_norm": 2.9137184619903564,
            "learning_rate": 3.531619855149267e-05,
            "epoch": 0.8810280869104399,
            "step": 13300
        },
        {
            "loss": 0.3687,
            "grad_norm": 0.7079203128814697,
            "learning_rate": 3.529411764705883e-05,
            "epoch": 0.8823529411764706,
            "step": 13320
        },
        {
            "loss": 0.4315,
            "grad_norm": 1.7186387777328491,
            "learning_rate": 3.527203674262498e-05,
            "epoch": 0.8836777954425014,
            "step": 13340
        },
        {
            "loss": 0.4179,
            "grad_norm": 1.0465726852416992,
            "learning_rate": 3.5249955838191136e-05,
            "epoch": 0.885002649708532,
            "step": 13360
        },
        {
            "loss": 0.4379,
            "grad_norm": 1.957946538925171,
            "learning_rate": 3.522787493375729e-05,
            "epoch": 0.8863275039745628,
            "step": 13380
        },
        {
            "loss": 0.4006,
            "grad_norm": 6.254727840423584,
            "learning_rate": 3.520579402932344e-05,
            "epoch": 0.8876523582405935,
            "step": 13400
        },
        {
            "loss": 0.3006,
            "grad_norm": 26.245018005371094,
            "learning_rate": 3.5183713124889596e-05,
            "epoch": 0.8889772125066243,
            "step": 13420
        },
        {
            "loss": 0.3683,
            "grad_norm": 0.9231598377227783,
            "learning_rate": 3.5161632220455754e-05,
            "epoch": 0.890302066772655,
            "step": 13440
        },
        {
            "loss": 0.3557,
            "grad_norm": 1.1384422779083252,
            "learning_rate": 3.5139551316021905e-05,
            "epoch": 0.8916269210386858,
            "step": 13460
        },
        {
            "loss": 0.3836,
            "grad_norm": 2.4425346851348877,
            "learning_rate": 3.511747041158806e-05,
            "epoch": 0.8929517753047165,
            "step": 13480
        },
        {
            "loss": 0.401,
            "grad_norm": 43.24196243286133,
            "learning_rate": 3.5095389507154214e-05,
            "epoch": 0.8942766295707473,
            "step": 13500
        },
        {
            "loss": 0.4391,
            "grad_norm": 0.9639146327972412,
            "learning_rate": 3.5073308602720365e-05,
            "epoch": 0.895601483836778,
            "step": 13520
        },
        {
            "loss": 0.3974,
            "grad_norm": 0.867728054523468,
            "learning_rate": 3.505122769828653e-05,
            "epoch": 0.8969263381028086,
            "step": 13540
        },
        {
            "loss": 0.4813,
            "grad_norm": 1.8314261436462402,
            "learning_rate": 3.502914679385268e-05,
            "epoch": 0.8982511923688394,
            "step": 13560
        },
        {
            "loss": 0.3903,
            "grad_norm": 14.66704273223877,
            "learning_rate": 3.500706588941883e-05,
            "epoch": 0.8995760466348701,
            "step": 13580
        },
        {
            "loss": 0.3696,
            "grad_norm": 0.9060614109039307,
            "learning_rate": 3.498498498498499e-05,
            "epoch": 0.9009009009009009,
            "step": 13600
        },
        {
            "loss": 0.4453,
            "grad_norm": 0.8666896224021912,
            "learning_rate": 3.496290408055114e-05,
            "epoch": 0.9022257551669316,
            "step": 13620
        },
        {
            "loss": 0.4045,
            "grad_norm": 2.9417033195495605,
            "learning_rate": 3.494082317611729e-05,
            "epoch": 0.9035506094329624,
            "step": 13640
        },
        {
            "loss": 0.4131,
            "grad_norm": 0.3933759331703186,
            "learning_rate": 3.491874227168345e-05,
            "epoch": 0.9048754636989931,
            "step": 13660
        },
        {
            "loss": 0.4217,
            "grad_norm": 2.331101417541504,
            "learning_rate": 3.489666136724961e-05,
            "epoch": 0.9062003179650239,
            "step": 13680
        },
        {
            "loss": 0.3884,
            "grad_norm": 1.2394840717315674,
            "learning_rate": 3.487458046281576e-05,
            "epoch": 0.9075251722310546,
            "step": 13700
        },
        {
            "loss": 0.3362,
            "grad_norm": 50.3687858581543,
            "learning_rate": 3.485249955838191e-05,
            "epoch": 0.9088500264970854,
            "step": 13720
        },
        {
            "loss": 0.3621,
            "grad_norm": 4.651730537414551,
            "learning_rate": 3.483041865394807e-05,
            "epoch": 0.910174880763116,
            "step": 13740
        },
        {
            "loss": 0.4349,
            "grad_norm": 0.8781138062477112,
            "learning_rate": 3.480833774951422e-05,
            "epoch": 0.9114997350291468,
            "step": 13760
        },
        {
            "loss": 0.4338,
            "grad_norm": 1.3556420803070068,
            "learning_rate": 3.4786256845080376e-05,
            "epoch": 0.9128245892951775,
            "step": 13780
        },
        {
            "loss": 0.4184,
            "grad_norm": 1.9649951457977295,
            "learning_rate": 3.4764175940646534e-05,
            "epoch": 0.9141494435612083,
            "step": 13800
        },
        {
            "loss": 0.414,
            "grad_norm": 1.4959685802459717,
            "learning_rate": 3.4742095036212685e-05,
            "epoch": 0.915474297827239,
            "step": 13820
        },
        {
            "loss": 0.3634,
            "grad_norm": 23.553720474243164,
            "learning_rate": 3.4720014131778836e-05,
            "epoch": 0.9167991520932698,
            "step": 13840
        },
        {
            "loss": 0.3554,
            "grad_norm": 2.0591816902160645,
            "learning_rate": 3.4697933227344994e-05,
            "epoch": 0.9181240063593005,
            "step": 13860
        },
        {
            "loss": 0.4201,
            "grad_norm": 2.147113561630249,
            "learning_rate": 3.4675852322911145e-05,
            "epoch": 0.9194488606253313,
            "step": 13880
        },
        {
            "loss": 0.4716,
            "grad_norm": 1.14065682888031,
            "learning_rate": 3.46537714184773e-05,
            "epoch": 0.920773714891362,
            "step": 13900
        },
        {
            "loss": 0.378,
            "grad_norm": 1.0037779808044434,
            "learning_rate": 3.463169051404346e-05,
            "epoch": 0.9220985691573926,
            "step": 13920
        },
        {
            "loss": 0.4462,
            "grad_norm": 23.044940948486328,
            "learning_rate": 3.460960960960961e-05,
            "epoch": 0.9234234234234234,
            "step": 13940
        },
        {
            "loss": 0.3903,
            "grad_norm": 0.9797386527061462,
            "learning_rate": 3.458752870517576e-05,
            "epoch": 0.9247482776894541,
            "step": 13960
        },
        {
            "loss": 0.3858,
            "grad_norm": 0.4562150835990906,
            "learning_rate": 3.456544780074192e-05,
            "epoch": 0.9260731319554849,
            "step": 13980
        },
        {
            "loss": 0.3487,
            "grad_norm": 2.0218613147735596,
            "learning_rate": 3.454336689630808e-05,
            "epoch": 0.9273979862215156,
            "step": 14000
        },
        {
            "loss": 0.4133,
            "grad_norm": 4.451293468475342,
            "learning_rate": 3.452128599187423e-05,
            "epoch": 0.9287228404875464,
            "step": 14020
        },
        {
            "loss": 0.3696,
            "grad_norm": 1.9265259504318237,
            "learning_rate": 3.449920508744039e-05,
            "epoch": 0.9300476947535771,
            "step": 14040
        },
        {
            "loss": 0.3857,
            "grad_norm": 4.769433498382568,
            "learning_rate": 3.447712418300654e-05,
            "epoch": 0.9313725490196079,
            "step": 14060
        },
        {
            "loss": 0.3269,
            "grad_norm": 3.6348323822021484,
            "learning_rate": 3.445504327857269e-05,
            "epoch": 0.9326974032856385,
            "step": 14080
        },
        {
            "loss": 0.3787,
            "grad_norm": 2.0523602962493896,
            "learning_rate": 3.443296237413885e-05,
            "epoch": 0.9340222575516693,
            "step": 14100
        },
        {
            "loss": 0.3712,
            "grad_norm": 0.9872114062309265,
            "learning_rate": 3.4410881469705005e-05,
            "epoch": 0.9353471118177,
            "step": 14120
        },
        {
            "loss": 0.3973,
            "grad_norm": 3.000608444213867,
            "learning_rate": 3.4388800565271156e-05,
            "epoch": 0.9366719660837308,
            "step": 14140
        },
        {
            "loss": 0.4057,
            "grad_norm": 1.6643218994140625,
            "learning_rate": 3.436671966083731e-05,
            "epoch": 0.9379968203497615,
            "step": 14160
        },
        {
            "loss": 0.3738,
            "grad_norm": 0.4420872926712036,
            "learning_rate": 3.4344638756403465e-05,
            "epoch": 0.9393216746157923,
            "step": 14180
        },
        {
            "loss": 0.4143,
            "grad_norm": 0.5126674175262451,
            "learning_rate": 3.4322557851969616e-05,
            "epoch": 0.940646528881823,
            "step": 14200
        },
        {
            "loss": 0.3744,
            "grad_norm": 1.4026274681091309,
            "learning_rate": 3.430047694753577e-05,
            "epoch": 0.9419713831478538,
            "step": 14220
        },
        {
            "loss": 0.4293,
            "grad_norm": 1.8735486268997192,
            "learning_rate": 3.427839604310193e-05,
            "epoch": 0.9432962374138845,
            "step": 14240
        },
        {
            "loss": 0.3654,
            "grad_norm": 1.3962479829788208,
            "learning_rate": 3.425631513866808e-05,
            "epoch": 0.9446210916799153,
            "step": 14260
        },
        {
            "loss": 0.3786,
            "grad_norm": 2.9949944019317627,
            "learning_rate": 3.4234234234234234e-05,
            "epoch": 0.9459459459459459,
            "step": 14280
        },
        {
            "loss": 0.4486,
            "grad_norm": 0.8643149733543396,
            "learning_rate": 3.421215332980039e-05,
            "epoch": 0.9472708002119767,
            "step": 14300
        },
        {
            "loss": 0.3827,
            "grad_norm": 3.8442366123199463,
            "learning_rate": 3.419007242536654e-05,
            "epoch": 0.9485956544780074,
            "step": 14320
        },
        {
            "loss": 0.4039,
            "grad_norm": 2.9391591548919678,
            "learning_rate": 3.41679915209327e-05,
            "epoch": 0.9499205087440381,
            "step": 14340
        },
        {
            "loss": 0.3364,
            "grad_norm": 1.145455002784729,
            "learning_rate": 3.414591061649886e-05,
            "epoch": 0.9512453630100689,
            "step": 14360
        },
        {
            "loss": 0.3607,
            "grad_norm": 0.7567086219787598,
            "learning_rate": 3.412382971206501e-05,
            "epoch": 0.9525702172760996,
            "step": 14380
        },
        {
            "loss": 0.4127,
            "grad_norm": 1.495760202407837,
            "learning_rate": 3.410174880763116e-05,
            "epoch": 0.9538950715421304,
            "step": 14400
        },
        {
            "loss": 0.5251,
            "grad_norm": 3.7847812175750732,
            "learning_rate": 3.407966790319732e-05,
            "epoch": 0.9552199258081611,
            "step": 14420
        },
        {
            "loss": 0.4116,
            "grad_norm": 5.817490577697754,
            "learning_rate": 3.405758699876347e-05,
            "epoch": 0.9565447800741919,
            "step": 14440
        },
        {
            "loss": 0.3491,
            "grad_norm": 58.841983795166016,
            "learning_rate": 3.403550609432963e-05,
            "epoch": 0.9578696343402225,
            "step": 14460
        },
        {
            "loss": 0.3989,
            "grad_norm": 1.5170505046844482,
            "learning_rate": 3.401342518989578e-05,
            "epoch": 0.9591944886062533,
            "step": 14480
        },
        {
            "loss": 0.4147,
            "grad_norm": 1.135764718055725,
            "learning_rate": 3.3991344285461936e-05,
            "epoch": 0.960519342872284,
            "step": 14500
        },
        {
            "loss": 0.4032,
            "grad_norm": 0.7729544639587402,
            "learning_rate": 3.396926338102809e-05,
            "epoch": 0.9618441971383148,
            "step": 14520
        },
        {
            "loss": 0.365,
            "grad_norm": 1.0893915891647339,
            "learning_rate": 3.3947182476594245e-05,
            "epoch": 0.9631690514043455,
            "step": 14540
        },
        {
            "loss": 0.4095,
            "grad_norm": 1.7745139598846436,
            "learning_rate": 3.3925101572160396e-05,
            "epoch": 0.9644939056703763,
            "step": 14560
        },
        {
            "loss": 0.3567,
            "grad_norm": 2.1875557899475098,
            "learning_rate": 3.3903020667726554e-05,
            "epoch": 0.965818759936407,
            "step": 14580
        },
        {
            "loss": 0.4238,
            "grad_norm": 0.7284694314002991,
            "learning_rate": 3.3880939763292705e-05,
            "epoch": 0.9671436142024378,
            "step": 14600
        },
        {
            "loss": 0.3512,
            "grad_norm": 2.649780750274658,
            "learning_rate": 3.385885885885886e-05,
            "epoch": 0.9684684684684685,
            "step": 14620
        },
        {
            "loss": 0.3594,
            "grad_norm": 4.582192897796631,
            "learning_rate": 3.3836777954425014e-05,
            "epoch": 0.9697933227344993,
            "step": 14640
        },
        {
            "loss": 0.3915,
            "grad_norm": 40.822574615478516,
            "learning_rate": 3.3814697049991165e-05,
            "epoch": 0.9711181770005299,
            "step": 14660
        },
        {
            "loss": 0.3408,
            "grad_norm": 1.3630831241607666,
            "learning_rate": 3.379261614555732e-05,
            "epoch": 0.9724430312665607,
            "step": 14680
        },
        {
            "loss": 0.4129,
            "grad_norm": 1.129650592803955,
            "learning_rate": 3.377053524112348e-05,
            "epoch": 0.9737678855325914,
            "step": 14700
        },
        {
            "loss": 0.4026,
            "grad_norm": 1.126085877418518,
            "learning_rate": 3.374845433668963e-05,
            "epoch": 0.9750927397986221,
            "step": 14720
        },
        {
            "loss": 0.404,
            "grad_norm": 11.310118675231934,
            "learning_rate": 3.372637343225579e-05,
            "epoch": 0.9764175940646529,
            "step": 14740
        },
        {
            "loss": 0.3946,
            "grad_norm": 1.1573604345321655,
            "learning_rate": 3.370429252782194e-05,
            "epoch": 0.9777424483306836,
            "step": 14760
        },
        {
            "loss": 0.3787,
            "grad_norm": 1.1047282218933105,
            "learning_rate": 3.368221162338809e-05,
            "epoch": 0.9790673025967144,
            "step": 14780
        },
        {
            "loss": 0.4038,
            "grad_norm": 1.3082364797592163,
            "learning_rate": 3.366013071895425e-05,
            "epoch": 0.9803921568627451,
            "step": 14800
        },
        {
            "loss": 0.3582,
            "grad_norm": 3.287889242172241,
            "learning_rate": 3.363804981452041e-05,
            "epoch": 0.9817170111287759,
            "step": 14820
        },
        {
            "loss": 0.4161,
            "grad_norm": 3.5402932167053223,
            "learning_rate": 3.361596891008656e-05,
            "epoch": 0.9830418653948065,
            "step": 14840
        },
        {
            "loss": 0.4201,
            "grad_norm": 3.2483582496643066,
            "learning_rate": 3.3593888005652716e-05,
            "epoch": 0.9843667196608373,
            "step": 14860
        },
        {
            "loss": 0.4014,
            "grad_norm": 1.1748805046081543,
            "learning_rate": 3.357180710121887e-05,
            "epoch": 0.985691573926868,
            "step": 14880
        },
        {
            "loss": 0.5104,
            "grad_norm": 0.9221141934394836,
            "learning_rate": 3.354972619678502e-05,
            "epoch": 0.9870164281928988,
            "step": 14900
        },
        {
            "loss": 0.4448,
            "grad_norm": 3.796844720840454,
            "learning_rate": 3.3527645292351176e-05,
            "epoch": 0.9883412824589295,
            "step": 14920
        },
        {
            "loss": 0.3942,
            "grad_norm": 3.4403903484344482,
            "learning_rate": 3.3505564387917334e-05,
            "epoch": 0.9896661367249603,
            "step": 14940
        },
        {
            "loss": 0.3462,
            "grad_norm": 4.034333229064941,
            "learning_rate": 3.3483483483483485e-05,
            "epoch": 0.990990990990991,
            "step": 14960
        },
        {
            "loss": 0.4217,
            "grad_norm": 1.485363245010376,
            "learning_rate": 3.3461402579049636e-05,
            "epoch": 0.9923158452570218,
            "step": 14980
        },
        {
            "loss": 0.3982,
            "grad_norm": 0.9858248829841614,
            "learning_rate": 3.3439321674615794e-05,
            "epoch": 0.9936406995230525,
            "step": 15000
        },
        {
            "loss": 0.4082,
            "grad_norm": 1.0606024265289307,
            "learning_rate": 3.3417240770181945e-05,
            "epoch": 0.9949655537890832,
            "step": 15020
        },
        {
            "loss": 0.4526,
            "grad_norm": 40.68191909790039,
            "learning_rate": 3.33951598657481e-05,
            "epoch": 0.9962904080551139,
            "step": 15040
        },
        {
            "loss": 0.4675,
            "grad_norm": 1.9881953001022339,
            "learning_rate": 3.337307896131426e-05,
            "epoch": 0.9976152623211447,
            "step": 15060
        },
        {
            "loss": 0.4944,
            "grad_norm": 1.2092089653015137,
            "learning_rate": 3.335099805688041e-05,
            "epoch": 0.9989401165871754,
            "step": 15080
        },
        {
            "eval_loss": 0.3965265154838562,
            "eval_runtime": 112.3748,
            "eval_samples_per_second": 268.672,
            "eval_steps_per_second": 33.584,
            "epoch": 1.0,
            "step": 15096
        },
        {
            "loss": 0.3973,
            "grad_norm": 0.4511980414390564,
            "learning_rate": 3.332891715244656e-05,
            "epoch": 1.000264970853206,
            "step": 15100
        },
        {
            "loss": 0.4215,
            "grad_norm": 1.1571288108825684,
            "learning_rate": 3.330683624801272e-05,
            "epoch": 1.0015898251192368,
            "step": 15120
        },
        {
            "loss": 0.3741,
            "grad_norm": 3.071228265762329,
            "learning_rate": 3.328475534357888e-05,
            "epoch": 1.0029146793852677,
            "step": 15140
        },
        {
            "loss": 0.3808,
            "grad_norm": 1.4205573797225952,
            "learning_rate": 3.326267443914503e-05,
            "epoch": 1.0042395336512984,
            "step": 15160
        },
        {
            "loss": 0.3863,
            "grad_norm": 1.3311288356781006,
            "learning_rate": 3.324059353471119e-05,
            "epoch": 1.005564387917329,
            "step": 15180
        },
        {
            "loss": 0.4079,
            "grad_norm": 0.8495075106620789,
            "learning_rate": 3.321851263027734e-05,
            "epoch": 1.0068892421833597,
            "step": 15200
        },
        {
            "loss": 0.4443,
            "grad_norm": 9.966660499572754,
            "learning_rate": 3.319643172584349e-05,
            "epoch": 1.0082140964493906,
            "step": 15220
        },
        {
            "loss": 0.3893,
            "grad_norm": 1.0639125108718872,
            "learning_rate": 3.317435082140965e-05,
            "epoch": 1.0095389507154213,
            "step": 15240
        },
        {
            "loss": 0.444,
            "grad_norm": 2.02597713470459,
            "learning_rate": 3.3152269916975805e-05,
            "epoch": 1.010863804981452,
            "step": 15260
        },
        {
            "loss": 0.4127,
            "grad_norm": 1.8262232542037964,
            "learning_rate": 3.3130189012541956e-05,
            "epoch": 1.0121886592474827,
            "step": 15280
        },
        {
            "loss": 0.4298,
            "grad_norm": 1.1148507595062256,
            "learning_rate": 3.310810810810811e-05,
            "epoch": 1.0135135135135136,
            "step": 15300
        },
        {
            "loss": 0.3466,
            "grad_norm": 0.8326893448829651,
            "learning_rate": 3.3086027203674265e-05,
            "epoch": 1.0148383677795443,
            "step": 15320
        },
        {
            "loss": 0.4148,
            "grad_norm": 2.2753264904022217,
            "learning_rate": 3.3063946299240416e-05,
            "epoch": 1.016163222045575,
            "step": 15340
        },
        {
            "loss": 0.4012,
            "grad_norm": 1.92824125289917,
            "learning_rate": 3.3041865394806574e-05,
            "epoch": 1.0174880763116056,
            "step": 15360
        },
        {
            "loss": 0.4163,
            "grad_norm": 1.1118069887161255,
            "learning_rate": 3.301978449037273e-05,
            "epoch": 1.0188129305776366,
            "step": 15380
        },
        {
            "loss": 0.369,
            "grad_norm": 1.8998081684112549,
            "learning_rate": 3.299770358593888e-05,
            "epoch": 1.0201377848436672,
            "step": 15400
        },
        {
            "loss": 0.4092,
            "grad_norm": 0.6560434103012085,
            "learning_rate": 3.2975622681505034e-05,
            "epoch": 1.021462639109698,
            "step": 15420
        },
        {
            "loss": 0.4077,
            "grad_norm": 2.0905463695526123,
            "learning_rate": 3.295354177707119e-05,
            "epoch": 1.0227874933757286,
            "step": 15440
        },
        {
            "loss": 0.3705,
            "grad_norm": 1.6900635957717896,
            "learning_rate": 3.293146087263734e-05,
            "epoch": 1.0241123476417595,
            "step": 15460
        },
        {
            "loss": 0.4474,
            "grad_norm": 2.9460160732269287,
            "learning_rate": 3.2909379968203493e-05,
            "epoch": 1.0254372019077902,
            "step": 15480
        },
        {
            "loss": 0.3719,
            "grad_norm": 29.130043029785156,
            "learning_rate": 3.288729906376966e-05,
            "epoch": 1.0267620561738209,
            "step": 15500
        },
        {
            "loss": 0.3495,
            "grad_norm": 1.2658334970474243,
            "learning_rate": 3.286521815933581e-05,
            "epoch": 1.0280869104398516,
            "step": 15520
        },
        {
            "loss": 0.4018,
            "grad_norm": 2.8106369972229004,
            "learning_rate": 3.284313725490196e-05,
            "epoch": 1.0294117647058822,
            "step": 15540
        },
        {
            "loss": 0.3353,
            "grad_norm": 0.9363091588020325,
            "learning_rate": 3.282105635046812e-05,
            "epoch": 1.0307366189719132,
            "step": 15560
        },
        {
            "loss": 0.3777,
            "grad_norm": 3.3323349952697754,
            "learning_rate": 3.279897544603427e-05,
            "epoch": 1.0320614732379438,
            "step": 15580
        },
        {
            "loss": 0.4971,
            "grad_norm": 2.612884998321533,
            "learning_rate": 3.277689454160043e-05,
            "epoch": 1.0333863275039745,
            "step": 15600
        },
        {
            "loss": 0.4217,
            "grad_norm": 1.5054121017456055,
            "learning_rate": 3.2754813637166585e-05,
            "epoch": 1.0347111817700052,
            "step": 15620
        },
        {
            "loss": 0.4299,
            "grad_norm": 0.6906440854072571,
            "learning_rate": 3.2732732732732736e-05,
            "epoch": 1.0360360360360361,
            "step": 15640
        },
        {
            "loss": 0.3649,
            "grad_norm": 2.1900436878204346,
            "learning_rate": 3.271065182829889e-05,
            "epoch": 1.0373608903020668,
            "step": 15660
        },
        {
            "loss": 0.3584,
            "grad_norm": 0.9373571872711182,
            "learning_rate": 3.2688570923865045e-05,
            "epoch": 1.0386857445680975,
            "step": 15680
        },
        {
            "loss": 0.3955,
            "grad_norm": 5.443979263305664,
            "learning_rate": 3.2666490019431196e-05,
            "epoch": 1.0400105988341282,
            "step": 15700
        },
        {
            "loss": 0.3853,
            "grad_norm": 0.530116617679596,
            "learning_rate": 3.2644409114997354e-05,
            "epoch": 1.041335453100159,
            "step": 15720
        },
        {
            "loss": 0.4089,
            "grad_norm": 0.8855524063110352,
            "learning_rate": 3.2622328210563505e-05,
            "epoch": 1.0426603073661898,
            "step": 15740
        },
        {
            "loss": 0.5249,
            "grad_norm": 0.6364471912384033,
            "learning_rate": 3.260024730612966e-05,
            "epoch": 1.0439851616322204,
            "step": 15760
        },
        {
            "loss": 0.3751,
            "grad_norm": 0.8302748799324036,
            "learning_rate": 3.2578166401695813e-05,
            "epoch": 1.0453100158982511,
            "step": 15780
        },
        {
            "loss": 0.3428,
            "grad_norm": 0.5907255411148071,
            "learning_rate": 3.2556085497261965e-05,
            "epoch": 1.046634870164282,
            "step": 15800
        },
        {
            "loss": 0.3802,
            "grad_norm": 0.669072151184082,
            "learning_rate": 3.253400459282812e-05,
            "epoch": 1.0479597244303127,
            "step": 15820
        },
        {
            "loss": 0.375,
            "grad_norm": 1.3503618240356445,
            "learning_rate": 3.251192368839428e-05,
            "epoch": 1.0492845786963434,
            "step": 15840
        },
        {
            "loss": 0.4301,
            "grad_norm": 1.4042085409164429,
            "learning_rate": 3.248984278396043e-05,
            "epoch": 1.050609432962374,
            "step": 15860
        },
        {
            "loss": 0.2956,
            "grad_norm": 2.9891462326049805,
            "learning_rate": 3.246776187952659e-05,
            "epoch": 1.0519342872284048,
            "step": 15880
        },
        {
            "loss": 0.4408,
            "grad_norm": 2.0179941654205322,
            "learning_rate": 3.244568097509274e-05,
            "epoch": 1.0532591414944357,
            "step": 15900
        },
        {
            "loss": 0.3692,
            "grad_norm": 2.699110984802246,
            "learning_rate": 3.242360007065889e-05,
            "epoch": 1.0545839957604664,
            "step": 15920
        },
        {
            "loss": 0.4371,
            "grad_norm": 3.337653160095215,
            "learning_rate": 3.2401519166225056e-05,
            "epoch": 1.055908850026497,
            "step": 15940
        },
        {
            "loss": 0.3258,
            "grad_norm": 1.7457249164581299,
            "learning_rate": 3.237943826179121e-05,
            "epoch": 1.0572337042925277,
            "step": 15960
        },
        {
            "loss": 0.3379,
            "grad_norm": 1.8404901027679443,
            "learning_rate": 3.235735735735736e-05,
            "epoch": 1.0585585585585586,
            "step": 15980
        },
        {
            "loss": 0.3649,
            "grad_norm": 3.803392171859741,
            "learning_rate": 3.2335276452923516e-05,
            "epoch": 1.0598834128245893,
            "step": 16000
        },
        {
            "loss": 0.3646,
            "grad_norm": 0.8683737516403198,
            "learning_rate": 3.231319554848967e-05,
            "epoch": 1.06120826709062,
            "step": 16020
        },
        {
            "loss": 0.4005,
            "grad_norm": 17.861631393432617,
            "learning_rate": 3.229111464405582e-05,
            "epoch": 1.0625331213566507,
            "step": 16040
        },
        {
            "loss": 0.3937,
            "grad_norm": 16.955827713012695,
            "learning_rate": 3.2269033739621976e-05,
            "epoch": 1.0638579756226816,
            "step": 16060
        },
        {
            "loss": 0.3761,
            "grad_norm": 1.2855874300003052,
            "learning_rate": 3.2246952835188134e-05,
            "epoch": 1.0651828298887123,
            "step": 16080
        },
        {
            "loss": 0.3777,
            "grad_norm": 0.7954356074333191,
            "learning_rate": 3.2224871930754285e-05,
            "epoch": 1.066507684154743,
            "step": 16100
        },
        {
            "loss": 0.4029,
            "grad_norm": 1.5053904056549072,
            "learning_rate": 3.220279102632044e-05,
            "epoch": 1.0678325384207736,
            "step": 16120
        },
        {
            "loss": 0.4136,
            "grad_norm": 1.3525866270065308,
            "learning_rate": 3.2180710121886593e-05,
            "epoch": 1.0691573926868045,
            "step": 16140
        },
        {
            "loss": 0.3471,
            "grad_norm": 1.1911674737930298,
            "learning_rate": 3.2158629217452744e-05,
            "epoch": 1.0704822469528352,
            "step": 16160
        },
        {
            "loss": 0.3806,
            "grad_norm": 0.9794151782989502,
            "learning_rate": 3.21365483130189e-05,
            "epoch": 1.071807101218866,
            "step": 16180
        },
        {
            "loss": 0.4247,
            "grad_norm": 1.2445369958877563,
            "learning_rate": 3.211446740858506e-05,
            "epoch": 1.0731319554848966,
            "step": 16200
        },
        {
            "loss": 0.3779,
            "grad_norm": 0.7079989314079285,
            "learning_rate": 3.209238650415121e-05,
            "epoch": 1.0744568097509275,
            "step": 16220
        },
        {
            "loss": 0.3364,
            "grad_norm": 0.6972644925117493,
            "learning_rate": 3.207030559971736e-05,
            "epoch": 1.0757816640169582,
            "step": 16240
        },
        {
            "loss": 0.3449,
            "grad_norm": 2.6788980960845947,
            "learning_rate": 3.204822469528352e-05,
            "epoch": 1.0771065182829889,
            "step": 16260
        },
        {
            "loss": 0.4541,
            "grad_norm": 1.8285890817642212,
            "learning_rate": 3.202614379084967e-05,
            "epoch": 1.0784313725490196,
            "step": 16280
        },
        {
            "loss": 0.3462,
            "grad_norm": 0.9538251757621765,
            "learning_rate": 3.200406288641583e-05,
            "epoch": 1.0797562268150505,
            "step": 16300
        },
        {
            "loss": 0.3468,
            "grad_norm": 2.2294235229492188,
            "learning_rate": 3.198198198198199e-05,
            "epoch": 1.0810810810810811,
            "step": 16320
        },
        {
            "loss": 0.3947,
            "grad_norm": 0.7521061897277832,
            "learning_rate": 3.195990107754814e-05,
            "epoch": 1.0824059353471118,
            "step": 16340
        },
        {
            "loss": 0.3226,
            "grad_norm": 0.6063995957374573,
            "learning_rate": 3.193782017311429e-05,
            "epoch": 1.0837307896131425,
            "step": 16360
        },
        {
            "loss": 0.3229,
            "grad_norm": 0.7337347865104675,
            "learning_rate": 3.191573926868045e-05,
            "epoch": 1.0850556438791732,
            "step": 16380
        },
        {
            "loss": 0.4118,
            "grad_norm": 2.6341028213500977,
            "learning_rate": 3.1893658364246605e-05,
            "epoch": 1.086380498145204,
            "step": 16400
        },
        {
            "loss": 0.4145,
            "grad_norm": 6.064100742340088,
            "learning_rate": 3.1871577459812756e-05,
            "epoch": 1.0877053524112348,
            "step": 16420
        },
        {
            "loss": 0.3759,
            "grad_norm": 0.9877030849456787,
            "learning_rate": 3.1849496555378913e-05,
            "epoch": 1.0890302066772655,
            "step": 16440
        },
        {
            "loss": 0.3464,
            "grad_norm": 0.6480843424797058,
            "learning_rate": 3.1827415650945065e-05,
            "epoch": 1.0903550609432962,
            "step": 16460
        },
        {
            "loss": 0.3888,
            "grad_norm": 4.5911078453063965,
            "learning_rate": 3.1805334746511216e-05,
            "epoch": 1.091679915209327,
            "step": 16480
        },
        {
            "loss": 0.4078,
            "grad_norm": 1.3818981647491455,
            "learning_rate": 3.1783253842077373e-05,
            "epoch": 1.0930047694753577,
            "step": 16500
        },
        {
            "loss": 0.3684,
            "grad_norm": 2.6999258995056152,
            "learning_rate": 3.176117293764353e-05,
            "epoch": 1.0943296237413884,
            "step": 16520
        },
        {
            "loss": 0.3489,
            "grad_norm": 3.258831262588501,
            "learning_rate": 3.173909203320968e-05,
            "epoch": 1.095654478007419,
            "step": 16540
        },
        {
            "loss": 0.405,
            "grad_norm": 1.1630178689956665,
            "learning_rate": 3.171701112877583e-05,
            "epoch": 1.09697933227345,
            "step": 16560
        },
        {
            "loss": 0.3782,
            "grad_norm": 1.8534067869186401,
            "learning_rate": 3.169493022434199e-05,
            "epoch": 1.0983041865394807,
            "step": 16580
        },
        {
            "loss": 0.3854,
            "grad_norm": 0.9848254919052124,
            "learning_rate": 3.167284931990814e-05,
            "epoch": 1.0996290408055114,
            "step": 16600
        },
        {
            "loss": 0.3954,
            "grad_norm": 1.7740209102630615,
            "learning_rate": 3.16507684154743e-05,
            "epoch": 1.100953895071542,
            "step": 16620
        },
        {
            "loss": 0.4023,
            "grad_norm": 1.9467343091964722,
            "learning_rate": 3.162868751104046e-05,
            "epoch": 1.102278749337573,
            "step": 16640
        },
        {
            "loss": 0.4267,
            "grad_norm": 0.7543272376060486,
            "learning_rate": 3.160660660660661e-05,
            "epoch": 1.1036036036036037,
            "step": 16660
        },
        {
            "loss": 0.4377,
            "grad_norm": 2.7099928855895996,
            "learning_rate": 3.158452570217276e-05,
            "epoch": 1.1049284578696343,
            "step": 16680
        },
        {
            "loss": 0.3949,
            "grad_norm": 1.5117257833480835,
            "learning_rate": 3.156244479773892e-05,
            "epoch": 1.106253312135665,
            "step": 16700
        },
        {
            "loss": 0.3878,
            "grad_norm": 1.9132051467895508,
            "learning_rate": 3.154036389330507e-05,
            "epoch": 1.1075781664016957,
            "step": 16720
        },
        {
            "loss": 0.4325,
            "grad_norm": 2.637817621231079,
            "learning_rate": 3.151828298887123e-05,
            "epoch": 1.1089030206677266,
            "step": 16740
        },
        {
            "loss": 0.4015,
            "grad_norm": 1.3958120346069336,
            "learning_rate": 3.1496202084437385e-05,
            "epoch": 1.1102278749337573,
            "step": 16760
        },
        {
            "loss": 0.3961,
            "grad_norm": 0.9473470449447632,
            "learning_rate": 3.1474121180003536e-05,
            "epoch": 1.111552729199788,
            "step": 16780
        },
        {
            "loss": 0.4357,
            "grad_norm": 3.782499074935913,
            "learning_rate": 3.145204027556969e-05,
            "epoch": 1.1128775834658187,
            "step": 16800
        },
        {
            "loss": 0.3843,
            "grad_norm": 3.028151035308838,
            "learning_rate": 3.1429959371135844e-05,
            "epoch": 1.1142024377318496,
            "step": 16820
        },
        {
            "loss": 0.3846,
            "grad_norm": 3.2054827213287354,
            "learning_rate": 3.1407878466701996e-05,
            "epoch": 1.1155272919978803,
            "step": 16840
        },
        {
            "loss": 0.3806,
            "grad_norm": 3.7840723991394043,
            "learning_rate": 3.138579756226815e-05,
            "epoch": 1.116852146263911,
            "step": 16860
        },
        {
            "loss": 0.3432,
            "grad_norm": 1.0319435596466064,
            "learning_rate": 3.1363716657834304e-05,
            "epoch": 1.1181770005299416,
            "step": 16880
        },
        {
            "loss": 0.3741,
            "grad_norm": 1.96378493309021,
            "learning_rate": 3.134163575340046e-05,
            "epoch": 1.1195018547959725,
            "step": 16900
        },
        {
            "loss": 0.3871,
            "grad_norm": 23.897417068481445,
            "learning_rate": 3.131955484896661e-05,
            "epoch": 1.1208267090620032,
            "step": 16920
        },
        {
            "loss": 0.4035,
            "grad_norm": 1.99395751953125,
            "learning_rate": 3.129747394453277e-05,
            "epoch": 1.122151563328034,
            "step": 16940
        },
        {
            "loss": 0.3953,
            "grad_norm": 1.9194985628128052,
            "learning_rate": 3.127539304009892e-05,
            "epoch": 1.1234764175940646,
            "step": 16960
        },
        {
            "loss": 0.3821,
            "grad_norm": 13.28632640838623,
            "learning_rate": 3.125331213566508e-05,
            "epoch": 1.1248012718600955,
            "step": 16980
        },
        {
            "loss": 0.3729,
            "grad_norm": 0.8434250354766846,
            "learning_rate": 3.123123123123123e-05,
            "epoch": 1.1261261261261262,
            "step": 17000
        },
        {
            "loss": 0.3743,
            "grad_norm": 1.610455870628357,
            "learning_rate": 3.120915032679739e-05,
            "epoch": 1.1274509803921569,
            "step": 17020
        },
        {
            "loss": 0.4238,
            "grad_norm": 0.8982141613960266,
            "learning_rate": 3.118706942236354e-05,
            "epoch": 1.1287758346581875,
            "step": 17040
        },
        {
            "loss": 0.3964,
            "grad_norm": 2.1738054752349854,
            "learning_rate": 3.116498851792969e-05,
            "epoch": 1.1301006889242182,
            "step": 17060
        },
        {
            "loss": 0.4301,
            "grad_norm": 2.4998183250427246,
            "learning_rate": 3.114290761349585e-05,
            "epoch": 1.1314255431902491,
            "step": 17080
        },
        {
            "loss": 0.3873,
            "grad_norm": 2.020318031311035,
            "learning_rate": 3.112082670906201e-05,
            "epoch": 1.1327503974562798,
            "step": 17100
        },
        {
            "loss": 0.3525,
            "grad_norm": 1.2531973123550415,
            "learning_rate": 3.109874580462816e-05,
            "epoch": 1.1340752517223105,
            "step": 17120
        },
        {
            "loss": 0.3409,
            "grad_norm": 7.921764373779297,
            "learning_rate": 3.1076664900194316e-05,
            "epoch": 1.1354001059883414,
            "step": 17140
        },
        {
            "loss": 0.3833,
            "grad_norm": 39.7625617980957,
            "learning_rate": 3.105458399576047e-05,
            "epoch": 1.136724960254372,
            "step": 17160
        },
        {
            "loss": 0.3551,
            "grad_norm": 0.6640488505363464,
            "learning_rate": 3.103250309132662e-05,
            "epoch": 1.1380498145204028,
            "step": 17180
        },
        {
            "loss": 0.4052,
            "grad_norm": 1.0208420753479004,
            "learning_rate": 3.101042218689278e-05,
            "epoch": 1.1393746687864335,
            "step": 17200
        },
        {
            "loss": 0.377,
            "grad_norm": 2.864675998687744,
            "learning_rate": 3.098834128245893e-05,
            "epoch": 1.1406995230524641,
            "step": 17220
        },
        {
            "loss": 0.4539,
            "grad_norm": 1.9204689264297485,
            "learning_rate": 3.0966260378025084e-05,
            "epoch": 1.142024377318495,
            "step": 17240
        },
        {
            "loss": 0.3421,
            "grad_norm": 0.7679526805877686,
            "learning_rate": 3.094417947359124e-05,
            "epoch": 1.1433492315845257,
            "step": 17260
        },
        {
            "loss": 0.335,
            "grad_norm": 1.1566344499588013,
            "learning_rate": 3.092209856915739e-05,
            "epoch": 1.1446740858505564,
            "step": 17280
        },
        {
            "loss": 0.4108,
            "grad_norm": 0.7216314673423767,
            "learning_rate": 3.0900017664723544e-05,
            "epoch": 1.145998940116587,
            "step": 17300
        },
        {
            "loss": 0.3842,
            "grad_norm": 2.4672281742095947,
            "learning_rate": 3.08779367602897e-05,
            "epoch": 1.147323794382618,
            "step": 17320
        },
        {
            "loss": 0.3889,
            "grad_norm": 0.7559905052185059,
            "learning_rate": 3.085585585585586e-05,
            "epoch": 1.1486486486486487,
            "step": 17340
        },
        {
            "loss": 0.3993,
            "grad_norm": 2.217453718185425,
            "learning_rate": 3.083377495142201e-05,
            "epoch": 1.1499735029146794,
            "step": 17360
        },
        {
            "loss": 0.405,
            "grad_norm": 1.309996485710144,
            "learning_rate": 3.081169404698816e-05,
            "epoch": 1.15129835718071,
            "step": 17380
        },
        {
            "loss": 0.3612,
            "grad_norm": 2.3716259002685547,
            "learning_rate": 3.078961314255432e-05,
            "epoch": 1.1526232114467407,
            "step": 17400
        },
        {
            "loss": 0.3492,
            "grad_norm": 1.8080722093582153,
            "learning_rate": 3.076753223812047e-05,
            "epoch": 1.1539480657127716,
            "step": 17420
        },
        {
            "loss": 0.3336,
            "grad_norm": 1.4352689981460571,
            "learning_rate": 3.074545133368663e-05,
            "epoch": 1.1552729199788023,
            "step": 17440
        },
        {
            "loss": 0.3739,
            "grad_norm": 1.4705870151519775,
            "learning_rate": 3.072337042925279e-05,
            "epoch": 1.156597774244833,
            "step": 17460
        },
        {
            "loss": 0.3517,
            "grad_norm": 8.334527015686035,
            "learning_rate": 3.070128952481894e-05,
            "epoch": 1.157922628510864,
            "step": 17480
        },
        {
            "loss": 0.3494,
            "grad_norm": 0.9632085561752319,
            "learning_rate": 3.067920862038509e-05,
            "epoch": 1.1592474827768946,
            "step": 17500
        },
        {
            "loss": 0.4114,
            "grad_norm": 2.4137091636657715,
            "learning_rate": 3.0657127715951247e-05,
            "epoch": 1.1605723370429253,
            "step": 17520
        },
        {
            "loss": 0.4359,
            "grad_norm": 4.170283317565918,
            "learning_rate": 3.0635046811517404e-05,
            "epoch": 1.161897191308956,
            "step": 17540
        },
        {
            "loss": 0.3589,
            "grad_norm": 1.7988355159759521,
            "learning_rate": 3.0612965907083555e-05,
            "epoch": 1.1632220455749867,
            "step": 17560
        },
        {
            "loss": 0.4277,
            "grad_norm": 1.2525051832199097,
            "learning_rate": 3.059088500264971e-05,
            "epoch": 1.1645468998410176,
            "step": 17580
        },
        {
            "loss": 0.4092,
            "grad_norm": 0.6721726059913635,
            "learning_rate": 3.0568804098215864e-05,
            "epoch": 1.1658717541070482,
            "step": 17600
        },
        {
            "loss": 0.3766,
            "grad_norm": 3.0510473251342773,
            "learning_rate": 3.0546723193782015e-05,
            "epoch": 1.167196608373079,
            "step": 17620
        },
        {
            "loss": 0.3626,
            "grad_norm": 0.7798462510108948,
            "learning_rate": 3.052464228934817e-05,
            "epoch": 1.1685214626391096,
            "step": 17640
        },
        {
            "loss": 0.4121,
            "grad_norm": 1.427878499031067,
            "learning_rate": 3.050256138491433e-05,
            "epoch": 1.1698463169051405,
            "step": 17660
        },
        {
            "loss": 0.3562,
            "grad_norm": 1.0306316614151,
            "learning_rate": 3.0480480480480482e-05,
            "epoch": 1.1711711711711712,
            "step": 17680
        },
        {
            "loss": 0.3692,
            "grad_norm": 1.3062849044799805,
            "learning_rate": 3.0458399576046637e-05,
            "epoch": 1.1724960254372019,
            "step": 17700
        },
        {
            "loss": 0.3733,
            "grad_norm": 1.5920977592468262,
            "learning_rate": 3.043631867161279e-05,
            "epoch": 1.1738208797032326,
            "step": 17720
        },
        {
            "loss": 0.4322,
            "grad_norm": 2.3211512565612793,
            "learning_rate": 3.0414237767178942e-05,
            "epoch": 1.1751457339692635,
            "step": 17740
        },
        {
            "loss": 0.3715,
            "grad_norm": 58.43589401245117,
            "learning_rate": 3.0392156862745097e-05,
            "epoch": 1.1764705882352942,
            "step": 17760
        },
        {
            "loss": 0.4392,
            "grad_norm": 1.395713448524475,
            "learning_rate": 3.0370075958311254e-05,
            "epoch": 1.1777954425013248,
            "step": 17780
        },
        {
            "loss": 0.4251,
            "grad_norm": 1.292801022529602,
            "learning_rate": 3.034799505387741e-05,
            "epoch": 1.1791202967673555,
            "step": 17800
        },
        {
            "loss": 0.4071,
            "grad_norm": 1.1501818895339966,
            "learning_rate": 3.0325914149443563e-05,
            "epoch": 1.1804451510333864,
            "step": 17820
        },
        {
            "loss": 0.3686,
            "grad_norm": 3.3537025451660156,
            "learning_rate": 3.0303833245009718e-05,
            "epoch": 1.1817700052994171,
            "step": 17840
        },
        {
            "loss": 0.3714,
            "grad_norm": 3.137033462524414,
            "learning_rate": 3.028175234057587e-05,
            "epoch": 1.1830948595654478,
            "step": 17860
        },
        {
            "loss": 0.3526,
            "grad_norm": 2.1791751384735107,
            "learning_rate": 3.025967143614203e-05,
            "epoch": 1.1844197138314785,
            "step": 17880
        },
        {
            "loss": 0.4783,
            "grad_norm": 2.337373971939087,
            "learning_rate": 3.023759053170818e-05,
            "epoch": 1.1857445680975092,
            "step": 17900
        },
        {
            "loss": 0.4016,
            "grad_norm": 0.6736034750938416,
            "learning_rate": 3.0215509627274335e-05,
            "epoch": 1.18706942236354,
            "step": 17920
        },
        {
            "loss": 0.4908,
            "grad_norm": 1.2139806747436523,
            "learning_rate": 3.019342872284049e-05,
            "epoch": 1.1883942766295708,
            "step": 17940
        },
        {
            "loss": 0.4373,
            "grad_norm": 0.8309804797172546,
            "learning_rate": 3.017134781840664e-05,
            "epoch": 1.1897191308956014,
            "step": 17960
        },
        {
            "loss": 0.4188,
            "grad_norm": 2.9868435859680176,
            "learning_rate": 3.0149266913972795e-05,
            "epoch": 1.1910439851616321,
            "step": 17980
        },
        {
            "loss": 0.4165,
            "grad_norm": 2.42846417427063,
            "learning_rate": 3.0127186009538953e-05,
            "epoch": 1.192368839427663,
            "step": 18000
        },
        {
            "loss": 0.4095,
            "grad_norm": 3.8647754192352295,
            "learning_rate": 3.0105105105105108e-05,
            "epoch": 1.1936936936936937,
            "step": 18020
        },
        {
            "loss": 0.3755,
            "grad_norm": 19.626108169555664,
            "learning_rate": 3.0083024200671262e-05,
            "epoch": 1.1950185479597244,
            "step": 18040
        },
        {
            "loss": 0.3938,
            "grad_norm": 2.2157349586486816,
            "learning_rate": 3.0060943296237417e-05,
            "epoch": 1.196343402225755,
            "step": 18060
        },
        {
            "loss": 0.4058,
            "grad_norm": 0.7924713492393494,
            "learning_rate": 3.0038862391803568e-05,
            "epoch": 1.197668256491786,
            "step": 18080
        },
        {
            "loss": 0.4043,
            "grad_norm": 2.890326738357544,
            "learning_rate": 3.0016781487369722e-05,
            "epoch": 1.1989931107578167,
            "step": 18100
        },
        {
            "loss": 0.3897,
            "grad_norm": 1.1473594903945923,
            "learning_rate": 2.999470058293588e-05,
            "epoch": 1.2003179650238474,
            "step": 18120
        },
        {
            "loss": 0.4844,
            "grad_norm": 2.0963797569274902,
            "learning_rate": 2.9972619678502034e-05,
            "epoch": 1.201642819289878,
            "step": 18140
        },
        {
            "loss": 0.3396,
            "grad_norm": 2.9272122383117676,
            "learning_rate": 2.995053877406819e-05,
            "epoch": 1.202967673555909,
            "step": 18160
        },
        {
            "loss": 0.3206,
            "grad_norm": 1.0313689708709717,
            "learning_rate": 2.992845786963434e-05,
            "epoch": 1.2042925278219396,
            "step": 18180
        },
        {
            "loss": 0.3799,
            "grad_norm": 0.9811476469039917,
            "learning_rate": 2.9906376965200494e-05,
            "epoch": 1.2056173820879703,
            "step": 18200
        },
        {
            "loss": 0.3334,
            "grad_norm": 1.3193110227584839,
            "learning_rate": 2.988429606076665e-05,
            "epoch": 1.206942236354001,
            "step": 18220
        },
        {
            "loss": 0.378,
            "grad_norm": 1.1595685482025146,
            "learning_rate": 2.9862215156332807e-05,
            "epoch": 1.2082670906200317,
            "step": 18240
        },
        {
            "loss": 0.3673,
            "grad_norm": 18.294967651367188,
            "learning_rate": 2.984013425189896e-05,
            "epoch": 1.2095919448860626,
            "step": 18260
        },
        {
            "loss": 0.4275,
            "grad_norm": 1.520616054534912,
            "learning_rate": 2.9818053347465112e-05,
            "epoch": 1.2109167991520933,
            "step": 18280
        },
        {
            "loss": 0.3318,
            "grad_norm": 2.162613868713379,
            "learning_rate": 2.9795972443031266e-05,
            "epoch": 1.212241653418124,
            "step": 18300
        },
        {
            "loss": 0.3717,
            "grad_norm": 1.3004083633422852,
            "learning_rate": 2.977389153859742e-05,
            "epoch": 1.2135665076841549,
            "step": 18320
        },
        {
            "loss": 0.3503,
            "grad_norm": 1.7361321449279785,
            "learning_rate": 2.975181063416358e-05,
            "epoch": 1.2148913619501855,
            "step": 18340
        },
        {
            "loss": 0.4058,
            "grad_norm": 1.3497215509414673,
            "learning_rate": 2.9729729729729733e-05,
            "epoch": 1.2162162162162162,
            "step": 18360
        },
        {
            "loss": 0.3821,
            "grad_norm": 1.9825648069381714,
            "learning_rate": 2.9707648825295888e-05,
            "epoch": 1.217541070482247,
            "step": 18380
        },
        {
            "loss": 0.3804,
            "grad_norm": 3.037039279937744,
            "learning_rate": 2.968556792086204e-05,
            "epoch": 1.2188659247482776,
            "step": 18400
        },
        {
            "loss": 0.3542,
            "grad_norm": 2.72239089012146,
            "learning_rate": 2.9663487016428193e-05,
            "epoch": 1.2201907790143085,
            "step": 18420
        },
        {
            "loss": 0.4101,
            "grad_norm": 2.501765251159668,
            "learning_rate": 2.9641406111994348e-05,
            "epoch": 1.2215156332803392,
            "step": 18440
        },
        {
            "loss": 0.3551,
            "grad_norm": 0.8071909546852112,
            "learning_rate": 2.9619325207560505e-05,
            "epoch": 1.2228404875463699,
            "step": 18460
        },
        {
            "loss": 0.3178,
            "grad_norm": 1.6724580526351929,
            "learning_rate": 2.959724430312666e-05,
            "epoch": 1.2241653418124006,
            "step": 18480
        },
        {
            "loss": 0.442,
            "grad_norm": 1.9431288242340088,
            "learning_rate": 2.957516339869281e-05,
            "epoch": 1.2254901960784315,
            "step": 18500
        },
        {
            "loss": 0.3428,
            "grad_norm": 1.1100982427597046,
            "learning_rate": 2.9553082494258965e-05,
            "epoch": 1.2268150503444621,
            "step": 18520
        },
        {
            "loss": 0.3664,
            "grad_norm": 0.9987859725952148,
            "learning_rate": 2.953100158982512e-05,
            "epoch": 1.2281399046104928,
            "step": 18540
        },
        {
            "loss": 0.43,
            "grad_norm": 8.983256340026855,
            "learning_rate": 2.9508920685391274e-05,
            "epoch": 1.2294647588765235,
            "step": 18560
        },
        {
            "loss": 0.4033,
            "grad_norm": 3.888576030731201,
            "learning_rate": 2.9486839780957432e-05,
            "epoch": 1.2307896131425542,
            "step": 18580
        },
        {
            "loss": 0.4091,
            "grad_norm": 2.8861641883850098,
            "learning_rate": 2.9464758876523586e-05,
            "epoch": 1.232114467408585,
            "step": 18600
        },
        {
            "loss": 0.3841,
            "grad_norm": 0.7303657531738281,
            "learning_rate": 2.9442677972089738e-05,
            "epoch": 1.2334393216746158,
            "step": 18620
        },
        {
            "loss": 0.3534,
            "grad_norm": 1.9055119752883911,
            "learning_rate": 2.9420597067655892e-05,
            "epoch": 1.2347641759406465,
            "step": 18640
        },
        {
            "loss": 0.3313,
            "grad_norm": 2.0794780254364014,
            "learning_rate": 2.9398516163222046e-05,
            "epoch": 1.2360890302066774,
            "step": 18660
        },
        {
            "loss": 0.4395,
            "grad_norm": 2.316009998321533,
            "learning_rate": 2.9376435258788204e-05,
            "epoch": 1.237413884472708,
            "step": 18680
        },
        {
            "loss": 0.3709,
            "grad_norm": 2.3149971961975098,
            "learning_rate": 2.935435435435436e-05,
            "epoch": 1.2387387387387387,
            "step": 18700
        },
        {
            "loss": 0.3906,
            "grad_norm": 1.5951128005981445,
            "learning_rate": 2.933227344992051e-05,
            "epoch": 1.2400635930047694,
            "step": 18720
        },
        {
            "loss": 0.3949,
            "grad_norm": 0.6647651791572571,
            "learning_rate": 2.9310192545486664e-05,
            "epoch": 1.2413884472708,
            "step": 18740
        },
        {
            "loss": 0.3806,
            "grad_norm": 0.44371873140335083,
            "learning_rate": 2.928811164105282e-05,
            "epoch": 1.242713301536831,
            "step": 18760
        },
        {
            "loss": 0.3596,
            "grad_norm": 1.6004753112792969,
            "learning_rate": 2.926603073661897e-05,
            "epoch": 1.2440381558028617,
            "step": 18780
        },
        {
            "loss": 0.3384,
            "grad_norm": 1.9459956884384155,
            "learning_rate": 2.924394983218513e-05,
            "epoch": 1.2453630100688924,
            "step": 18800
        },
        {
            "loss": 0.4265,
            "grad_norm": 1.1236810684204102,
            "learning_rate": 2.9221868927751282e-05,
            "epoch": 1.246687864334923,
            "step": 18820
        },
        {
            "loss": 0.4532,
            "grad_norm": 1.2099697589874268,
            "learning_rate": 2.9199788023317436e-05,
            "epoch": 1.248012718600954,
            "step": 18840
        },
        {
            "loss": 0.3769,
            "grad_norm": 1.2982275485992432,
            "learning_rate": 2.917770711888359e-05,
            "epoch": 1.2493375728669847,
            "step": 18860
        },
        {
            "loss": 0.383,
            "grad_norm": 0.814802348613739,
            "learning_rate": 2.9155626214449745e-05,
            "epoch": 1.2506624271330153,
            "step": 18880
        },
        {
            "loss": 0.3659,
            "grad_norm": 0.804328978061676,
            "learning_rate": 2.9133545310015896e-05,
            "epoch": 1.251987281399046,
            "step": 18900
        },
        {
            "loss": 0.3947,
            "grad_norm": 2.008718729019165,
            "learning_rate": 2.9111464405582058e-05,
            "epoch": 1.2533121356650767,
            "step": 18920
        },
        {
            "loss": 0.4045,
            "grad_norm": 2.927530288696289,
            "learning_rate": 2.908938350114821e-05,
            "epoch": 1.2546369899311076,
            "step": 18940
        },
        {
            "loss": 0.3887,
            "grad_norm": 4.178719520568848,
            "learning_rate": 2.9067302596714363e-05,
            "epoch": 1.2559618441971383,
            "step": 18960
        },
        {
            "loss": 0.3816,
            "grad_norm": 2.8958487510681152,
            "learning_rate": 2.9045221692280517e-05,
            "epoch": 1.257286698463169,
            "step": 18980
        },
        {
            "loss": 0.3886,
            "grad_norm": 1.9628636837005615,
            "learning_rate": 2.902314078784667e-05,
            "epoch": 1.2586115527292,
            "step": 19000
        },
        {
            "loss": 0.3922,
            "grad_norm": 0.9984176158905029,
            "learning_rate": 2.9001059883412823e-05,
            "epoch": 1.2599364069952306,
            "step": 19020
        },
        {
            "loss": 0.4361,
            "grad_norm": 3.859879732131958,
            "learning_rate": 2.897897897897898e-05,
            "epoch": 1.2612612612612613,
            "step": 19040
        },
        {
            "loss": 0.4114,
            "grad_norm": 1.0875729322433472,
            "learning_rate": 2.8956898074545135e-05,
            "epoch": 1.262586115527292,
            "step": 19060
        },
        {
            "loss": 0.3724,
            "grad_norm": 0.7004137635231018,
            "learning_rate": 2.893481717011129e-05,
            "epoch": 1.2639109697933226,
            "step": 19080
        },
        {
            "loss": 0.346,
            "grad_norm": 1.6117275953292847,
            "learning_rate": 2.8912736265677444e-05,
            "epoch": 1.2652358240593535,
            "step": 19100
        },
        {
            "loss": 0.3776,
            "grad_norm": 2.7553746700286865,
            "learning_rate": 2.8890655361243595e-05,
            "epoch": 1.2665606783253842,
            "step": 19120
        },
        {
            "loss": 0.3699,
            "grad_norm": 1.0477412939071655,
            "learning_rate": 2.8868574456809756e-05,
            "epoch": 1.267885532591415,
            "step": 19140
        },
        {
            "loss": 0.4013,
            "grad_norm": 4.379734039306641,
            "learning_rate": 2.8846493552375907e-05,
            "epoch": 1.2692103868574458,
            "step": 19160
        },
        {
            "loss": 0.4622,
            "grad_norm": 3.6973462104797363,
            "learning_rate": 2.8824412647942062e-05,
            "epoch": 1.2705352411234765,
            "step": 19180
        },
        {
            "loss": 0.3741,
            "grad_norm": 0.8610756993293762,
            "learning_rate": 2.8802331743508216e-05,
            "epoch": 1.2718600953895072,
            "step": 19200
        },
        {
            "loss": 0.3764,
            "grad_norm": 1.2453781366348267,
            "learning_rate": 2.8780250839074367e-05,
            "epoch": 1.2731849496555379,
            "step": 19220
        },
        {
            "loss": 0.4311,
            "grad_norm": 4.197838306427002,
            "learning_rate": 2.8758169934640522e-05,
            "epoch": 1.2745098039215685,
            "step": 19240
        },
        {
            "loss": 0.3021,
            "grad_norm": 1.0949299335479736,
            "learning_rate": 2.873608903020668e-05,
            "epoch": 1.2758346581875994,
            "step": 19260
        },
        {
            "loss": 0.3848,
            "grad_norm": 0.7164369225502014,
            "learning_rate": 2.8714008125772834e-05,
            "epoch": 1.2771595124536301,
            "step": 19280
        },
        {
            "loss": 0.3691,
            "grad_norm": 1.1657729148864746,
            "learning_rate": 2.869192722133899e-05,
            "epoch": 1.2784843667196608,
            "step": 19300
        },
        {
            "loss": 0.3165,
            "grad_norm": 2.084778308868408,
            "learning_rate": 2.866984631690514e-05,
            "epoch": 1.2798092209856915,
            "step": 19320
        },
        {
            "loss": 0.3535,
            "grad_norm": 5.628119945526123,
            "learning_rate": 2.8647765412471294e-05,
            "epoch": 1.2811340752517224,
            "step": 19340
        },
        {
            "loss": 0.4178,
            "grad_norm": 0.5317241549491882,
            "learning_rate": 2.862568450803745e-05,
            "epoch": 1.282458929517753,
            "step": 19360
        },
        {
            "loss": 0.3503,
            "grad_norm": 2.289003849029541,
            "learning_rate": 2.8603603603603606e-05,
            "epoch": 1.2837837837837838,
            "step": 19380
        },
        {
            "loss": 0.3876,
            "grad_norm": 1.2409886121749878,
            "learning_rate": 2.858152269916976e-05,
            "epoch": 1.2851086380498145,
            "step": 19400
        },
        {
            "loss": 0.3595,
            "grad_norm": 1.221868634223938,
            "learning_rate": 2.8559441794735915e-05,
            "epoch": 1.2864334923158451,
            "step": 19420
        },
        {
            "loss": 0.4238,
            "grad_norm": 4.5897135734558105,
            "learning_rate": 2.8537360890302066e-05,
            "epoch": 1.287758346581876,
            "step": 19440
        },
        {
            "loss": 0.4023,
            "grad_norm": 3.3017234802246094,
            "learning_rate": 2.851527998586822e-05,
            "epoch": 1.2890832008479067,
            "step": 19460
        },
        {
            "loss": 0.4124,
            "grad_norm": 0.8963485360145569,
            "learning_rate": 2.849319908143438e-05,
            "epoch": 1.2904080551139374,
            "step": 19480
        },
        {
            "loss": 0.4029,
            "grad_norm": 1.810623288154602,
            "learning_rate": 2.8471118177000533e-05,
            "epoch": 1.2917329093799683,
            "step": 19500
        },
        {
            "loss": 0.4175,
            "grad_norm": 2.536327600479126,
            "learning_rate": 2.8449037272566687e-05,
            "epoch": 1.293057763645999,
            "step": 19520
        },
        {
            "loss": 0.3743,
            "grad_norm": 4.070684432983398,
            "learning_rate": 2.842695636813284e-05,
            "epoch": 1.2943826179120297,
            "step": 19540
        },
        {
            "loss": 0.329,
            "grad_norm": 2.6048429012298584,
            "learning_rate": 2.8404875463698993e-05,
            "epoch": 1.2957074721780604,
            "step": 19560
        },
        {
            "loss": 0.4341,
            "grad_norm": 0.643717348575592,
            "learning_rate": 2.8382794559265147e-05,
            "epoch": 1.297032326444091,
            "step": 19580
        },
        {
            "loss": 0.3971,
            "grad_norm": 1.0415750741958618,
            "learning_rate": 2.8360713654831305e-05,
            "epoch": 1.298357180710122,
            "step": 19600
        },
        {
            "loss": 0.4009,
            "grad_norm": 0.9173647165298462,
            "learning_rate": 2.833863275039746e-05,
            "epoch": 1.2996820349761526,
            "step": 19620
        },
        {
            "loss": 0.3749,
            "grad_norm": 5.1862287521362305,
            "learning_rate": 2.8316551845963614e-05,
            "epoch": 1.3010068892421833,
            "step": 19640
        },
        {
            "loss": 0.4223,
            "grad_norm": 0.6301106810569763,
            "learning_rate": 2.8294470941529765e-05,
            "epoch": 1.3023317435082142,
            "step": 19660
        },
        {
            "loss": 0.406,
            "grad_norm": 0.8885966539382935,
            "learning_rate": 2.827239003709592e-05,
            "epoch": 1.303656597774245,
            "step": 19680
        },
        {
            "loss": 0.3617,
            "grad_norm": 2.568739175796509,
            "learning_rate": 2.8250309132662074e-05,
            "epoch": 1.3049814520402756,
            "step": 19700
        },
        {
            "loss": 0.3875,
            "grad_norm": 5.042489051818848,
            "learning_rate": 2.8228228228228232e-05,
            "epoch": 1.3063063063063063,
            "step": 19720
        },
        {
            "loss": 0.3569,
            "grad_norm": 4.7136311531066895,
            "learning_rate": 2.8206147323794386e-05,
            "epoch": 1.307631160572337,
            "step": 19740
        },
        {
            "loss": 0.3939,
            "grad_norm": 1.0551643371582031,
            "learning_rate": 2.8184066419360537e-05,
            "epoch": 1.3089560148383677,
            "step": 19760
        },
        {
            "loss": 0.3744,
            "grad_norm": 0.899191677570343,
            "learning_rate": 2.8161985514926692e-05,
            "epoch": 1.3102808691043986,
            "step": 19780
        },
        {
            "loss": 0.3549,
            "grad_norm": 0.4974687993526459,
            "learning_rate": 2.8139904610492846e-05,
            "epoch": 1.3116057233704292,
            "step": 19800
        },
        {
            "loss": 0.4031,
            "grad_norm": 8.961020469665527,
            "learning_rate": 2.8117823706058997e-05,
            "epoch": 1.31293057763646,
            "step": 19820
        },
        {
            "loss": 0.4901,
            "grad_norm": 2.266957998275757,
            "learning_rate": 2.809574280162516e-05,
            "epoch": 1.3142554319024908,
            "step": 19840
        },
        {
            "loss": 0.411,
            "grad_norm": 2.0714566707611084,
            "learning_rate": 2.807366189719131e-05,
            "epoch": 1.3155802861685215,
            "step": 19860
        },
        {
            "loss": 0.411,
            "grad_norm": 2.4828989505767822,
            "learning_rate": 2.8051580992757464e-05,
            "epoch": 1.3169051404345522,
            "step": 19880
        },
        {
            "loss": 0.4053,
            "grad_norm": 2.9960312843322754,
            "learning_rate": 2.802950008832362e-05,
            "epoch": 1.3182299947005829,
            "step": 19900
        },
        {
            "loss": 0.3937,
            "grad_norm": 2.6083755493164062,
            "learning_rate": 2.8007419183889773e-05,
            "epoch": 1.3195548489666136,
            "step": 19920
        },
        {
            "loss": 0.3455,
            "grad_norm": 2.0475716590881348,
            "learning_rate": 2.798533827945593e-05,
            "epoch": 1.3208797032326445,
            "step": 19940
        },
        {
            "loss": 0.3995,
            "grad_norm": 3.1718313694000244,
            "learning_rate": 2.7963257375022085e-05,
            "epoch": 1.3222045574986752,
            "step": 19960
        },
        {
            "loss": 0.3882,
            "grad_norm": 1.467879056930542,
            "learning_rate": 2.7941176470588236e-05,
            "epoch": 1.3235294117647058,
            "step": 19980
        },
        {
            "loss": 0.3415,
            "grad_norm": 1.2001835107803345,
            "learning_rate": 2.791909556615439e-05,
            "epoch": 1.3248542660307367,
            "step": 20000
        },
        {
            "loss": 0.4161,
            "grad_norm": 4.687375068664551,
            "learning_rate": 2.7897014661720545e-05,
            "epoch": 1.3261791202967674,
            "step": 20020
        },
        {
            "loss": 0.4061,
            "grad_norm": 0.7241650819778442,
            "learning_rate": 2.7874933757286696e-05,
            "epoch": 1.3275039745627981,
            "step": 20040
        },
        {
            "loss": 0.404,
            "grad_norm": 3.699523687362671,
            "learning_rate": 2.7852852852852857e-05,
            "epoch": 1.3288288288288288,
            "step": 20060
        },
        {
            "loss": 0.3651,
            "grad_norm": 1.815887689590454,
            "learning_rate": 2.783077194841901e-05,
            "epoch": 1.3301536830948595,
            "step": 20080
        },
        {
            "loss": 0.3679,
            "grad_norm": 1.2152947187423706,
            "learning_rate": 2.7808691043985163e-05,
            "epoch": 1.3314785373608902,
            "step": 20100
        },
        {
            "loss": 0.3729,
            "grad_norm": 0.031929805874824524,
            "learning_rate": 2.7786610139551317e-05,
            "epoch": 1.332803391626921,
            "step": 20120
        },
        {
            "loss": 0.3596,
            "grad_norm": 2.7237935066223145,
            "learning_rate": 2.7764529235117472e-05,
            "epoch": 1.3341282458929518,
            "step": 20140
        },
        {
            "loss": 0.3773,
            "grad_norm": 1.0407072305679321,
            "learning_rate": 2.7742448330683623e-05,
            "epoch": 1.3354531001589824,
            "step": 20160
        },
        {
            "loss": 0.3963,
            "grad_norm": 1.3630973100662231,
            "learning_rate": 2.7720367426249784e-05,
            "epoch": 1.3367779544250133,
            "step": 20180
        },
        {
            "loss": 0.3613,
            "grad_norm": 2.586275815963745,
            "learning_rate": 2.7698286521815935e-05,
            "epoch": 1.338102808691044,
            "step": 20200
        },
        {
            "loss": 0.3809,
            "grad_norm": 1.1554973125457764,
            "learning_rate": 2.767620561738209e-05,
            "epoch": 1.3394276629570747,
            "step": 20220
        },
        {
            "loss": 0.4107,
            "grad_norm": 1.841215968132019,
            "learning_rate": 2.7654124712948244e-05,
            "epoch": 1.3407525172231054,
            "step": 20240
        },
        {
            "loss": 0.358,
            "grad_norm": 1.8182445764541626,
            "learning_rate": 2.7632043808514395e-05,
            "epoch": 1.342077371489136,
            "step": 20260
        },
        {
            "loss": 0.3392,
            "grad_norm": 1.1132006645202637,
            "learning_rate": 2.7609962904080556e-05,
            "epoch": 1.343402225755167,
            "step": 20280
        },
        {
            "loss": 0.3936,
            "grad_norm": 87.35824584960938,
            "learning_rate": 2.7587881999646707e-05,
            "epoch": 1.3447270800211977,
            "step": 20300
        },
        {
            "loss": 0.4242,
            "grad_norm": 1.0018705129623413,
            "learning_rate": 2.7565801095212862e-05,
            "epoch": 1.3460519342872284,
            "step": 20320
        },
        {
            "loss": 0.4228,
            "grad_norm": 1.156909704208374,
            "learning_rate": 2.7543720190779016e-05,
            "epoch": 1.3473767885532593,
            "step": 20340
        },
        {
            "loss": 0.4015,
            "grad_norm": 1.9754977226257324,
            "learning_rate": 2.7521639286345167e-05,
            "epoch": 1.34870164281929,
            "step": 20360
        },
        {
            "loss": 0.3947,
            "grad_norm": 5.220659255981445,
            "learning_rate": 2.7499558381911322e-05,
            "epoch": 1.3500264970853206,
            "step": 20380
        },
        {
            "loss": 0.3662,
            "grad_norm": 1.1444107294082642,
            "learning_rate": 2.7477477477477483e-05,
            "epoch": 1.3513513513513513,
            "step": 20400
        },
        {
            "loss": 0.4132,
            "grad_norm": 38.93659210205078,
            "learning_rate": 2.7455396573043634e-05,
            "epoch": 1.352676205617382,
            "step": 20420
        },
        {
            "loss": 0.41,
            "grad_norm": 1.0705571174621582,
            "learning_rate": 2.743331566860979e-05,
            "epoch": 1.354001059883413,
            "step": 20440
        },
        {
            "loss": 0.3838,
            "grad_norm": 2.7887961864471436,
            "learning_rate": 2.7411234764175943e-05,
            "epoch": 1.3553259141494436,
            "step": 20460
        },
        {
            "loss": 0.404,
            "grad_norm": 4.103074550628662,
            "learning_rate": 2.7389153859742094e-05,
            "epoch": 1.3566507684154743,
            "step": 20480
        },
        {
            "loss": 0.42,
            "grad_norm": 2.796457290649414,
            "learning_rate": 2.736707295530825e-05,
            "epoch": 1.357975622681505,
            "step": 20500
        },
        {
            "loss": 0.477,
            "grad_norm": 0.9621010422706604,
            "learning_rate": 2.7344992050874406e-05,
            "epoch": 1.3593004769475359,
            "step": 20520
        },
        {
            "loss": 0.4212,
            "grad_norm": 1.2045420408248901,
            "learning_rate": 2.732291114644056e-05,
            "epoch": 1.3606253312135665,
            "step": 20540
        },
        {
            "loss": 0.3918,
            "grad_norm": 5.3604044914245605,
            "learning_rate": 2.7300830242006715e-05,
            "epoch": 1.3619501854795972,
            "step": 20560
        },
        {
            "loss": 0.384,
            "grad_norm": 0.8305299878120422,
            "learning_rate": 2.7278749337572866e-05,
            "epoch": 1.363275039745628,
            "step": 20580
        },
        {
            "loss": 0.3816,
            "grad_norm": 1.08710777759552,
            "learning_rate": 2.725666843313902e-05,
            "epoch": 1.3645998940116586,
            "step": 20600
        },
        {
            "loss": 0.419,
            "grad_norm": 8.831400871276855,
            "learning_rate": 2.7234587528705175e-05,
            "epoch": 1.3659247482776895,
            "step": 20620
        },
        {
            "loss": 0.3783,
            "grad_norm": 2.017648220062256,
            "learning_rate": 2.7212506624271333e-05,
            "epoch": 1.3672496025437202,
            "step": 20640
        },
        {
            "loss": 0.3567,
            "grad_norm": 1.0476758480072021,
            "learning_rate": 2.7190425719837487e-05,
            "epoch": 1.3685744568097509,
            "step": 20660
        },
        {
            "loss": 0.3519,
            "grad_norm": 1.8469533920288086,
            "learning_rate": 2.7168344815403642e-05,
            "epoch": 1.3698993110757818,
            "step": 20680
        },
        {
            "loss": 0.3753,
            "grad_norm": 3.027970314025879,
            "learning_rate": 2.7146263910969793e-05,
            "epoch": 1.3712241653418125,
            "step": 20700
        },
        {
            "loss": 0.3503,
            "grad_norm": 0.6595026850700378,
            "learning_rate": 2.7124183006535947e-05,
            "epoch": 1.3725490196078431,
            "step": 20720
        },
        {
            "loss": 0.3374,
            "grad_norm": 1.7539268732070923,
            "learning_rate": 2.7102102102102105e-05,
            "epoch": 1.3738738738738738,
            "step": 20740
        },
        {
            "loss": 0.3994,
            "grad_norm": 3.429250717163086,
            "learning_rate": 2.708002119766826e-05,
            "epoch": 1.3751987281399045,
            "step": 20760
        },
        {
            "loss": 0.3551,
            "grad_norm": 2.3647665977478027,
            "learning_rate": 2.7057940293234414e-05,
            "epoch": 1.3765235824059354,
            "step": 20780
        },
        {
            "loss": 0.3924,
            "grad_norm": 0.886641800403595,
            "learning_rate": 2.7035859388800565e-05,
            "epoch": 1.377848436671966,
            "step": 20800
        },
        {
            "loss": 0.3229,
            "grad_norm": 1.0095551013946533,
            "learning_rate": 2.701377848436672e-05,
            "epoch": 1.3791732909379968,
            "step": 20820
        },
        {
            "loss": 0.4039,
            "grad_norm": 1.7129772901535034,
            "learning_rate": 2.6991697579932874e-05,
            "epoch": 1.3804981452040277,
            "step": 20840
        },
        {
            "loss": 0.3841,
            "grad_norm": 0.44752341508865356,
            "learning_rate": 2.6969616675499032e-05,
            "epoch": 1.3818229994700584,
            "step": 20860
        },
        {
            "loss": 0.4048,
            "grad_norm": 4.061570644378662,
            "learning_rate": 2.6947535771065186e-05,
            "epoch": 1.383147853736089,
            "step": 20880
        },
        {
            "loss": 0.4194,
            "grad_norm": 0.9955775737762451,
            "learning_rate": 2.6925454866631337e-05,
            "epoch": 1.3844727080021197,
            "step": 20900
        },
        {
            "loss": 0.4217,
            "grad_norm": 1.0899802446365356,
            "learning_rate": 2.690337396219749e-05,
            "epoch": 1.3857975622681504,
            "step": 20920
        },
        {
            "loss": 0.3981,
            "grad_norm": 0.6540979146957397,
            "learning_rate": 2.6881293057763646e-05,
            "epoch": 1.3871224165341811,
            "step": 20940
        },
        {
            "loss": 0.4408,
            "grad_norm": 3.879556894302368,
            "learning_rate": 2.68592121533298e-05,
            "epoch": 1.388447270800212,
            "step": 20960
        },
        {
            "loss": 0.3713,
            "grad_norm": 2.016631841659546,
            "learning_rate": 2.683713124889596e-05,
            "epoch": 1.3897721250662427,
            "step": 20980
        },
        {
            "loss": 0.3401,
            "grad_norm": 2.012528419494629,
            "learning_rate": 2.6815050344462113e-05,
            "epoch": 1.3910969793322734,
            "step": 21000
        },
        {
            "loss": 0.3888,
            "grad_norm": 14.357027053833008,
            "learning_rate": 2.6792969440028264e-05,
            "epoch": 1.3924218335983043,
            "step": 21020
        },
        {
            "loss": 0.323,
            "grad_norm": 0.4764322340488434,
            "learning_rate": 2.6770888535594418e-05,
            "epoch": 1.393746687864335,
            "step": 21040
        },
        {
            "loss": 0.407,
            "grad_norm": 0.7830637097358704,
            "learning_rate": 2.6748807631160573e-05,
            "epoch": 1.3950715421303657,
            "step": 21060
        },
        {
            "loss": 0.3873,
            "grad_norm": 1.5317678451538086,
            "learning_rate": 2.672672672672673e-05,
            "epoch": 1.3963963963963963,
            "step": 21080
        },
        {
            "loss": 0.3367,
            "grad_norm": 2.075587272644043,
            "learning_rate": 2.6704645822292885e-05,
            "epoch": 1.397721250662427,
            "step": 21100
        },
        {
            "loss": 0.3915,
            "grad_norm": 2.100651741027832,
            "learning_rate": 2.6682564917859036e-05,
            "epoch": 1.399046104928458,
            "step": 21120
        },
        {
            "loss": 0.3753,
            "grad_norm": 2.202166795730591,
            "learning_rate": 2.666048401342519e-05,
            "epoch": 1.4003709591944886,
            "step": 21140
        },
        {
            "loss": 0.3118,
            "grad_norm": 0.4555087387561798,
            "learning_rate": 2.6638403108991345e-05,
            "epoch": 1.4016958134605193,
            "step": 21160
        },
        {
            "loss": 0.3764,
            "grad_norm": 2.4884164333343506,
            "learning_rate": 2.66163222045575e-05,
            "epoch": 1.4030206677265502,
            "step": 21180
        },
        {
            "loss": 0.3604,
            "grad_norm": 1.144597053527832,
            "learning_rate": 2.6594241300123657e-05,
            "epoch": 1.404345521992581,
            "step": 21200
        },
        {
            "loss": 0.3795,
            "grad_norm": 0.5475817322731018,
            "learning_rate": 2.657216039568981e-05,
            "epoch": 1.4056703762586116,
            "step": 21220
        },
        {
            "loss": 0.3586,
            "grad_norm": 1.8838785886764526,
            "learning_rate": 2.6550079491255963e-05,
            "epoch": 1.4069952305246423,
            "step": 21240
        },
        {
            "loss": 0.3951,
            "grad_norm": 1.5483556985855103,
            "learning_rate": 2.6527998586822117e-05,
            "epoch": 1.408320084790673,
            "step": 21260
        },
        {
            "loss": 0.3568,
            "grad_norm": 0.9137465953826904,
            "learning_rate": 2.650591768238827e-05,
            "epoch": 1.4096449390567036,
            "step": 21280
        },
        {
            "loss": 0.4168,
            "grad_norm": 1.7036584615707397,
            "learning_rate": 2.6483836777954423e-05,
            "epoch": 1.4109697933227345,
            "step": 21300
        },
        {
            "loss": 0.3813,
            "grad_norm": 3.2387239933013916,
            "learning_rate": 2.6461755873520584e-05,
            "epoch": 1.4122946475887652,
            "step": 21320
        },
        {
            "loss": 0.4198,
            "grad_norm": 1.984537959098816,
            "learning_rate": 2.6439674969086735e-05,
            "epoch": 1.413619501854796,
            "step": 21340
        },
        {
            "loss": 0.4572,
            "grad_norm": 1.2390556335449219,
            "learning_rate": 2.641759406465289e-05,
            "epoch": 1.4149443561208268,
            "step": 21360
        },
        {
            "loss": 0.4042,
            "grad_norm": 2.3217294216156006,
            "learning_rate": 2.6395513160219044e-05,
            "epoch": 1.4162692103868575,
            "step": 21380
        },
        {
            "loss": 0.4175,
            "grad_norm": 0.7342498898506165,
            "learning_rate": 2.6373432255785195e-05,
            "epoch": 1.4175940646528882,
            "step": 21400
        },
        {
            "loss": 0.3791,
            "grad_norm": 1.9373043775558472,
            "learning_rate": 2.635135135135135e-05,
            "epoch": 1.4189189189189189,
            "step": 21420
        },
        {
            "loss": 0.3862,
            "grad_norm": 2.2186279296875,
            "learning_rate": 2.632927044691751e-05,
            "epoch": 1.4202437731849495,
            "step": 21440
        },
        {
            "loss": 0.3764,
            "grad_norm": 2.974092721939087,
            "learning_rate": 2.630718954248366e-05,
            "epoch": 1.4215686274509804,
            "step": 21460
        },
        {
            "loss": 0.3493,
            "grad_norm": 3.0993244647979736,
            "learning_rate": 2.6285108638049816e-05,
            "epoch": 1.4228934817170111,
            "step": 21480
        },
        {
            "loss": 0.3747,
            "grad_norm": 1.4200687408447266,
            "learning_rate": 2.626302773361597e-05,
            "epoch": 1.4242183359830418,
            "step": 21500
        },
        {
            "loss": 0.3691,
            "grad_norm": 1.8922096490859985,
            "learning_rate": 2.624094682918212e-05,
            "epoch": 1.4255431902490727,
            "step": 21520
        },
        {
            "loss": 0.3731,
            "grad_norm": 2.1043741703033447,
            "learning_rate": 2.6218865924748283e-05,
            "epoch": 1.4268680445151034,
            "step": 21540
        },
        {
            "loss": 0.3336,
            "grad_norm": 2.6531789302825928,
            "learning_rate": 2.6196785020314434e-05,
            "epoch": 1.428192898781134,
            "step": 21560
        },
        {
            "loss": 0.398,
            "grad_norm": 0.7919091582298279,
            "learning_rate": 2.6174704115880588e-05,
            "epoch": 1.4295177530471648,
            "step": 21580
        },
        {
            "loss": 0.3792,
            "grad_norm": 1.9862905740737915,
            "learning_rate": 2.6152623211446743e-05,
            "epoch": 1.4308426073131955,
            "step": 21600
        },
        {
            "loss": 0.3609,
            "grad_norm": 0.8399407267570496,
            "learning_rate": 2.6130542307012894e-05,
            "epoch": 1.4321674615792264,
            "step": 21620
        },
        {
            "loss": 0.4667,
            "grad_norm": 4.319310665130615,
            "learning_rate": 2.6108461402579048e-05,
            "epoch": 1.433492315845257,
            "step": 21640
        },
        {
            "loss": 0.436,
            "grad_norm": 3.2501676082611084,
            "learning_rate": 2.6086380498145206e-05,
            "epoch": 1.4348171701112877,
            "step": 21660
        },
        {
            "loss": 0.4202,
            "grad_norm": 2.04362154006958,
            "learning_rate": 2.606429959371136e-05,
            "epoch": 1.4361420243773184,
            "step": 21680
        },
        {
            "loss": 0.3876,
            "grad_norm": 2.970923900604248,
            "learning_rate": 2.6042218689277515e-05,
            "epoch": 1.4374668786433493,
            "step": 21700
        },
        {
            "loss": 0.3495,
            "grad_norm": 2.1280531883239746,
            "learning_rate": 2.602013778484367e-05,
            "epoch": 1.43879173290938,
            "step": 21720
        },
        {
            "loss": 0.3882,
            "grad_norm": 1.7885346412658691,
            "learning_rate": 2.599805688040982e-05,
            "epoch": 1.4401165871754107,
            "step": 21740
        },
        {
            "loss": 0.351,
            "grad_norm": 1.9551151990890503,
            "learning_rate": 2.5975975975975975e-05,
            "epoch": 1.4414414414414414,
            "step": 21760
        },
        {
            "loss": 0.4269,
            "grad_norm": 0.8958476781845093,
            "learning_rate": 2.5953895071542133e-05,
            "epoch": 1.442766295707472,
            "step": 21780
        },
        {
            "loss": 0.378,
            "grad_norm": 2.305471658706665,
            "learning_rate": 2.5931814167108287e-05,
            "epoch": 1.444091149973503,
            "step": 21800
        },
        {
            "loss": 0.4179,
            "grad_norm": 1.8048624992370605,
            "learning_rate": 2.590973326267444e-05,
            "epoch": 1.4454160042395336,
            "step": 21820
        },
        {
            "loss": 0.4009,
            "grad_norm": 1.1164932250976562,
            "learning_rate": 2.5887652358240593e-05,
            "epoch": 1.4467408585055643,
            "step": 21840
        },
        {
            "loss": 0.3424,
            "grad_norm": 0.4212428033351898,
            "learning_rate": 2.5865571453806747e-05,
            "epoch": 1.4480657127715952,
            "step": 21860
        },
        {
            "loss": 0.367,
            "grad_norm": 2.057468891143799,
            "learning_rate": 2.5843490549372905e-05,
            "epoch": 1.449390567037626,
            "step": 21880
        },
        {
            "loss": 0.4042,
            "grad_norm": 1.4529929161071777,
            "learning_rate": 2.582140964493906e-05,
            "epoch": 1.4507154213036566,
            "step": 21900
        },
        {
            "loss": 0.3402,
            "grad_norm": 0.9404963850975037,
            "learning_rate": 2.5799328740505214e-05,
            "epoch": 1.4520402755696873,
            "step": 21920
        },
        {
            "loss": 0.3582,
            "grad_norm": 2.390747547149658,
            "learning_rate": 2.5777247836071365e-05,
            "epoch": 1.453365129835718,
            "step": 21940
        },
        {
            "loss": 0.3318,
            "grad_norm": 2.785080909729004,
            "learning_rate": 2.575516693163752e-05,
            "epoch": 1.4546899841017489,
            "step": 21960
        },
        {
            "loss": 0.5018,
            "grad_norm": 1.1517138481140137,
            "learning_rate": 2.5733086027203674e-05,
            "epoch": 1.4560148383677796,
            "step": 21980
        },
        {
            "loss": 0.4098,
            "grad_norm": 0.7745372653007507,
            "learning_rate": 2.571100512276983e-05,
            "epoch": 1.4573396926338102,
            "step": 22000
        },
        {
            "loss": 0.319,
            "grad_norm": 3.0930685997009277,
            "learning_rate": 2.5688924218335986e-05,
            "epoch": 1.4586645468998412,
            "step": 22020
        },
        {
            "loss": 0.4752,
            "grad_norm": 1.3108251094818115,
            "learning_rate": 2.566684331390214e-05,
            "epoch": 1.4599894011658718,
            "step": 22040
        },
        {
            "loss": 0.3469,
            "grad_norm": 0.6770437955856323,
            "learning_rate": 2.564476240946829e-05,
            "epoch": 1.4613142554319025,
            "step": 22060
        },
        {
            "loss": 0.426,
            "grad_norm": 1.7412493228912354,
            "learning_rate": 2.5622681505034446e-05,
            "epoch": 1.4626391096979332,
            "step": 22080
        },
        {
            "loss": 0.4092,
            "grad_norm": 0.9807854294776917,
            "learning_rate": 2.56006006006006e-05,
            "epoch": 1.4639639639639639,
            "step": 22100
        },
        {
            "loss": 0.4544,
            "grad_norm": 2.72910737991333,
            "learning_rate": 2.5578519696166758e-05,
            "epoch": 1.4652888182299946,
            "step": 22120
        },
        {
            "loss": 0.3344,
            "grad_norm": 1.2249956130981445,
            "learning_rate": 2.5556438791732913e-05,
            "epoch": 1.4666136724960255,
            "step": 22140
        },
        {
            "loss": 0.3835,
            "grad_norm": 1.676974892616272,
            "learning_rate": 2.5534357887299064e-05,
            "epoch": 1.4679385267620562,
            "step": 22160
        },
        {
            "loss": 0.3946,
            "grad_norm": 1.1057639122009277,
            "learning_rate": 2.5512276982865218e-05,
            "epoch": 1.4692633810280868,
            "step": 22180
        },
        {
            "loss": 0.4326,
            "grad_norm": 1.0729774236679077,
            "learning_rate": 2.5490196078431373e-05,
            "epoch": 1.4705882352941178,
            "step": 22200
        },
        {
            "loss": 0.3772,
            "grad_norm": 3.502037763595581,
            "learning_rate": 2.5468115173997527e-05,
            "epoch": 1.4719130895601484,
            "step": 22220
        },
        {
            "loss": 0.3424,
            "grad_norm": 0.48455268144607544,
            "learning_rate": 2.5446034269563685e-05,
            "epoch": 1.4732379438261791,
            "step": 22240
        },
        {
            "loss": 0.3342,
            "grad_norm": 0.9831498861312866,
            "learning_rate": 2.542395336512984e-05,
            "epoch": 1.4745627980922098,
            "step": 22260
        },
        {
            "loss": 0.4224,
            "grad_norm": 1.4867888689041138,
            "learning_rate": 2.540187246069599e-05,
            "epoch": 1.4758876523582405,
            "step": 22280
        },
        {
            "loss": 0.4148,
            "grad_norm": 0.7439035177230835,
            "learning_rate": 2.5379791556262145e-05,
            "epoch": 1.4772125066242714,
            "step": 22300
        },
        {
            "loss": 0.4016,
            "grad_norm": 2.0997016429901123,
            "learning_rate": 2.53577106518283e-05,
            "epoch": 1.478537360890302,
            "step": 22320
        },
        {
            "loss": 0.4525,
            "grad_norm": 1.582850694656372,
            "learning_rate": 2.5335629747394457e-05,
            "epoch": 1.4798622151563328,
            "step": 22340
        },
        {
            "loss": 0.3976,
            "grad_norm": 1.9495177268981934,
            "learning_rate": 2.531354884296061e-05,
            "epoch": 1.4811870694223637,
            "step": 22360
        },
        {
            "loss": 0.3859,
            "grad_norm": 34.11611557006836,
            "learning_rate": 2.5291467938526763e-05,
            "epoch": 1.4825119236883944,
            "step": 22380
        },
        {
            "loss": 0.3167,
            "grad_norm": 2.880267858505249,
            "learning_rate": 2.5269387034092917e-05,
            "epoch": 1.483836777954425,
            "step": 22400
        },
        {
            "loss": 0.3873,
            "grad_norm": 1.0308330059051514,
            "learning_rate": 2.524730612965907e-05,
            "epoch": 1.4851616322204557,
            "step": 22420
        },
        {
            "loss": 0.4217,
            "grad_norm": 4.022214889526367,
            "learning_rate": 2.5225225225225222e-05,
            "epoch": 1.4864864864864864,
            "step": 22440
        },
        {
            "loss": 0.3903,
            "grad_norm": 2.0881905555725098,
            "learning_rate": 2.5203144320791384e-05,
            "epoch": 1.487811340752517,
            "step": 22460
        },
        {
            "loss": 0.3483,
            "grad_norm": 3.043435573577881,
            "learning_rate": 2.5181063416357538e-05,
            "epoch": 1.489136195018548,
            "step": 22480
        },
        {
            "loss": 0.3484,
            "grad_norm": 3.1509206295013428,
            "learning_rate": 2.515898251192369e-05,
            "epoch": 1.4904610492845787,
            "step": 22500
        },
        {
            "loss": 0.3696,
            "grad_norm": 0.823827862739563,
            "learning_rate": 2.5136901607489844e-05,
            "epoch": 1.4917859035506094,
            "step": 22520
        },
        {
            "loss": 0.3793,
            "grad_norm": 1.6863861083984375,
            "learning_rate": 2.5114820703055998e-05,
            "epoch": 1.4931107578166403,
            "step": 22540
        },
        {
            "loss": 0.4404,
            "grad_norm": 3.391575336456299,
            "learning_rate": 2.509273979862215e-05,
            "epoch": 1.494435612082671,
            "step": 22560
        },
        {
            "loss": 0.4039,
            "grad_norm": 1.3441842794418335,
            "learning_rate": 2.507065889418831e-05,
            "epoch": 1.4957604663487016,
            "step": 22580
        },
        {
            "loss": 0.3433,
            "grad_norm": 1.1061474084854126,
            "learning_rate": 2.504857798975446e-05,
            "epoch": 1.4970853206147323,
            "step": 22600
        },
        {
            "loss": 0.3747,
            "grad_norm": 2.678480863571167,
            "learning_rate": 2.5026497085320616e-05,
            "epoch": 1.498410174880763,
            "step": 22620
        },
        {
            "loss": 0.3165,
            "grad_norm": 2.2180519104003906,
            "learning_rate": 2.500441618088677e-05,
            "epoch": 1.499735029146794,
            "step": 22640
        },
        {
            "loss": 0.3866,
            "grad_norm": 1.9834297895431519,
            "learning_rate": 2.4982335276452925e-05,
            "epoch": 1.5010598834128246,
            "step": 22660
        },
        {
            "loss": 0.4045,
            "grad_norm": 0.7175257802009583,
            "learning_rate": 2.496025437201908e-05,
            "epoch": 1.5023847376788553,
            "step": 22680
        },
        {
            "loss": 0.3464,
            "grad_norm": 1.4945290088653564,
            "learning_rate": 2.4938173467585234e-05,
            "epoch": 1.5037095919448862,
            "step": 22700
        },
        {
            "loss": 0.4435,
            "grad_norm": 2.0002686977386475,
            "learning_rate": 2.4916092563151388e-05,
            "epoch": 1.5050344462109169,
            "step": 22720
        },
        {
            "loss": 0.3295,
            "grad_norm": 1.096522331237793,
            "learning_rate": 2.4894011658717543e-05,
            "epoch": 1.5063593004769475,
            "step": 22740
        },
        {
            "loss": 0.3763,
            "grad_norm": 1.059997797012329,
            "learning_rate": 2.4871930754283697e-05,
            "epoch": 1.5076841547429782,
            "step": 22760
        },
        {
            "loss": 0.377,
            "grad_norm": 0.6058964133262634,
            "learning_rate": 2.484984984984985e-05,
            "epoch": 1.509009009009009,
            "step": 22780
        },
        {
            "loss": 0.3819,
            "grad_norm": 2.8283979892730713,
            "learning_rate": 2.4827768945416006e-05,
            "epoch": 1.5103338632750396,
            "step": 22800
        },
        {
            "loss": 0.3402,
            "grad_norm": 0.9091123938560486,
            "learning_rate": 2.4805688040982157e-05,
            "epoch": 1.5116587175410705,
            "step": 22820
        },
        {
            "loss": 0.3661,
            "grad_norm": 2.818331480026245,
            "learning_rate": 2.4783607136548315e-05,
            "epoch": 1.5129835718071012,
            "step": 22840
        },
        {
            "loss": 0.3695,
            "grad_norm": 0.9385946989059448,
            "learning_rate": 2.476152623211447e-05,
            "epoch": 1.514308426073132,
            "step": 22860
        },
        {
            "loss": 0.3517,
            "grad_norm": 0.8321818709373474,
            "learning_rate": 2.4739445327680624e-05,
            "epoch": 1.5156332803391628,
            "step": 22880
        },
        {
            "loss": 0.408,
            "grad_norm": 0.9403988122940063,
            "learning_rate": 2.4717364423246778e-05,
            "epoch": 1.5169581346051935,
            "step": 22900
        },
        {
            "loss": 0.3441,
            "grad_norm": 1.4250802993774414,
            "learning_rate": 2.4695283518812933e-05,
            "epoch": 1.5182829888712241,
            "step": 22920
        },
        {
            "loss": 0.416,
            "grad_norm": 1.3167074918746948,
            "learning_rate": 2.4673202614379087e-05,
            "epoch": 1.5196078431372548,
            "step": 22940
        },
        {
            "loss": 0.3912,
            "grad_norm": 0.8219124674797058,
            "learning_rate": 2.465112170994524e-05,
            "epoch": 1.5209326974032855,
            "step": 22960
        },
        {
            "loss": 0.4139,
            "grad_norm": 2.103752374649048,
            "learning_rate": 2.4629040805511396e-05,
            "epoch": 1.5222575516693164,
            "step": 22980
        },
        {
            "loss": 0.3984,
            "grad_norm": 0.7477485537528992,
            "learning_rate": 2.460695990107755e-05,
            "epoch": 1.523582405935347,
            "step": 23000
        },
        {
            "loss": 0.3301,
            "grad_norm": 0.7994176149368286,
            "learning_rate": 2.4584878996643705e-05,
            "epoch": 1.524907260201378,
            "step": 23020
        },
        {
            "loss": 0.3687,
            "grad_norm": 1.0770928859710693,
            "learning_rate": 2.4562798092209856e-05,
            "epoch": 1.5262321144674087,
            "step": 23040
        },
        {
            "loss": 0.3825,
            "grad_norm": 1.6889152526855469,
            "learning_rate": 2.4540717187776014e-05,
            "epoch": 1.5275569687334394,
            "step": 23060
        },
        {
            "loss": 0.3907,
            "grad_norm": 2.1583242416381836,
            "learning_rate": 2.4518636283342168e-05,
            "epoch": 1.52888182299947,
            "step": 23080
        },
        {
            "loss": 0.3574,
            "grad_norm": 1.9430581331253052,
            "learning_rate": 2.449655537890832e-05,
            "epoch": 1.5302066772655007,
            "step": 23100
        },
        {
            "loss": 0.3333,
            "grad_norm": 1.3661874532699585,
            "learning_rate": 2.4474474474474477e-05,
            "epoch": 1.5315315315315314,
            "step": 23120
        },
        {
            "loss": 0.3397,
            "grad_norm": 0.702081024646759,
            "learning_rate": 2.445239357004063e-05,
            "epoch": 1.5328563857975621,
            "step": 23140
        },
        {
            "loss": 0.3737,
            "grad_norm": 1.3613277673721313,
            "learning_rate": 2.4430312665606782e-05,
            "epoch": 1.534181240063593,
            "step": 23160
        },
        {
            "loss": 0.3971,
            "grad_norm": 3.138981819152832,
            "learning_rate": 2.440823176117294e-05,
            "epoch": 1.5355060943296237,
            "step": 23180
        },
        {
            "loss": 0.3567,
            "grad_norm": 1.303261637687683,
            "learning_rate": 2.438615085673909e-05,
            "epoch": 1.5368309485956546,
            "step": 23200
        },
        {
            "loss": 0.3463,
            "grad_norm": 2.0556459426879883,
            "learning_rate": 2.4364069952305246e-05,
            "epoch": 1.5381558028616853,
            "step": 23220
        },
        {
            "loss": 0.397,
            "grad_norm": 2.0845746994018555,
            "learning_rate": 2.4341989047871404e-05,
            "epoch": 1.539480657127716,
            "step": 23240
        },
        {
            "loss": 0.3988,
            "grad_norm": 0.6297532320022583,
            "learning_rate": 2.4319908143437555e-05,
            "epoch": 1.5408055113937467,
            "step": 23260
        },
        {
            "loss": 0.3716,
            "grad_norm": 1.1351507902145386,
            "learning_rate": 2.4297827239003712e-05,
            "epoch": 1.5421303656597773,
            "step": 23280
        },
        {
            "loss": 0.4412,
            "grad_norm": 2.102832078933716,
            "learning_rate": 2.4275746334569867e-05,
            "epoch": 1.543455219925808,
            "step": 23300
        },
        {
            "loss": 0.4295,
            "grad_norm": 27.59033203125,
            "learning_rate": 2.4253665430136018e-05,
            "epoch": 1.544780074191839,
            "step": 23320
        },
        {
            "loss": 0.4108,
            "grad_norm": 0.6084385514259338,
            "learning_rate": 2.4231584525702176e-05,
            "epoch": 1.5461049284578696,
            "step": 23340
        },
        {
            "loss": 0.3504,
            "grad_norm": 1.1997452974319458,
            "learning_rate": 2.4209503621268327e-05,
            "epoch": 1.5474297827239005,
            "step": 23360
        },
        {
            "loss": 0.3577,
            "grad_norm": 0.983056902885437,
            "learning_rate": 2.418742271683448e-05,
            "epoch": 1.5487546369899312,
            "step": 23380
        },
        {
            "loss": 0.3855,
            "grad_norm": 3.658170700073242,
            "learning_rate": 2.416534181240064e-05,
            "epoch": 1.550079491255962,
            "step": 23400
        },
        {
            "loss": 0.4141,
            "grad_norm": 1.9945933818817139,
            "learning_rate": 2.414326090796679e-05,
            "epoch": 1.5514043455219926,
            "step": 23420
        },
        {
            "loss": 0.3858,
            "grad_norm": 2.017742156982422,
            "learning_rate": 2.4121180003532945e-05,
            "epoch": 1.5527291997880233,
            "step": 23440
        },
        {
            "loss": 0.3779,
            "grad_norm": 1.272164225578308,
            "learning_rate": 2.4099099099099102e-05,
            "epoch": 1.554054054054054,
            "step": 23460
        },
        {
            "loss": 0.3501,
            "grad_norm": 1.7736536264419556,
            "learning_rate": 2.4077018194665254e-05,
            "epoch": 1.5553789083200846,
            "step": 23480
        },
        {
            "loss": 0.3813,
            "grad_norm": 1.3152687549591064,
            "learning_rate": 2.4054937290231408e-05,
            "epoch": 1.5567037625861155,
            "step": 23500
        },
        {
            "loss": 0.3817,
            "grad_norm": 3.4106130599975586,
            "learning_rate": 2.4032856385797566e-05,
            "epoch": 1.5580286168521462,
            "step": 23520
        },
        {
            "loss": 0.3794,
            "grad_norm": 4.148656368255615,
            "learning_rate": 2.4010775481363717e-05,
            "epoch": 1.5593534711181771,
            "step": 23540
        },
        {
            "loss": 0.417,
            "grad_norm": 0.8990389704704285,
            "learning_rate": 2.398869457692987e-05,
            "epoch": 1.5606783253842078,
            "step": 23560
        },
        {
            "loss": 0.4713,
            "grad_norm": 1.4216655492782593,
            "learning_rate": 2.3966613672496026e-05,
            "epoch": 1.5620031796502385,
            "step": 23580
        },
        {
            "loss": 0.3582,
            "grad_norm": 1.9677354097366333,
            "learning_rate": 2.394453276806218e-05,
            "epoch": 1.5633280339162692,
            "step": 23600
        },
        {
            "loss": 0.4037,
            "grad_norm": 4.065940856933594,
            "learning_rate": 2.3922451863628335e-05,
            "epoch": 1.5646528881822999,
            "step": 23620
        },
        {
            "loss": 0.4356,
            "grad_norm": 1.0177098512649536,
            "learning_rate": 2.390037095919449e-05,
            "epoch": 1.5659777424483305,
            "step": 23640
        },
        {
            "loss": 0.3847,
            "grad_norm": 0.45754343271255493,
            "learning_rate": 2.3878290054760643e-05,
            "epoch": 1.5673025967143615,
            "step": 23660
        },
        {
            "loss": 0.3794,
            "grad_norm": 2.260411024093628,
            "learning_rate": 2.38562091503268e-05,
            "epoch": 1.5686274509803921,
            "step": 23680
        },
        {
            "loss": 0.3509,
            "grad_norm": 1.2205866575241089,
            "learning_rate": 2.3834128245892952e-05,
            "epoch": 1.569952305246423,
            "step": 23700
        },
        {
            "loss": 0.3798,
            "grad_norm": 0.9629526138305664,
            "learning_rate": 2.3812047341459107e-05,
            "epoch": 1.5712771595124537,
            "step": 23720
        },
        {
            "loss": 0.3458,
            "grad_norm": 3.6411380767822266,
            "learning_rate": 2.378996643702526e-05,
            "epoch": 1.5726020137784844,
            "step": 23740
        },
        {
            "loss": 0.3898,
            "grad_norm": 0.7681859731674194,
            "learning_rate": 2.3767885532591416e-05,
            "epoch": 1.573926868044515,
            "step": 23760
        },
        {
            "loss": 0.3765,
            "grad_norm": 2.03104829788208,
            "learning_rate": 2.374580462815757e-05,
            "epoch": 1.5752517223105458,
            "step": 23780
        },
        {
            "loss": 0.3485,
            "grad_norm": 0.9462175965309143,
            "learning_rate": 2.3723723723723725e-05,
            "epoch": 1.5765765765765765,
            "step": 23800
        },
        {
            "loss": 0.3709,
            "grad_norm": 1.891663908958435,
            "learning_rate": 2.370164281928988e-05,
            "epoch": 1.5779014308426074,
            "step": 23820
        },
        {
            "loss": 0.3856,
            "grad_norm": 0.7369021773338318,
            "learning_rate": 2.3679561914856033e-05,
            "epoch": 1.579226285108638,
            "step": 23840
        },
        {
            "loss": 0.3799,
            "grad_norm": 1.858727216720581,
            "learning_rate": 2.3657481010422188e-05,
            "epoch": 1.5805511393746687,
            "step": 23860
        },
        {
            "loss": 0.359,
            "grad_norm": 3.0179381370544434,
            "learning_rate": 2.3635400105988342e-05,
            "epoch": 1.5818759936406996,
            "step": 23880
        },
        {
            "loss": 0.3464,
            "grad_norm": 2.197911500930786,
            "learning_rate": 2.3613319201554497e-05,
            "epoch": 1.5832008479067303,
            "step": 23900
        },
        {
            "loss": 0.3795,
            "grad_norm": 2.3915936946868896,
            "learning_rate": 2.359123829712065e-05,
            "epoch": 1.584525702172761,
            "step": 23920
        },
        {
            "loss": 0.3642,
            "grad_norm": 1.6680541038513184,
            "learning_rate": 2.3569157392686806e-05,
            "epoch": 1.5858505564387917,
            "step": 23940
        },
        {
            "loss": 0.394,
            "grad_norm": 1.2783316373825073,
            "learning_rate": 2.354707648825296e-05,
            "epoch": 1.5871754107048224,
            "step": 23960
        },
        {
            "loss": 0.3777,
            "grad_norm": 1.4490063190460205,
            "learning_rate": 2.3524995583819115e-05,
            "epoch": 1.588500264970853,
            "step": 23980
        },
        {
            "loss": 0.4196,
            "grad_norm": 2.0498249530792236,
            "learning_rate": 2.350291467938527e-05,
            "epoch": 1.589825119236884,
            "step": 24000
        },
        {
            "loss": 0.4178,
            "grad_norm": 2.7698912620544434,
            "learning_rate": 2.3480833774951423e-05,
            "epoch": 1.5911499735029146,
            "step": 24020
        },
        {
            "loss": 0.4127,
            "grad_norm": 1.6956239938735962,
            "learning_rate": 2.3458752870517578e-05,
            "epoch": 1.5924748277689456,
            "step": 24040
        },
        {
            "loss": 0.3449,
            "grad_norm": 1.3494077920913696,
            "learning_rate": 2.3436671966083732e-05,
            "epoch": 1.5937996820349762,
            "step": 24060
        },
        {
            "loss": 0.4476,
            "grad_norm": 2.4697799682617188,
            "learning_rate": 2.3414591061649887e-05,
            "epoch": 1.595124536301007,
            "step": 24080
        },
        {
            "loss": 0.3938,
            "grad_norm": 0.8300893902778625,
            "learning_rate": 2.339251015721604e-05,
            "epoch": 1.5964493905670376,
            "step": 24100
        },
        {
            "loss": 0.3807,
            "grad_norm": 2.658491849899292,
            "learning_rate": 2.3370429252782196e-05,
            "epoch": 1.5977742448330683,
            "step": 24120
        },
        {
            "loss": 0.3881,
            "grad_norm": 1.414715051651001,
            "learning_rate": 2.334834834834835e-05,
            "epoch": 1.599099099099099,
            "step": 24140
        },
        {
            "loss": 0.4136,
            "grad_norm": 0.9578255414962769,
            "learning_rate": 2.3326267443914505e-05,
            "epoch": 1.6004239533651299,
            "step": 24160
        },
        {
            "loss": 0.3847,
            "grad_norm": 2.694742441177368,
            "learning_rate": 2.330418653948066e-05,
            "epoch": 1.6017488076311606,
            "step": 24180
        },
        {
            "loss": 0.3497,
            "grad_norm": 1.7351748943328857,
            "learning_rate": 2.3282105635046813e-05,
            "epoch": 1.6030736618971915,
            "step": 24200
        },
        {
            "loss": 0.3688,
            "grad_norm": 3.316012144088745,
            "learning_rate": 2.3260024730612968e-05,
            "epoch": 1.6043985161632222,
            "step": 24220
        },
        {
            "loss": 0.3519,
            "grad_norm": 1.8383595943450928,
            "learning_rate": 2.323794382617912e-05,
            "epoch": 1.6057233704292528,
            "step": 24240
        },
        {
            "loss": 0.4189,
            "grad_norm": 36.214637756347656,
            "learning_rate": 2.3215862921745277e-05,
            "epoch": 1.6070482246952835,
            "step": 24260
        },
        {
            "loss": 0.3988,
            "grad_norm": 1.5735632181167603,
            "learning_rate": 2.319378201731143e-05,
            "epoch": 1.6083730789613142,
            "step": 24280
        },
        {
            "loss": 0.4424,
            "grad_norm": 1.899833083152771,
            "learning_rate": 2.3171701112877582e-05,
            "epoch": 1.609697933227345,
            "step": 24300
        },
        {
            "loss": 0.3958,
            "grad_norm": 2.0514473915100098,
            "learning_rate": 2.314962020844374e-05,
            "epoch": 1.6110227874933756,
            "step": 24320
        },
        {
            "loss": 0.3449,
            "grad_norm": 0.6716446876525879,
            "learning_rate": 2.3127539304009895e-05,
            "epoch": 1.6123476417594065,
            "step": 24340
        },
        {
            "loss": 0.3845,
            "grad_norm": 0.518949031829834,
            "learning_rate": 2.3105458399576046e-05,
            "epoch": 1.6136724960254372,
            "step": 24360
        },
        {
            "loss": 0.3853,
            "grad_norm": 2.675416946411133,
            "learning_rate": 2.3083377495142203e-05,
            "epoch": 1.614997350291468,
            "step": 24380
        },
        {
            "loss": 0.3952,
            "grad_norm": 1.4928418397903442,
            "learning_rate": 2.3061296590708354e-05,
            "epoch": 1.6163222045574988,
            "step": 24400
        },
        {
            "loss": 0.3799,
            "grad_norm": 0.5281102657318115,
            "learning_rate": 2.303921568627451e-05,
            "epoch": 1.6176470588235294,
            "step": 24420
        },
        {
            "loss": 0.3641,
            "grad_norm": 2.1058483123779297,
            "learning_rate": 2.3017134781840667e-05,
            "epoch": 1.6189719130895601,
            "step": 24440
        },
        {
            "loss": 0.4119,
            "grad_norm": 0.835732102394104,
            "learning_rate": 2.2995053877406818e-05,
            "epoch": 1.6202967673555908,
            "step": 24460
        },
        {
            "loss": 0.4086,
            "grad_norm": 0.8559969067573547,
            "learning_rate": 2.2972972972972976e-05,
            "epoch": 1.6216216216216215,
            "step": 24480
        },
        {
            "loss": 0.4306,
            "grad_norm": 1.0650697946548462,
            "learning_rate": 2.295089206853913e-05,
            "epoch": 1.6229464758876524,
            "step": 24500
        },
        {
            "loss": 0.3741,
            "grad_norm": 0.8818148970603943,
            "learning_rate": 2.292881116410528e-05,
            "epoch": 1.624271330153683,
            "step": 24520
        },
        {
            "loss": 0.3693,
            "grad_norm": 1.5539627075195312,
            "learning_rate": 2.290673025967144e-05,
            "epoch": 1.625596184419714,
            "step": 24540
        },
        {
            "loss": 0.3933,
            "grad_norm": 1.0042524337768555,
            "learning_rate": 2.2884649355237593e-05,
            "epoch": 1.6269210386857447,
            "step": 24560
        },
        {
            "loss": 0.3628,
            "grad_norm": 4.167697906494141,
            "learning_rate": 2.2862568450803744e-05,
            "epoch": 1.6282458929517754,
            "step": 24580
        },
        {
            "loss": 0.3618,
            "grad_norm": 1.0817430019378662,
            "learning_rate": 2.2840487546369902e-05,
            "epoch": 1.629570747217806,
            "step": 24600
        },
        {
            "loss": 0.4101,
            "grad_norm": 0.7699588537216187,
            "learning_rate": 2.2818406641936053e-05,
            "epoch": 1.6308956014838367,
            "step": 24620
        },
        {
            "loss": 0.3616,
            "grad_norm": 5.537390232086182,
            "learning_rate": 2.2796325737502208e-05,
            "epoch": 1.6322204557498674,
            "step": 24640
        },
        {
            "loss": 0.3813,
            "grad_norm": 0.5740407109260559,
            "learning_rate": 2.2774244833068366e-05,
            "epoch": 1.633545310015898,
            "step": 24660
        },
        {
            "loss": 0.3864,
            "grad_norm": 1.261810064315796,
            "learning_rate": 2.2752163928634517e-05,
            "epoch": 1.634870164281929,
            "step": 24680
        },
        {
            "loss": 0.3761,
            "grad_norm": 20.69923210144043,
            "learning_rate": 2.273008302420067e-05,
            "epoch": 1.6361950185479597,
            "step": 24700
        },
        {
            "loss": 0.4096,
            "grad_norm": 1.3318777084350586,
            "learning_rate": 2.270800211976683e-05,
            "epoch": 1.6375198728139906,
            "step": 24720
        },
        {
            "loss": 0.3853,
            "grad_norm": 2.045442819595337,
            "learning_rate": 2.268592121533298e-05,
            "epoch": 1.6388447270800213,
            "step": 24740
        },
        {
            "loss": 0.388,
            "grad_norm": 1.1820652484893799,
            "learning_rate": 2.2663840310899134e-05,
            "epoch": 1.640169581346052,
            "step": 24760
        },
        {
            "loss": 0.385,
            "grad_norm": 1.2025457620620728,
            "learning_rate": 2.264175940646529e-05,
            "epoch": 1.6414944356120826,
            "step": 24780
        },
        {
            "loss": 0.3589,
            "grad_norm": 1.252779483795166,
            "learning_rate": 2.2619678502031443e-05,
            "epoch": 1.6428192898781133,
            "step": 24800
        },
        {
            "loss": 0.3562,
            "grad_norm": 4.711751461029053,
            "learning_rate": 2.2597597597597598e-05,
            "epoch": 1.644144144144144,
            "step": 24820
        },
        {
            "loss": 0.3854,
            "grad_norm": 2.1441829204559326,
            "learning_rate": 2.2575516693163752e-05,
            "epoch": 1.645468998410175,
            "step": 24840
        },
        {
            "loss": 0.4051,
            "grad_norm": 1.981376051902771,
            "learning_rate": 2.2553435788729907e-05,
            "epoch": 1.6467938526762056,
            "step": 24860
        },
        {
            "loss": 0.4325,
            "grad_norm": 1.201419472694397,
            "learning_rate": 2.2531354884296064e-05,
            "epoch": 1.6481187069422365,
            "step": 24880
        },
        {
            "loss": 0.361,
            "grad_norm": 1.0163207054138184,
            "learning_rate": 2.2509273979862216e-05,
            "epoch": 1.6494435612082672,
            "step": 24900
        },
        {
            "loss": 0.432,
            "grad_norm": 1.1478222608566284,
            "learning_rate": 2.248719307542837e-05,
            "epoch": 1.6507684154742979,
            "step": 24920
        },
        {
            "loss": 0.3763,
            "grad_norm": 2.3113760948181152,
            "learning_rate": 2.2465112170994528e-05,
            "epoch": 1.6520932697403286,
            "step": 24940
        },
        {
            "loss": 0.4397,
            "grad_norm": 2.1273884773254395,
            "learning_rate": 2.244303126656068e-05,
            "epoch": 1.6534181240063592,
            "step": 24960
        },
        {
            "loss": 0.3895,
            "grad_norm": 1.0119940042495728,
            "learning_rate": 2.2420950362126833e-05,
            "epoch": 1.65474297827239,
            "step": 24980
        },
        {
            "loss": 0.3721,
            "grad_norm": 2.4780306816101074,
            "learning_rate": 2.2398869457692988e-05,
            "epoch": 1.6560678325384208,
            "step": 25000
        },
        {
            "loss": 0.3475,
            "grad_norm": 2.15718936920166,
            "learning_rate": 2.2376788553259142e-05,
            "epoch": 1.6573926868044515,
            "step": 25020
        },
        {
            "loss": 0.3837,
            "grad_norm": 0.9924262762069702,
            "learning_rate": 2.2354707648825297e-05,
            "epoch": 1.6587175410704822,
            "step": 25040
        },
        {
            "loss": 0.3491,
            "grad_norm": 0.7580395936965942,
            "learning_rate": 2.233262674439145e-05,
            "epoch": 1.660042395336513,
            "step": 25060
        },
        {
            "loss": 0.4578,
            "grad_norm": 1.4292521476745605,
            "learning_rate": 2.2310545839957606e-05,
            "epoch": 1.6613672496025438,
            "step": 25080
        },
        {
            "loss": 0.362,
            "grad_norm": 1.419512152671814,
            "learning_rate": 2.228846493552376e-05,
            "epoch": 1.6626921038685745,
            "step": 25100
        },
        {
            "loss": 0.4227,
            "grad_norm": 4.861110210418701,
            "learning_rate": 2.2266384031089914e-05,
            "epoch": 1.6640169581346052,
            "step": 25120
        },
        {
            "loss": 0.4369,
            "grad_norm": 2.2727932929992676,
            "learning_rate": 2.224430312665607e-05,
            "epoch": 1.6653418124006358,
            "step": 25140
        },
        {
            "loss": 0.3895,
            "grad_norm": 2.2011327743530273,
            "learning_rate": 2.2222222222222223e-05,
            "epoch": 1.6666666666666665,
            "step": 25160
        },
        {
            "loss": 0.407,
            "grad_norm": 3.260472059249878,
            "learning_rate": 2.2200141317788378e-05,
            "epoch": 1.6679915209326974,
            "step": 25180
        },
        {
            "loss": 0.3473,
            "grad_norm": 27.850406646728516,
            "learning_rate": 2.2178060413354532e-05,
            "epoch": 1.669316375198728,
            "step": 25200
        },
        {
            "loss": 0.4303,
            "grad_norm": 2.6639907360076904,
            "learning_rate": 2.2155979508920687e-05,
            "epoch": 1.670641229464759,
            "step": 25220
        },
        {
            "loss": 0.4766,
            "grad_norm": 1.932528018951416,
            "learning_rate": 2.213389860448684e-05,
            "epoch": 1.6719660837307897,
            "step": 25240
        },
        {
            "loss": 0.4288,
            "grad_norm": 2.543422222137451,
            "learning_rate": 2.2111817700052995e-05,
            "epoch": 1.6732909379968204,
            "step": 25260
        },
        {
            "loss": 0.385,
            "grad_norm": 1.2157076597213745,
            "learning_rate": 2.208973679561915e-05,
            "epoch": 1.674615792262851,
            "step": 25280
        },
        {
            "loss": 0.4187,
            "grad_norm": 1.9513020515441895,
            "learning_rate": 2.2067655891185304e-05,
            "epoch": 1.6759406465288818,
            "step": 25300
        },
        {
            "loss": 0.331,
            "grad_norm": 0.6397936940193176,
            "learning_rate": 2.204557498675146e-05,
            "epoch": 1.6772655007949124,
            "step": 25320
        },
        {
            "loss": 0.371,
            "grad_norm": 1.9111826419830322,
            "learning_rate": 2.2023494082317613e-05,
            "epoch": 1.6785903550609433,
            "step": 25340
        },
        {
            "loss": 0.4192,
            "grad_norm": 5.9007062911987305,
            "learning_rate": 2.2001413177883768e-05,
            "epoch": 1.679915209326974,
            "step": 25360
        },
        {
            "loss": 0.3806,
            "grad_norm": 2.0314085483551025,
            "learning_rate": 2.1979332273449922e-05,
            "epoch": 1.681240063593005,
            "step": 25380
        },
        {
            "loss": 0.3803,
            "grad_norm": 2.761843681335449,
            "learning_rate": 2.1957251369016077e-05,
            "epoch": 1.6825649178590356,
            "step": 25400
        },
        {
            "loss": 0.3975,
            "grad_norm": 3.357790946960449,
            "learning_rate": 2.193517046458223e-05,
            "epoch": 1.6838897721250663,
            "step": 25420
        },
        {
            "loss": 0.3542,
            "grad_norm": 0.5166599750518799,
            "learning_rate": 2.1913089560148382e-05,
            "epoch": 1.685214626391097,
            "step": 25440
        },
        {
            "loss": 0.4541,
            "grad_norm": 10.730010986328125,
            "learning_rate": 2.189100865571454e-05,
            "epoch": 1.6865394806571277,
            "step": 25460
        },
        {
            "loss": 0.3844,
            "grad_norm": 1.621995210647583,
            "learning_rate": 2.1868927751280694e-05,
            "epoch": 1.6878643349231583,
            "step": 25480
        },
        {
            "loss": 0.3564,
            "grad_norm": 0.6299605965614319,
            "learning_rate": 2.1846846846846845e-05,
            "epoch": 1.689189189189189,
            "step": 25500
        },
        {
            "loss": 0.3339,
            "grad_norm": 3.2925710678100586,
            "learning_rate": 2.1824765942413003e-05,
            "epoch": 1.69051404345522,
            "step": 25520
        },
        {
            "loss": 0.4184,
            "grad_norm": 2.4639739990234375,
            "learning_rate": 2.1802685037979158e-05,
            "epoch": 1.6918388977212506,
            "step": 25540
        },
        {
            "loss": 0.4126,
            "grad_norm": 1.0894367694854736,
            "learning_rate": 2.178060413354531e-05,
            "epoch": 1.6931637519872815,
            "step": 25560
        },
        {
            "loss": 0.3118,
            "grad_norm": 1.7533273696899414,
            "learning_rate": 2.1758523229111467e-05,
            "epoch": 1.6944886062533122,
            "step": 25580
        },
        {
            "loss": 0.3161,
            "grad_norm": 4.844845771789551,
            "learning_rate": 2.173644232467762e-05,
            "epoch": 1.695813460519343,
            "step": 25600
        },
        {
            "loss": 0.3466,
            "grad_norm": 0.8433002829551697,
            "learning_rate": 2.1714361420243772e-05,
            "epoch": 1.6971383147853736,
            "step": 25620
        },
        {
            "loss": 0.3966,
            "grad_norm": 0.9841585159301758,
            "learning_rate": 2.169228051580993e-05,
            "epoch": 1.6984631690514043,
            "step": 25640
        },
        {
            "loss": 0.3489,
            "grad_norm": 0.6814509630203247,
            "learning_rate": 2.167019961137608e-05,
            "epoch": 1.699788023317435,
            "step": 25660
        },
        {
            "loss": 0.4194,
            "grad_norm": 0.8482334613800049,
            "learning_rate": 2.164811870694224e-05,
            "epoch": 1.7011128775834659,
            "step": 25680
        },
        {
            "loss": 0.4535,
            "grad_norm": 0.8429886698722839,
            "learning_rate": 2.1626037802508393e-05,
            "epoch": 1.7024377318494965,
            "step": 25700
        },
        {
            "loss": 0.5185,
            "grad_norm": 9.163312911987305,
            "learning_rate": 2.1603956898074544e-05,
            "epoch": 1.7037625861155274,
            "step": 25720
        },
        {
            "loss": 0.3887,
            "grad_norm": 45.22924041748047,
            "learning_rate": 2.1581875993640702e-05,
            "epoch": 1.7050874403815581,
            "step": 25740
        },
        {
            "loss": 0.3187,
            "grad_norm": 1.1794418096542358,
            "learning_rate": 2.1559795089206857e-05,
            "epoch": 1.7064122946475888,
            "step": 25760
        },
        {
            "loss": 0.3034,
            "grad_norm": 0.836519718170166,
            "learning_rate": 2.1537714184773008e-05,
            "epoch": 1.7077371489136195,
            "step": 25780
        },
        {
            "loss": 0.3831,
            "grad_norm": 1.9825724363327026,
            "learning_rate": 2.1515633280339165e-05,
            "epoch": 1.7090620031796502,
            "step": 25800
        },
        {
            "loss": 0.387,
            "grad_norm": 0.631525993347168,
            "learning_rate": 2.1493552375905317e-05,
            "epoch": 1.7103868574456809,
            "step": 25820
        },
        {
            "loss": 0.4765,
            "grad_norm": 60.553428649902344,
            "learning_rate": 2.147147147147147e-05,
            "epoch": 1.7117117117117115,
            "step": 25840
        },
        {
            "loss": 0.404,
            "grad_norm": 0.7347974181175232,
            "learning_rate": 2.144939056703763e-05,
            "epoch": 1.7130365659777425,
            "step": 25860
        },
        {
            "loss": 0.3557,
            "grad_norm": 0.6947106122970581,
            "learning_rate": 2.142730966260378e-05,
            "epoch": 1.7143614202437731,
            "step": 25880
        },
        {
            "loss": 0.3711,
            "grad_norm": 2.831047534942627,
            "learning_rate": 2.1405228758169934e-05,
            "epoch": 1.715686274509804,
            "step": 25900
        },
        {
            "loss": 0.3602,
            "grad_norm": 1.2548903226852417,
            "learning_rate": 2.1383147853736092e-05,
            "epoch": 1.7170111287758347,
            "step": 25920
        },
        {
            "loss": 0.395,
            "grad_norm": 1.1388822793960571,
            "learning_rate": 2.1361066949302243e-05,
            "epoch": 1.7183359830418654,
            "step": 25940
        },
        {
            "loss": 0.3675,
            "grad_norm": 1.3174494504928589,
            "learning_rate": 2.1338986044868398e-05,
            "epoch": 1.719660837307896,
            "step": 25960
        },
        {
            "loss": 0.3742,
            "grad_norm": 1.9293774366378784,
            "learning_rate": 2.1316905140434555e-05,
            "epoch": 1.7209856915739268,
            "step": 25980
        },
        {
            "loss": 0.3883,
            "grad_norm": 3.829077959060669,
            "learning_rate": 2.1294824236000706e-05,
            "epoch": 1.7223105458399575,
            "step": 26000
        },
        {
            "loss": 0.3855,
            "grad_norm": 1.3138325214385986,
            "learning_rate": 2.127274333156686e-05,
            "epoch": 1.7236354001059884,
            "step": 26020
        },
        {
            "loss": 0.3503,
            "grad_norm": 5.053769111633301,
            "learning_rate": 2.1250662427133015e-05,
            "epoch": 1.724960254372019,
            "step": 26040
        },
        {
            "loss": 0.3714,
            "grad_norm": 2.0973362922668457,
            "learning_rate": 2.122858152269917e-05,
            "epoch": 1.72628510863805,
            "step": 26060
        },
        {
            "loss": 0.3794,
            "grad_norm": 0.939991295337677,
            "learning_rate": 2.1206500618265328e-05,
            "epoch": 1.7276099629040806,
            "step": 26080
        },
        {
            "loss": 0.391,
            "grad_norm": 2.617182731628418,
            "learning_rate": 2.118441971383148e-05,
            "epoch": 1.7289348171701113,
            "step": 26100
        },
        {
            "loss": 0.4569,
            "grad_norm": 0.6483941078186035,
            "learning_rate": 2.1162338809397633e-05,
            "epoch": 1.730259671436142,
            "step": 26120
        },
        {
            "loss": 0.3722,
            "grad_norm": 2.05572247505188,
            "learning_rate": 2.114025790496379e-05,
            "epoch": 1.7315845257021727,
            "step": 26140
        },
        {
            "loss": 0.3688,
            "grad_norm": 0.536177933216095,
            "learning_rate": 2.1118177000529942e-05,
            "epoch": 1.7329093799682034,
            "step": 26160
        },
        {
            "loss": 0.3382,
            "grad_norm": 0.411622554063797,
            "learning_rate": 2.1096096096096096e-05,
            "epoch": 1.7342342342342343,
            "step": 26180
        },
        {
            "loss": 0.3575,
            "grad_norm": 0.7527443170547485,
            "learning_rate": 2.107401519166225e-05,
            "epoch": 1.735559088500265,
            "step": 26200
        },
        {
            "loss": 0.3769,
            "grad_norm": 1.137202262878418,
            "learning_rate": 2.1051934287228405e-05,
            "epoch": 1.7368839427662957,
            "step": 26220
        },
        {
            "loss": 0.3635,
            "grad_norm": 0.9611105918884277,
            "learning_rate": 2.102985338279456e-05,
            "epoch": 1.7382087970323266,
            "step": 26240
        },
        {
            "loss": 0.4027,
            "grad_norm": 0.8299526572227478,
            "learning_rate": 2.1007772478360714e-05,
            "epoch": 1.7395336512983572,
            "step": 26260
        },
        {
            "loss": 0.3395,
            "grad_norm": 2.7513346672058105,
            "learning_rate": 2.098569157392687e-05,
            "epoch": 1.740858505564388,
            "step": 26280
        },
        {
            "loss": 0.3701,
            "grad_norm": 3.556137800216675,
            "learning_rate": 2.0963610669493023e-05,
            "epoch": 1.7421833598304186,
            "step": 26300
        },
        {
            "loss": 0.4013,
            "grad_norm": 2.624338388442993,
            "learning_rate": 2.0941529765059178e-05,
            "epoch": 1.7435082140964493,
            "step": 26320
        },
        {
            "loss": 0.3218,
            "grad_norm": 2.0389087200164795,
            "learning_rate": 2.0919448860625332e-05,
            "epoch": 1.74483306836248,
            "step": 26340
        },
        {
            "loss": 0.3877,
            "grad_norm": 1.3228464126586914,
            "learning_rate": 2.0897367956191486e-05,
            "epoch": 1.7461579226285109,
            "step": 26360
        },
        {
            "loss": 0.3594,
            "grad_norm": 1.0434731245040894,
            "learning_rate": 2.087528705175764e-05,
            "epoch": 1.7474827768945416,
            "step": 26380
        },
        {
            "loss": 0.4082,
            "grad_norm": 1.3131239414215088,
            "learning_rate": 2.0853206147323795e-05,
            "epoch": 1.7488076311605725,
            "step": 26400
        },
        {
            "loss": 0.3908,
            "grad_norm": 1.2986713647842407,
            "learning_rate": 2.083112524288995e-05,
            "epoch": 1.7501324854266032,
            "step": 26420
        },
        {
            "loss": 0.4285,
            "grad_norm": 18.403947830200195,
            "learning_rate": 2.0809044338456104e-05,
            "epoch": 1.7514573396926338,
            "step": 26440
        },
        {
            "loss": 0.4898,
            "grad_norm": 1.9836469888687134,
            "learning_rate": 2.078696343402226e-05,
            "epoch": 1.7527821939586645,
            "step": 26460
        },
        {
            "loss": 0.3652,
            "grad_norm": 0.6969223618507385,
            "learning_rate": 2.0764882529588413e-05,
            "epoch": 1.7541070482246952,
            "step": 26480
        },
        {
            "loss": 0.3773,
            "grad_norm": 4.122799396514893,
            "learning_rate": 2.0742801625154568e-05,
            "epoch": 1.755431902490726,
            "step": 26500
        },
        {
            "loss": 0.3788,
            "grad_norm": 2.832531452178955,
            "learning_rate": 2.0720720720720722e-05,
            "epoch": 1.7567567567567568,
            "step": 26520
        },
        {
            "loss": 0.3724,
            "grad_norm": 3.362583875656128,
            "learning_rate": 2.0698639816286876e-05,
            "epoch": 1.7580816110227875,
            "step": 26540
        },
        {
            "loss": 0.379,
            "grad_norm": 2.970351219177246,
            "learning_rate": 2.067655891185303e-05,
            "epoch": 1.7594064652888184,
            "step": 26560
        },
        {
            "loss": 0.4829,
            "grad_norm": 3.6159751415252686,
            "learning_rate": 2.0654478007419185e-05,
            "epoch": 1.760731319554849,
            "step": 26580
        },
        {
            "loss": 0.365,
            "grad_norm": 3.4406940937042236,
            "learning_rate": 2.063239710298534e-05,
            "epoch": 1.7620561738208798,
            "step": 26600
        },
        {
            "loss": 0.3817,
            "grad_norm": 1.052492618560791,
            "learning_rate": 2.0610316198551494e-05,
            "epoch": 1.7633810280869104,
            "step": 26620
        },
        {
            "loss": 0.3578,
            "grad_norm": 0.6830037236213684,
            "learning_rate": 2.058823529411765e-05,
            "epoch": 1.7647058823529411,
            "step": 26640
        },
        {
            "loss": 0.3836,
            "grad_norm": 3.8843955993652344,
            "learning_rate": 2.0566154389683803e-05,
            "epoch": 1.7660307366189718,
            "step": 26660
        },
        {
            "loss": 0.3287,
            "grad_norm": 0.9253038763999939,
            "learning_rate": 2.0544073485249958e-05,
            "epoch": 1.7673555908850025,
            "step": 26680
        },
        {
            "loss": 0.3709,
            "grad_norm": 1.8348784446716309,
            "learning_rate": 2.052199258081611e-05,
            "epoch": 1.7686804451510334,
            "step": 26700
        },
        {
            "loss": 0.3674,
            "grad_norm": 1.7836443185806274,
            "learning_rate": 2.0499911676382266e-05,
            "epoch": 1.770005299417064,
            "step": 26720
        },
        {
            "loss": 0.3323,
            "grad_norm": 0.7690187096595764,
            "learning_rate": 2.047783077194842e-05,
            "epoch": 1.771330153683095,
            "step": 26740
        },
        {
            "loss": 0.3595,
            "grad_norm": 0.603100597858429,
            "learning_rate": 2.0455749867514572e-05,
            "epoch": 1.7726550079491257,
            "step": 26760
        },
        {
            "loss": 0.4389,
            "grad_norm": 3.778430938720703,
            "learning_rate": 2.043366896308073e-05,
            "epoch": 1.7739798622151564,
            "step": 26780
        },
        {
            "loss": 0.4004,
            "grad_norm": 0.9290494918823242,
            "learning_rate": 2.0411588058646884e-05,
            "epoch": 1.775304716481187,
            "step": 26800
        },
        {
            "loss": 0.4343,
            "grad_norm": 1.3870432376861572,
            "learning_rate": 2.038950715421304e-05,
            "epoch": 1.7766295707472177,
            "step": 26820
        },
        {
            "loss": 0.3266,
            "grad_norm": 3.438331127166748,
            "learning_rate": 2.0367426249779193e-05,
            "epoch": 1.7779544250132484,
            "step": 26840
        },
        {
            "loss": 0.4801,
            "grad_norm": 46.38615798950195,
            "learning_rate": 2.0345345345345344e-05,
            "epoch": 1.7792792792792793,
            "step": 26860
        },
        {
            "loss": 0.4433,
            "grad_norm": 1.0146801471710205,
            "learning_rate": 2.0323264440911502e-05,
            "epoch": 1.78060413354531,
            "step": 26880
        },
        {
            "loss": 0.423,
            "grad_norm": 1.876042366027832,
            "learning_rate": 2.0301183536477656e-05,
            "epoch": 1.781928987811341,
            "step": 26900
        },
        {
            "loss": 0.3988,
            "grad_norm": 0.9867933392524719,
            "learning_rate": 2.0279102632043807e-05,
            "epoch": 1.7832538420773716,
            "step": 26920
        },
        {
            "loss": 0.3573,
            "grad_norm": 0.958175539970398,
            "learning_rate": 2.0257021727609965e-05,
            "epoch": 1.7845786963434023,
            "step": 26940
        },
        {
            "loss": 0.4555,
            "grad_norm": 0.7877267003059387,
            "learning_rate": 2.023494082317612e-05,
            "epoch": 1.785903550609433,
            "step": 26960
        },
        {
            "loss": 0.376,
            "grad_norm": 1.066149353981018,
            "learning_rate": 2.021285991874227e-05,
            "epoch": 1.7872284048754636,
            "step": 26980
        },
        {
            "loss": 0.3828,
            "grad_norm": 4.714112281799316,
            "learning_rate": 2.019077901430843e-05,
            "epoch": 1.7885532591414943,
            "step": 27000
        },
        {
            "loss": 0.3404,
            "grad_norm": 1.8769252300262451,
            "learning_rate": 2.0168698109874583e-05,
            "epoch": 1.789878113407525,
            "step": 27020
        },
        {
            "loss": 0.4125,
            "grad_norm": 1.1047126054763794,
            "learning_rate": 2.0146617205440734e-05,
            "epoch": 1.791202967673556,
            "step": 27040
        },
        {
            "loss": 0.3511,
            "grad_norm": 1.887678861618042,
            "learning_rate": 2.0124536301006892e-05,
            "epoch": 1.7925278219395866,
            "step": 27060
        },
        {
            "loss": 0.3608,
            "grad_norm": 1.975265622138977,
            "learning_rate": 2.0102455396573043e-05,
            "epoch": 1.7938526762056175,
            "step": 27080
        },
        {
            "loss": 0.3724,
            "grad_norm": 1.9874838590621948,
            "learning_rate": 2.0080374492139197e-05,
            "epoch": 1.7951775304716482,
            "step": 27100
        },
        {
            "loss": 0.4023,
            "grad_norm": 4.323646545410156,
            "learning_rate": 2.0058293587705355e-05,
            "epoch": 1.7965023847376789,
            "step": 27120
        },
        {
            "loss": 0.3543,
            "grad_norm": 1.0116798877716064,
            "learning_rate": 2.0036212683271506e-05,
            "epoch": 1.7978272390037096,
            "step": 27140
        },
        {
            "loss": 0.3664,
            "grad_norm": 1.3079285621643066,
            "learning_rate": 2.001413177883766e-05,
            "epoch": 1.7991520932697402,
            "step": 27160
        },
        {
            "loss": 0.3866,
            "grad_norm": 1.478579044342041,
            "learning_rate": 1.999205087440382e-05,
            "epoch": 1.800476947535771,
            "step": 27180
        },
        {
            "loss": 0.3453,
            "grad_norm": 3.0904178619384766,
            "learning_rate": 1.996996996996997e-05,
            "epoch": 1.8018018018018018,
            "step": 27200
        },
        {
            "loss": 0.4002,
            "grad_norm": 0.911910355091095,
            "learning_rate": 1.9947889065536127e-05,
            "epoch": 1.8031266560678325,
            "step": 27220
        },
        {
            "loss": 0.4057,
            "grad_norm": 1.0426007509231567,
            "learning_rate": 1.992580816110228e-05,
            "epoch": 1.8044515103338634,
            "step": 27240
        },
        {
            "loss": 0.4851,
            "grad_norm": 1.2161448001861572,
            "learning_rate": 1.9903727256668433e-05,
            "epoch": 1.805776364599894,
            "step": 27260
        },
        {
            "loss": 0.4415,
            "grad_norm": 1.2179473638534546,
            "learning_rate": 1.988164635223459e-05,
            "epoch": 1.8071012188659248,
            "step": 27280
        },
        {
            "loss": 0.3241,
            "grad_norm": 2.0248868465423584,
            "learning_rate": 1.9859565447800742e-05,
            "epoch": 1.8084260731319555,
            "step": 27300
        },
        {
            "loss": 0.4604,
            "grad_norm": 3.570971965789795,
            "learning_rate": 1.9837484543366896e-05,
            "epoch": 1.8097509273979862,
            "step": 27320
        },
        {
            "loss": 0.3735,
            "grad_norm": 1.377310872077942,
            "learning_rate": 1.9815403638933054e-05,
            "epoch": 1.8110757816640168,
            "step": 27340
        },
        {
            "loss": 0.4,
            "grad_norm": 2.041205644607544,
            "learning_rate": 1.9793322734499205e-05,
            "epoch": 1.8124006359300477,
            "step": 27360
        },
        {
            "loss": 0.3827,
            "grad_norm": 3.122244119644165,
            "learning_rate": 1.977124183006536e-05,
            "epoch": 1.8137254901960784,
            "step": 27380
        },
        {
            "loss": 0.4186,
            "grad_norm": 0.9121114015579224,
            "learning_rate": 1.9749160925631517e-05,
            "epoch": 1.8150503444621093,
            "step": 27400
        },
        {
            "loss": 0.3882,
            "grad_norm": 1.0443493127822876,
            "learning_rate": 1.972708002119767e-05,
            "epoch": 1.81637519872814,
            "step": 27420
        },
        {
            "loss": 0.3808,
            "grad_norm": 4.537777423858643,
            "learning_rate": 1.9704999116763823e-05,
            "epoch": 1.8177000529941707,
            "step": 27440
        },
        {
            "loss": 0.3876,
            "grad_norm": 5.025732040405273,
            "learning_rate": 1.9682918212329977e-05,
            "epoch": 1.8190249072602014,
            "step": 27460
        },
        {
            "loss": 0.3927,
            "grad_norm": 1.3812260627746582,
            "learning_rate": 1.9660837307896132e-05,
            "epoch": 1.820349761526232,
            "step": 27480
        },
        {
            "loss": 0.3876,
            "grad_norm": 2.0840587615966797,
            "learning_rate": 1.9638756403462286e-05,
            "epoch": 1.8216746157922628,
            "step": 27500
        },
        {
            "loss": 0.4147,
            "grad_norm": 1.1562762260437012,
            "learning_rate": 1.961667549902844e-05,
            "epoch": 1.8229994700582934,
            "step": 27520
        },
        {
            "loss": 0.4123,
            "grad_norm": 0.7520290613174438,
            "learning_rate": 1.9594594594594595e-05,
            "epoch": 1.8243243243243243,
            "step": 27540
        },
        {
            "loss": 0.3714,
            "grad_norm": 7.952520847320557,
            "learning_rate": 1.957251369016075e-05,
            "epoch": 1.825649178590355,
            "step": 27560
        },
        {
            "loss": 0.3595,
            "grad_norm": 1.9079115390777588,
            "learning_rate": 1.9550432785726904e-05,
            "epoch": 1.826974032856386,
            "step": 27580
        },
        {
            "loss": 0.408,
            "grad_norm": 1.4200172424316406,
            "learning_rate": 1.952835188129306e-05,
            "epoch": 1.8282988871224166,
            "step": 27600
        },
        {
            "loss": 0.449,
            "grad_norm": 2.326388120651245,
            "learning_rate": 1.9506270976859213e-05,
            "epoch": 1.8296237413884473,
            "step": 27620
        },
        {
            "loss": 0.3853,
            "grad_norm": 1.2318787574768066,
            "learning_rate": 1.9484190072425367e-05,
            "epoch": 1.830948595654478,
            "step": 27640
        },
        {
            "loss": 0.338,
            "grad_norm": 1.2334082126617432,
            "learning_rate": 1.9462109167991522e-05,
            "epoch": 1.8322734499205087,
            "step": 27660
        },
        {
            "loss": 0.4392,
            "grad_norm": 79.79118347167969,
            "learning_rate": 1.9440028263557676e-05,
            "epoch": 1.8335983041865394,
            "step": 27680
        },
        {
            "loss": 0.3947,
            "grad_norm": 5.176578521728516,
            "learning_rate": 1.941794735912383e-05,
            "epoch": 1.8349231584525703,
            "step": 27700
        },
        {
            "loss": 0.3731,
            "grad_norm": 0.6617486476898193,
            "learning_rate": 1.9395866454689985e-05,
            "epoch": 1.836248012718601,
            "step": 27720
        },
        {
            "loss": 0.3937,
            "grad_norm": 2.3564064502716064,
            "learning_rate": 1.937378555025614e-05,
            "epoch": 1.8375728669846318,
            "step": 27740
        },
        {
            "loss": 0.3818,
            "grad_norm": 0.9448261857032776,
            "learning_rate": 1.9351704645822294e-05,
            "epoch": 1.8388977212506625,
            "step": 27760
        },
        {
            "loss": 0.3528,
            "grad_norm": 1.34800386428833,
            "learning_rate": 1.932962374138845e-05,
            "epoch": 1.8402225755166932,
            "step": 27780
        },
        {
            "loss": 0.3577,
            "grad_norm": 3.1407670974731445,
            "learning_rate": 1.9307542836954603e-05,
            "epoch": 1.841547429782724,
            "step": 27800
        },
        {
            "loss": 0.3817,
            "grad_norm": 1.6708779335021973,
            "learning_rate": 1.9285461932520757e-05,
            "epoch": 1.8428722840487546,
            "step": 27820
        },
        {
            "loss": 0.4377,
            "grad_norm": 0.912267804145813,
            "learning_rate": 1.9263381028086912e-05,
            "epoch": 1.8441971383147853,
            "step": 27840
        },
        {
            "loss": 0.3236,
            "grad_norm": 1.4146970510482788,
            "learning_rate": 1.9241300123653066e-05,
            "epoch": 1.845521992580816,
            "step": 27860
        },
        {
            "loss": 0.4329,
            "grad_norm": 1.4637809991836548,
            "learning_rate": 1.921921921921922e-05,
            "epoch": 1.8468468468468469,
            "step": 27880
        },
        {
            "loss": 0.3746,
            "grad_norm": 1.1301043033599854,
            "learning_rate": 1.9197138314785372e-05,
            "epoch": 1.8481717011128775,
            "step": 27900
        },
        {
            "loss": 0.3791,
            "grad_norm": 1.6308386325836182,
            "learning_rate": 1.917505741035153e-05,
            "epoch": 1.8494965553789084,
            "step": 27920
        },
        {
            "loss": 0.4168,
            "grad_norm": 1.0675324201583862,
            "learning_rate": 1.9152976505917684e-05,
            "epoch": 1.8508214096449391,
            "step": 27940
        },
        {
            "loss": 0.4084,
            "grad_norm": 1.004461407661438,
            "learning_rate": 1.9130895601483835e-05,
            "epoch": 1.8521462639109698,
            "step": 27960
        },
        {
            "loss": 0.377,
            "grad_norm": 0.9889718294143677,
            "learning_rate": 1.9108814697049993e-05,
            "epoch": 1.8534711181770005,
            "step": 27980
        },
        {
            "loss": 0.4707,
            "grad_norm": 1.6870107650756836,
            "learning_rate": 1.9086733792616147e-05,
            "epoch": 1.8547959724430312,
            "step": 28000
        },
        {
            "loss": 0.3866,
            "grad_norm": 0.8614294528961182,
            "learning_rate": 1.9064652888182302e-05,
            "epoch": 1.8561208267090619,
            "step": 28020
        },
        {
            "loss": 0.4013,
            "grad_norm": 1.5396581888198853,
            "learning_rate": 1.9042571983748456e-05,
            "epoch": 1.8574456809750928,
            "step": 28040
        },
        {
            "loss": 0.3582,
            "grad_norm": 0.7199345827102661,
            "learning_rate": 1.902049107931461e-05,
            "epoch": 1.8587705352411235,
            "step": 28060
        },
        {
            "loss": 0.3889,
            "grad_norm": 0.47916853427886963,
            "learning_rate": 1.8998410174880765e-05,
            "epoch": 1.8600953895071544,
            "step": 28080
        },
        {
            "loss": 0.3723,
            "grad_norm": 2.0244665145874023,
            "learning_rate": 1.897632927044692e-05,
            "epoch": 1.861420243773185,
            "step": 28100
        },
        {
            "loss": 0.383,
            "grad_norm": 0.7716596722602844,
            "learning_rate": 1.895424836601307e-05,
            "epoch": 1.8627450980392157,
            "step": 28120
        },
        {
            "loss": 0.3914,
            "grad_norm": 0.7793866395950317,
            "learning_rate": 1.893216746157923e-05,
            "epoch": 1.8640699523052464,
            "step": 28140
        },
        {
            "loss": 0.3325,
            "grad_norm": 10.338347434997559,
            "learning_rate": 1.8910086557145383e-05,
            "epoch": 1.865394806571277,
            "step": 28160
        },
        {
            "loss": 0.3939,
            "grad_norm": 0.854997992515564,
            "learning_rate": 1.8888005652711534e-05,
            "epoch": 1.8667196608373078,
            "step": 28180
        },
        {
            "loss": 0.39,
            "grad_norm": 1.7754793167114258,
            "learning_rate": 1.8865924748277692e-05,
            "epoch": 1.8680445151033387,
            "step": 28200
        },
        {
            "loss": 0.326,
            "grad_norm": 0.8726991415023804,
            "learning_rate": 1.8843843843843846e-05,
            "epoch": 1.8693693693693694,
            "step": 28220
        },
        {
            "loss": 0.3349,
            "grad_norm": 1.1119085550308228,
            "learning_rate": 1.8821762939409997e-05,
            "epoch": 1.8706942236354,
            "step": 28240
        },
        {
            "loss": 0.4312,
            "grad_norm": 2.747084617614746,
            "learning_rate": 1.8799682034976155e-05,
            "epoch": 1.872019077901431,
            "step": 28260
        },
        {
            "loss": 0.4074,
            "grad_norm": 1.8252193927764893,
            "learning_rate": 1.8777601130542306e-05,
            "epoch": 1.8733439321674616,
            "step": 28280
        },
        {
            "loss": 0.3947,
            "grad_norm": 2.9345669746398926,
            "learning_rate": 1.875552022610846e-05,
            "epoch": 1.8746687864334923,
            "step": 28300
        },
        {
            "loss": 0.4013,
            "grad_norm": 1.3851847648620605,
            "learning_rate": 1.873343932167462e-05,
            "epoch": 1.875993640699523,
            "step": 28320
        },
        {
            "loss": 0.3557,
            "grad_norm": 2.865084409713745,
            "learning_rate": 1.871135841724077e-05,
            "epoch": 1.8773184949655537,
            "step": 28340
        },
        {
            "loss": 0.3899,
            "grad_norm": 0.8068868517875671,
            "learning_rate": 1.8689277512806924e-05,
            "epoch": 1.8786433492315844,
            "step": 28360
        },
        {
            "loss": 0.3918,
            "grad_norm": 2.7429771423339844,
            "learning_rate": 1.8667196608373082e-05,
            "epoch": 1.8799682034976153,
            "step": 28380
        },
        {
            "loss": 0.3888,
            "grad_norm": 2.349013090133667,
            "learning_rate": 1.8645115703939233e-05,
            "epoch": 1.881293057763646,
            "step": 28400
        },
        {
            "loss": 0.4215,
            "grad_norm": 2.668184280395508,
            "learning_rate": 1.862303479950539e-05,
            "epoch": 1.8826179120296769,
            "step": 28420
        },
        {
            "loss": 0.4129,
            "grad_norm": 2.166485071182251,
            "learning_rate": 1.8600953895071545e-05,
            "epoch": 1.8839427662957076,
            "step": 28440
        },
        {
            "loss": 0.423,
            "grad_norm": 2.3171815872192383,
            "learning_rate": 1.8578872990637696e-05,
            "epoch": 1.8852676205617382,
            "step": 28460
        },
        {
            "loss": 0.3873,
            "grad_norm": 0.8010053634643555,
            "learning_rate": 1.8556792086203854e-05,
            "epoch": 1.886592474827769,
            "step": 28480
        },
        {
            "loss": 0.389,
            "grad_norm": 3.077209711074829,
            "learning_rate": 1.8534711181770005e-05,
            "epoch": 1.8879173290937996,
            "step": 28500
        },
        {
            "loss": 0.4431,
            "grad_norm": 1.9201009273529053,
            "learning_rate": 1.851263027733616e-05,
            "epoch": 1.8892421833598303,
            "step": 28520
        },
        {
            "loss": 0.3984,
            "grad_norm": 0.9293473958969116,
            "learning_rate": 1.8490549372902317e-05,
            "epoch": 1.8905670376258612,
            "step": 28540
        },
        {
            "loss": 0.3311,
            "grad_norm": 0.6751832962036133,
            "learning_rate": 1.846846846846847e-05,
            "epoch": 1.8918918918918919,
            "step": 28560
        },
        {
            "loss": 0.4296,
            "grad_norm": 3.906451463699341,
            "learning_rate": 1.8446387564034623e-05,
            "epoch": 1.8932167461579228,
            "step": 28580
        },
        {
            "loss": 0.3958,
            "grad_norm": 1.870389461517334,
            "learning_rate": 1.842430665960078e-05,
            "epoch": 1.8945416004239535,
            "step": 28600
        },
        {
            "loss": 0.3562,
            "grad_norm": 0.7937237620353699,
            "learning_rate": 1.840222575516693e-05,
            "epoch": 1.8958664546899842,
            "step": 28620
        },
        {
            "loss": 0.375,
            "grad_norm": 0.8586218357086182,
            "learning_rate": 1.8380144850733086e-05,
            "epoch": 1.8971913089560148,
            "step": 28640
        },
        {
            "loss": 0.4081,
            "grad_norm": 1.2786669731140137,
            "learning_rate": 1.835806394629924e-05,
            "epoch": 1.8985161632220455,
            "step": 28660
        },
        {
            "loss": 0.3796,
            "grad_norm": 1.2720794677734375,
            "learning_rate": 1.8335983041865395e-05,
            "epoch": 1.8998410174880762,
            "step": 28680
        },
        {
            "loss": 0.3962,
            "grad_norm": 1.8628332614898682,
            "learning_rate": 1.831390213743155e-05,
            "epoch": 1.901165871754107,
            "step": 28700
        },
        {
            "loss": 0.3995,
            "grad_norm": 0.7014040350914001,
            "learning_rate": 1.8291821232997704e-05,
            "epoch": 1.9024907260201378,
            "step": 28720
        },
        {
            "loss": 0.4303,
            "grad_norm": 0.7645531892776489,
            "learning_rate": 1.826974032856386e-05,
            "epoch": 1.9038155802861685,
            "step": 28740
        },
        {
            "loss": 0.3474,
            "grad_norm": 6.792300224304199,
            "learning_rate": 1.8247659424130013e-05,
            "epoch": 1.9051404345521994,
            "step": 28760
        },
        {
            "loss": 0.4707,
            "grad_norm": 0.7893709540367126,
            "learning_rate": 1.8225578519696167e-05,
            "epoch": 1.90646528881823,
            "step": 28780
        },
        {
            "loss": 0.3964,
            "grad_norm": 1.2459040880203247,
            "learning_rate": 1.820349761526232e-05,
            "epoch": 1.9077901430842608,
            "step": 28800
        },
        {
            "loss": 0.3906,
            "grad_norm": 1.6937259435653687,
            "learning_rate": 1.8181416710828476e-05,
            "epoch": 1.9091149973502914,
            "step": 28820
        },
        {
            "loss": 0.4003,
            "grad_norm": 29.585866928100586,
            "learning_rate": 1.815933580639463e-05,
            "epoch": 1.9104398516163221,
            "step": 28840
        },
        {
            "loss": 0.4509,
            "grad_norm": 0.8960446715354919,
            "learning_rate": 1.8137254901960785e-05,
            "epoch": 1.9117647058823528,
            "step": 28860
        },
        {
            "loss": 0.4228,
            "grad_norm": 1.1478666067123413,
            "learning_rate": 1.811517399752694e-05,
            "epoch": 1.9130895601483837,
            "step": 28880
        },
        {
            "loss": 0.3575,
            "grad_norm": 1.984663963317871,
            "learning_rate": 1.8093093093093094e-05,
            "epoch": 1.9144144144144144,
            "step": 28900
        },
        {
            "loss": 0.4001,
            "grad_norm": 1.0567095279693604,
            "learning_rate": 1.807101218865925e-05,
            "epoch": 1.9157392686804453,
            "step": 28920
        },
        {
            "loss": 0.4136,
            "grad_norm": 3.451812982559204,
            "learning_rate": 1.8048931284225403e-05,
            "epoch": 1.917064122946476,
            "step": 28940
        },
        {
            "loss": 0.3725,
            "grad_norm": 0.751393735408783,
            "learning_rate": 1.8026850379791557e-05,
            "epoch": 1.9183889772125067,
            "step": 28960
        },
        {
            "loss": 0.4047,
            "grad_norm": 1.0332261323928833,
            "learning_rate": 1.800476947535771e-05,
            "epoch": 1.9197138314785374,
            "step": 28980
        },
        {
            "loss": 0.3731,
            "grad_norm": 1.7204431295394897,
            "learning_rate": 1.7982688570923866e-05,
            "epoch": 1.921038685744568,
            "step": 29000
        },
        {
            "loss": 0.4178,
            "grad_norm": 2.564274787902832,
            "learning_rate": 1.796060766649002e-05,
            "epoch": 1.9223635400105987,
            "step": 29020
        },
        {
            "loss": 0.3465,
            "grad_norm": 0.6005190014839172,
            "learning_rate": 1.7938526762056175e-05,
            "epoch": 1.9236883942766294,
            "step": 29040
        },
        {
            "loss": 0.4188,
            "grad_norm": 1.87642240524292,
            "learning_rate": 1.791644585762233e-05,
            "epoch": 1.9250132485426603,
            "step": 29060
        },
        {
            "loss": 0.3975,
            "grad_norm": 3.6804988384246826,
            "learning_rate": 1.7894364953188484e-05,
            "epoch": 1.926338102808691,
            "step": 29080
        },
        {
            "loss": 0.3801,
            "grad_norm": 2.258676528930664,
            "learning_rate": 1.7872284048754638e-05,
            "epoch": 1.927662957074722,
            "step": 29100
        },
        {
            "loss": 0.3421,
            "grad_norm": 1.6283220052719116,
            "learning_rate": 1.7850203144320793e-05,
            "epoch": 1.9289878113407526,
            "step": 29120
        },
        {
            "loss": 0.4321,
            "grad_norm": 1.2399474382400513,
            "learning_rate": 1.7828122239886947e-05,
            "epoch": 1.9303126656067833,
            "step": 29140
        },
        {
            "loss": 0.4029,
            "grad_norm": 3.2585387229919434,
            "learning_rate": 1.7806041335453098e-05,
            "epoch": 1.931637519872814,
            "step": 29160
        },
        {
            "loss": 0.3906,
            "grad_norm": 2.747946262359619,
            "learning_rate": 1.7783960431019256e-05,
            "epoch": 1.9329623741388446,
            "step": 29180
        },
        {
            "loss": 0.3974,
            "grad_norm": 1.0065592527389526,
            "learning_rate": 1.776187952658541e-05,
            "epoch": 1.9342872284048753,
            "step": 29200
        },
        {
            "loss": 0.4014,
            "grad_norm": 3.8960635662078857,
            "learning_rate": 1.7739798622151565e-05,
            "epoch": 1.9356120826709062,
            "step": 29220
        },
        {
            "loss": 0.3945,
            "grad_norm": 1.0834068059921265,
            "learning_rate": 1.771771771771772e-05,
            "epoch": 1.936936936936937,
            "step": 29240
        },
        {
            "loss": 0.3584,
            "grad_norm": 0.37623319029808044,
            "learning_rate": 1.7695636813283874e-05,
            "epoch": 1.9382617912029678,
            "step": 29260
        },
        {
            "loss": 0.4228,
            "grad_norm": 1.9055290222167969,
            "learning_rate": 1.7673555908850028e-05,
            "epoch": 1.9395866454689985,
            "step": 29280
        },
        {
            "loss": 0.401,
            "grad_norm": 5.5105085372924805,
            "learning_rate": 1.7651475004416183e-05,
            "epoch": 1.9409114997350292,
            "step": 29300
        },
        {
            "loss": 0.4289,
            "grad_norm": 0.76108318567276,
            "learning_rate": 1.7629394099982334e-05,
            "epoch": 1.9422363540010599,
            "step": 29320
        },
        {
            "loss": 0.3561,
            "grad_norm": 1.275984525680542,
            "learning_rate": 1.760731319554849e-05,
            "epoch": 1.9435612082670906,
            "step": 29340
        },
        {
            "loss": 0.3983,
            "grad_norm": 1.0368741750717163,
            "learning_rate": 1.7585232291114646e-05,
            "epoch": 1.9448860625331212,
            "step": 29360
        },
        {
            "loss": 0.3696,
            "grad_norm": 1.3813447952270508,
            "learning_rate": 1.7563151386680797e-05,
            "epoch": 1.9462109167991521,
            "step": 29380
        },
        {
            "loss": 0.3758,
            "grad_norm": 0.9702858328819275,
            "learning_rate": 1.7541070482246955e-05,
            "epoch": 1.9475357710651828,
            "step": 29400
        },
        {
            "loss": 0.4046,
            "grad_norm": 1.939556360244751,
            "learning_rate": 1.751898957781311e-05,
            "epoch": 1.9488606253312135,
            "step": 29420
        },
        {
            "loss": 0.3374,
            "grad_norm": 1.1329807043075562,
            "learning_rate": 1.749690867337926e-05,
            "epoch": 1.9501854795972444,
            "step": 29440
        },
        {
            "loss": 0.4541,
            "grad_norm": 0.8665097951889038,
            "learning_rate": 1.7474827768945418e-05,
            "epoch": 1.951510333863275,
            "step": 29460
        },
        {
            "loss": 0.3787,
            "grad_norm": 34.9691162109375,
            "learning_rate": 1.7452746864511573e-05,
            "epoch": 1.9528351881293058,
            "step": 29480
        },
        {
            "loss": 0.3496,
            "grad_norm": 1.030255913734436,
            "learning_rate": 1.7430665960077724e-05,
            "epoch": 1.9541600423953365,
            "step": 29500
        },
        {
            "loss": 0.3421,
            "grad_norm": 1.108047366142273,
            "learning_rate": 1.740858505564388e-05,
            "epoch": 1.9554848966613672,
            "step": 29520
        },
        {
            "loss": 0.4272,
            "grad_norm": 2.2786760330200195,
            "learning_rate": 1.7386504151210033e-05,
            "epoch": 1.9568097509273978,
            "step": 29540
        },
        {
            "loss": 0.4339,
            "grad_norm": 2.041518449783325,
            "learning_rate": 1.7364423246776187e-05,
            "epoch": 1.9581346051934287,
            "step": 29560
        },
        {
            "loss": 0.3561,
            "grad_norm": 1.9974586963653564,
            "learning_rate": 1.7342342342342345e-05,
            "epoch": 1.9594594594594594,
            "step": 29580
        },
        {
            "loss": 0.3752,
            "grad_norm": 1.204020619392395,
            "learning_rate": 1.7320261437908496e-05,
            "epoch": 1.9607843137254903,
            "step": 29600
        },
        {
            "loss": 0.3921,
            "grad_norm": 0.6619825959205627,
            "learning_rate": 1.7298180533474654e-05,
            "epoch": 1.962109167991521,
            "step": 29620
        },
        {
            "loss": 0.3447,
            "grad_norm": 1.9881964921951294,
            "learning_rate": 1.7276099629040808e-05,
            "epoch": 1.9634340222575517,
            "step": 29640
        },
        {
            "loss": 0.345,
            "grad_norm": 0.9637891054153442,
            "learning_rate": 1.725401872460696e-05,
            "epoch": 1.9647588765235824,
            "step": 29660
        },
        {
            "loss": 0.3271,
            "grad_norm": 1.9102909564971924,
            "learning_rate": 1.7231937820173117e-05,
            "epoch": 1.966083730789613,
            "step": 29680
        },
        {
            "loss": 0.3915,
            "grad_norm": 2.624112844467163,
            "learning_rate": 1.7209856915739268e-05,
            "epoch": 1.9674085850556438,
            "step": 29700
        },
        {
            "loss": 0.3381,
            "grad_norm": 1.3109837770462036,
            "learning_rate": 1.7187776011305423e-05,
            "epoch": 1.9687334393216747,
            "step": 29720
        },
        {
            "loss": 0.3304,
            "grad_norm": 0.7662848234176636,
            "learning_rate": 1.716569510687158e-05,
            "epoch": 1.9700582935877053,
            "step": 29740
        },
        {
            "loss": 0.3992,
            "grad_norm": 1.9770421981811523,
            "learning_rate": 1.714361420243773e-05,
            "epoch": 1.9713831478537363,
            "step": 29760
        },
        {
            "loss": 0.353,
            "grad_norm": 0.6310861110687256,
            "learning_rate": 1.7121533298003886e-05,
            "epoch": 1.972708002119767,
            "step": 29780
        },
        {
            "loss": 0.3962,
            "grad_norm": 0.8625069856643677,
            "learning_rate": 1.7099452393570044e-05,
            "epoch": 1.9740328563857976,
            "step": 29800
        },
        {
            "loss": 0.3777,
            "grad_norm": 0.6802531480789185,
            "learning_rate": 1.7077371489136195e-05,
            "epoch": 1.9753577106518283,
            "step": 29820
        },
        {
            "loss": 0.3722,
            "grad_norm": 3.155714750289917,
            "learning_rate": 1.705529058470235e-05,
            "epoch": 1.976682564917859,
            "step": 29840
        },
        {
            "loss": 0.3835,
            "grad_norm": 0.6390455365180969,
            "learning_rate": 1.7033209680268504e-05,
            "epoch": 1.9780074191838897,
            "step": 29860
        },
        {
            "loss": 0.3623,
            "grad_norm": 3.803445339202881,
            "learning_rate": 1.7011128775834658e-05,
            "epoch": 1.9793322734499204,
            "step": 29880
        },
        {
            "loss": 0.4057,
            "grad_norm": 35.91698455810547,
            "learning_rate": 1.6989047871400813e-05,
            "epoch": 1.9806571277159513,
            "step": 29900
        },
        {
            "loss": 0.3929,
            "grad_norm": 3.262673854827881,
            "learning_rate": 1.6966966966966967e-05,
            "epoch": 1.981981981981982,
            "step": 29920
        },
        {
            "loss": 0.389,
            "grad_norm": 3.1217620372772217,
            "learning_rate": 1.694488606253312e-05,
            "epoch": 1.9833068362480128,
            "step": 29940
        },
        {
            "loss": 0.3772,
            "grad_norm": 3.5848207473754883,
            "learning_rate": 1.6922805158099276e-05,
            "epoch": 1.9846316905140435,
            "step": 29960
        },
        {
            "loss": 0.3953,
            "grad_norm": 2.302523374557495,
            "learning_rate": 1.690072425366543e-05,
            "epoch": 1.9859565447800742,
            "step": 29980
        },
        {
            "loss": 0.3393,
            "grad_norm": 5.18171501159668,
            "learning_rate": 1.6878643349231585e-05,
            "epoch": 1.987281399046105,
            "step": 30000
        },
        {
            "loss": 0.352,
            "grad_norm": 1.0311402082443237,
            "learning_rate": 1.6856562444797743e-05,
            "epoch": 1.9886062533121356,
            "step": 30020
        },
        {
            "loss": 0.3706,
            "grad_norm": 0.7488168478012085,
            "learning_rate": 1.6834481540363894e-05,
            "epoch": 1.9899311075781663,
            "step": 30040
        },
        {
            "loss": 0.3444,
            "grad_norm": 1.3326267004013062,
            "learning_rate": 1.6812400635930048e-05,
            "epoch": 1.9912559618441972,
            "step": 30060
        },
        {
            "loss": 0.451,
            "grad_norm": 2.7404565811157227,
            "learning_rate": 1.6790319731496203e-05,
            "epoch": 1.9925808161102279,
            "step": 30080
        },
        {
            "loss": 0.369,
            "grad_norm": 1.00349760055542,
            "learning_rate": 1.6768238827062357e-05,
            "epoch": 1.9939056703762588,
            "step": 30100
        },
        {
            "loss": 0.4685,
            "grad_norm": 2.5914878845214844,
            "learning_rate": 1.674615792262851e-05,
            "epoch": 1.9952305246422894,
            "step": 30120
        },
        {
            "loss": 0.3989,
            "grad_norm": 2.9530341625213623,
            "learning_rate": 1.6724077018194666e-05,
            "epoch": 1.9965553789083201,
            "step": 30140
        },
        {
            "loss": 0.3639,
            "grad_norm": 1.347661018371582,
            "learning_rate": 1.670199611376082e-05,
            "epoch": 1.9978802331743508,
            "step": 30160
        },
        {
            "loss": 0.3757,
            "grad_norm": 3.3653945922851562,
            "learning_rate": 1.6679915209326975e-05,
            "epoch": 1.9992050874403815,
            "step": 30180
        },
        {
            "eval_loss": 0.3895553946495056,
            "eval_runtime": 111.8851,
            "eval_samples_per_second": 269.848,
            "eval_steps_per_second": 33.731,
            "epoch": 2.0,
            "step": 30192
        },
        {
            "loss": 0.389,
            "grad_norm": 3.8386664390563965,
            "learning_rate": 1.665783430489313e-05,
            "epoch": 2.000529941706412,
            "step": 30200
        },
        {
            "loss": 0.4033,
            "grad_norm": 1.0855909585952759,
            "learning_rate": 1.6635753400459284e-05,
            "epoch": 2.001854795972443,
            "step": 30220
        },
        {
            "loss": 0.3788,
            "grad_norm": 0.5159087777137756,
            "learning_rate": 1.6613672496025438e-05,
            "epoch": 2.0031796502384736,
            "step": 30240
        },
        {
            "loss": 0.3838,
            "grad_norm": 0.8490985035896301,
            "learning_rate": 1.6591591591591593e-05,
            "epoch": 2.0045045045045047,
            "step": 30260
        },
        {
            "loss": 0.3762,
            "grad_norm": 0.8630375862121582,
            "learning_rate": 1.6569510687157747e-05,
            "epoch": 2.0058293587705354,
            "step": 30280
        },
        {
            "loss": 0.3844,
            "grad_norm": 1.3643114566802979,
            "learning_rate": 1.65474297827239e-05,
            "epoch": 2.007154213036566,
            "step": 30300
        },
        {
            "loss": 0.3526,
            "grad_norm": 1.3298673629760742,
            "learning_rate": 1.6525348878290056e-05,
            "epoch": 2.0084790673025967,
            "step": 30320
        },
        {
            "loss": 0.3902,
            "grad_norm": 0.5773290395736694,
            "learning_rate": 1.650326797385621e-05,
            "epoch": 2.0098039215686274,
            "step": 30340
        },
        {
            "loss": 0.3825,
            "grad_norm": 5.01725959777832,
            "learning_rate": 1.648118706942236e-05,
            "epoch": 2.011128775834658,
            "step": 30360
        },
        {
            "loss": 0.3432,
            "grad_norm": 43.41165542602539,
            "learning_rate": 1.645910616498852e-05,
            "epoch": 2.012453630100689,
            "step": 30380
        },
        {
            "loss": 0.3699,
            "grad_norm": 1.022039532661438,
            "learning_rate": 1.6437025260554674e-05,
            "epoch": 2.0137784843667195,
            "step": 30400
        },
        {
            "loss": 0.4032,
            "grad_norm": 1.4453074932098389,
            "learning_rate": 1.6414944356120828e-05,
            "epoch": 2.0151033386327506,
            "step": 30420
        },
        {
            "loss": 0.3684,
            "grad_norm": 1.1041293144226074,
            "learning_rate": 1.6392863451686983e-05,
            "epoch": 2.0164281928987813,
            "step": 30440
        },
        {
            "loss": 0.3971,
            "grad_norm": 0.6507434248924255,
            "learning_rate": 1.6370782547253137e-05,
            "epoch": 2.017753047164812,
            "step": 30460
        },
        {
            "loss": 0.3644,
            "grad_norm": 1.0355126857757568,
            "learning_rate": 1.634870164281929e-05,
            "epoch": 2.0190779014308426,
            "step": 30480
        },
        {
            "loss": 0.3757,
            "grad_norm": 0.5576854944229126,
            "learning_rate": 1.6326620738385446e-05,
            "epoch": 2.0204027556968733,
            "step": 30500
        },
        {
            "loss": 0.3734,
            "grad_norm": 0.6592018604278564,
            "learning_rate": 1.63045398339516e-05,
            "epoch": 2.021727609962904,
            "step": 30520
        },
        {
            "loss": 0.339,
            "grad_norm": 0.555088222026825,
            "learning_rate": 1.6282458929517755e-05,
            "epoch": 2.0230524642289347,
            "step": 30540
        },
        {
            "loss": 0.3773,
            "grad_norm": 0.8854609727859497,
            "learning_rate": 1.626037802508391e-05,
            "epoch": 2.0243773184949654,
            "step": 30560
        },
        {
            "loss": 0.395,
            "grad_norm": 3.5318572521209717,
            "learning_rate": 1.623829712065006e-05,
            "epoch": 2.0257021727609965,
            "step": 30580
        },
        {
            "loss": 0.3947,
            "grad_norm": 1.6690783500671387,
            "learning_rate": 1.6216216216216218e-05,
            "epoch": 2.027027027027027,
            "step": 30600
        },
        {
            "loss": 0.3781,
            "grad_norm": 1.9924068450927734,
            "learning_rate": 1.6194135311782373e-05,
            "epoch": 2.028351881293058,
            "step": 30620
        },
        {
            "loss": 0.3826,
            "grad_norm": 3.5378615856170654,
            "learning_rate": 1.6172054407348524e-05,
            "epoch": 2.0296767355590886,
            "step": 30640
        },
        {
            "loss": 0.4054,
            "grad_norm": 1.1594585180282593,
            "learning_rate": 1.614997350291468e-05,
            "epoch": 2.0310015898251192,
            "step": 30660
        },
        {
            "loss": 0.3941,
            "grad_norm": 1.7062828540802002,
            "learning_rate": 1.6127892598480836e-05,
            "epoch": 2.03232644409115,
            "step": 30680
        },
        {
            "loss": 0.4104,
            "grad_norm": 2.7367794513702393,
            "learning_rate": 1.6105811694046987e-05,
            "epoch": 2.0336512983571806,
            "step": 30700
        },
        {
            "loss": 0.3916,
            "grad_norm": 1.0419127941131592,
            "learning_rate": 1.6083730789613145e-05,
            "epoch": 2.0349761526232113,
            "step": 30720
        },
        {
            "loss": 0.3691,
            "grad_norm": 0.6738903522491455,
            "learning_rate": 1.6061649885179296e-05,
            "epoch": 2.036301006889242,
            "step": 30740
        },
        {
            "loss": 0.371,
            "grad_norm": 0.9375220537185669,
            "learning_rate": 1.603956898074545e-05,
            "epoch": 2.037625861155273,
            "step": 30760
        },
        {
            "loss": 0.3624,
            "grad_norm": 4.431347846984863,
            "learning_rate": 1.6017488076311608e-05,
            "epoch": 2.038950715421304,
            "step": 30780
        },
        {
            "loss": 0.3806,
            "grad_norm": 1.325984001159668,
            "learning_rate": 1.599540717187776e-05,
            "epoch": 2.0402755696873345,
            "step": 30800
        },
        {
            "loss": 0.3658,
            "grad_norm": 3.128751516342163,
            "learning_rate": 1.5973326267443917e-05,
            "epoch": 2.041600423953365,
            "step": 30820
        },
        {
            "loss": 0.4151,
            "grad_norm": 2.006983995437622,
            "learning_rate": 1.595124536301007e-05,
            "epoch": 2.042925278219396,
            "step": 30840
        },
        {
            "loss": 0.3425,
            "grad_norm": 1.5358538627624512,
            "learning_rate": 1.5929164458576222e-05,
            "epoch": 2.0442501324854265,
            "step": 30860
        },
        {
            "loss": 0.367,
            "grad_norm": 0.644825279712677,
            "learning_rate": 1.590708355414238e-05,
            "epoch": 2.045574986751457,
            "step": 30880
        },
        {
            "loss": 0.3717,
            "grad_norm": 2.1763525009155273,
            "learning_rate": 1.588500264970853e-05,
            "epoch": 2.046899841017488,
            "step": 30900
        },
        {
            "loss": 0.3762,
            "grad_norm": 2.774672746658325,
            "learning_rate": 1.5862921745274686e-05,
            "epoch": 2.048224695283519,
            "step": 30920
        },
        {
            "loss": 0.3781,
            "grad_norm": 2.249789237976074,
            "learning_rate": 1.5840840840840844e-05,
            "epoch": 2.0495495495495497,
            "step": 30940
        },
        {
            "loss": 0.3489,
            "grad_norm": 2.718874216079712,
            "learning_rate": 1.5818759936406995e-05,
            "epoch": 2.0508744038155804,
            "step": 30960
        },
        {
            "loss": 0.3611,
            "grad_norm": 1.8091881275177002,
            "learning_rate": 1.579667903197315e-05,
            "epoch": 2.052199258081611,
            "step": 30980
        },
        {
            "loss": 0.4348,
            "grad_norm": 1.1220970153808594,
            "learning_rate": 1.5774598127539307e-05,
            "epoch": 2.0535241123476418,
            "step": 31000
        },
        {
            "loss": 0.3547,
            "grad_norm": 1.45306396484375,
            "learning_rate": 1.5752517223105458e-05,
            "epoch": 2.0548489666136724,
            "step": 31020
        },
        {
            "loss": 0.3675,
            "grad_norm": 0.8837578892707825,
            "learning_rate": 1.5730436318671612e-05,
            "epoch": 2.056173820879703,
            "step": 31040
        },
        {
            "loss": 0.3669,
            "grad_norm": 1.7365041971206665,
            "learning_rate": 1.570835541423777e-05,
            "epoch": 2.057498675145734,
            "step": 31060
        },
        {
            "loss": 0.3653,
            "grad_norm": 1.0192046165466309,
            "learning_rate": 1.568627450980392e-05,
            "epoch": 2.0588235294117645,
            "step": 31080
        },
        {
            "loss": 0.4049,
            "grad_norm": 1.6595557928085327,
            "learning_rate": 1.5664193605370076e-05,
            "epoch": 2.0601483836777956,
            "step": 31100
        },
        {
            "loss": 0.3574,
            "grad_norm": 0.8755849599838257,
            "learning_rate": 1.564211270093623e-05,
            "epoch": 2.0614732379438263,
            "step": 31120
        },
        {
            "loss": 0.3967,
            "grad_norm": 0.5724426507949829,
            "learning_rate": 1.5620031796502385e-05,
            "epoch": 2.062798092209857,
            "step": 31140
        },
        {
            "loss": 0.4124,
            "grad_norm": 1.6835418939590454,
            "learning_rate": 1.559795089206854e-05,
            "epoch": 2.0641229464758877,
            "step": 31160
        },
        {
            "loss": 0.3673,
            "grad_norm": 0.7889415621757507,
            "learning_rate": 1.5575869987634694e-05,
            "epoch": 2.0654478007419184,
            "step": 31180
        },
        {
            "loss": 0.4054,
            "grad_norm": 1.4030396938323975,
            "learning_rate": 1.5553789083200848e-05,
            "epoch": 2.066772655007949,
            "step": 31200
        },
        {
            "loss": 0.41,
            "grad_norm": 1.7811318635940552,
            "learning_rate": 1.5531708178767006e-05,
            "epoch": 2.0680975092739797,
            "step": 31220
        },
        {
            "loss": 0.3657,
            "grad_norm": 3.0888078212738037,
            "learning_rate": 1.5509627274333157e-05,
            "epoch": 2.0694223635400104,
            "step": 31240
        },
        {
            "loss": 0.3296,
            "grad_norm": 1.49370539188385,
            "learning_rate": 1.548754636989931e-05,
            "epoch": 2.0707472178060415,
            "step": 31260
        },
        {
            "loss": 0.3791,
            "grad_norm": 3.2676053047180176,
            "learning_rate": 1.5465465465465466e-05,
            "epoch": 2.0720720720720722,
            "step": 31280
        },
        {
            "loss": 0.3728,
            "grad_norm": 2.001232624053955,
            "learning_rate": 1.544338456103162e-05,
            "epoch": 2.073396926338103,
            "step": 31300
        },
        {
            "loss": 0.3881,
            "grad_norm": 2.9514503479003906,
            "learning_rate": 1.5421303656597775e-05,
            "epoch": 2.0747217806041336,
            "step": 31320
        },
        {
            "loss": 0.3949,
            "grad_norm": 2.043630599975586,
            "learning_rate": 1.539922275216393e-05,
            "epoch": 2.0760466348701643,
            "step": 31340
        },
        {
            "loss": 0.3747,
            "grad_norm": 3.2125234603881836,
            "learning_rate": 1.5377141847730084e-05,
            "epoch": 2.077371489136195,
            "step": 31360
        },
        {
            "loss": 0.3738,
            "grad_norm": 2.140141725540161,
            "learning_rate": 1.5355060943296238e-05,
            "epoch": 2.0786963434022256,
            "step": 31380
        },
        {
            "loss": 0.3971,
            "grad_norm": 3.1142663955688477,
            "learning_rate": 1.5332980038862392e-05,
            "epoch": 2.0800211976682563,
            "step": 31400
        },
        {
            "loss": 0.3708,
            "grad_norm": 0.7172598838806152,
            "learning_rate": 1.5310899134428547e-05,
            "epoch": 2.081346051934287,
            "step": 31420
        },
        {
            "loss": 0.3834,
            "grad_norm": 1.2599120140075684,
            "learning_rate": 1.52888182299947e-05,
            "epoch": 2.082670906200318,
            "step": 31440
        },
        {
            "loss": 0.3643,
            "grad_norm": 1.698525071144104,
            "learning_rate": 1.5266737325560856e-05,
            "epoch": 2.083995760466349,
            "step": 31460
        },
        {
            "loss": 0.3732,
            "grad_norm": 1.1030646562576294,
            "learning_rate": 1.524465642112701e-05,
            "epoch": 2.0853206147323795,
            "step": 31480
        },
        {
            "loss": 0.4137,
            "grad_norm": 0.709351658821106,
            "learning_rate": 1.5222575516693163e-05,
            "epoch": 2.08664546899841,
            "step": 31500
        },
        {
            "loss": 0.3527,
            "grad_norm": 0.45242977142333984,
            "learning_rate": 1.5200494612259319e-05,
            "epoch": 2.087970323264441,
            "step": 31520
        },
        {
            "loss": 0.3857,
            "grad_norm": 13.085314750671387,
            "learning_rate": 1.5178413707825474e-05,
            "epoch": 2.0892951775304716,
            "step": 31540
        },
        {
            "loss": 0.3463,
            "grad_norm": 3.4435973167419434,
            "learning_rate": 1.5156332803391626e-05,
            "epoch": 2.0906200317965022,
            "step": 31560
        },
        {
            "loss": 0.3745,
            "grad_norm": 1.065740704536438,
            "learning_rate": 1.5134251898957782e-05,
            "epoch": 2.091944886062533,
            "step": 31580
        },
        {
            "loss": 0.3807,
            "grad_norm": 2.6340532302856445,
            "learning_rate": 1.5112170994523935e-05,
            "epoch": 2.093269740328564,
            "step": 31600
        },
        {
            "loss": 0.3606,
            "grad_norm": 1.2993754148483276,
            "learning_rate": 1.5090090090090091e-05,
            "epoch": 2.0945945945945947,
            "step": 31620
        },
        {
            "loss": 0.3083,
            "grad_norm": 0.6113020181655884,
            "learning_rate": 1.5068009185656246e-05,
            "epoch": 2.0959194488606254,
            "step": 31640
        },
        {
            "loss": 0.4006,
            "grad_norm": 0.8882072567939758,
            "learning_rate": 1.5045928281222398e-05,
            "epoch": 2.097244303126656,
            "step": 31660
        },
        {
            "loss": 0.3893,
            "grad_norm": 1.1755996942520142,
            "learning_rate": 1.5023847376788555e-05,
            "epoch": 2.098569157392687,
            "step": 31680
        },
        {
            "loss": 0.4172,
            "grad_norm": 3.813242197036743,
            "learning_rate": 1.5001766472354709e-05,
            "epoch": 2.0998940116587175,
            "step": 31700
        },
        {
            "loss": 0.4008,
            "grad_norm": 1.1958723068237305,
            "learning_rate": 1.4979685567920862e-05,
            "epoch": 2.101218865924748,
            "step": 31720
        },
        {
            "loss": 0.3395,
            "grad_norm": 0.9384604096412659,
            "learning_rate": 1.4957604663487018e-05,
            "epoch": 2.102543720190779,
            "step": 31740
        },
        {
            "loss": 0.3417,
            "grad_norm": 0.46862244606018066,
            "learning_rate": 1.4935523759053172e-05,
            "epoch": 2.1038685744568095,
            "step": 31760
        },
        {
            "loss": 0.3393,
            "grad_norm": 1.071026086807251,
            "learning_rate": 1.4913442854619325e-05,
            "epoch": 2.1051934287228407,
            "step": 31780
        },
        {
            "loss": 0.3629,
            "grad_norm": 30.226245880126953,
            "learning_rate": 1.4891361950185481e-05,
            "epoch": 2.1065182829888713,
            "step": 31800
        },
        {
            "loss": 0.391,
            "grad_norm": 1.0196037292480469,
            "learning_rate": 1.4869281045751634e-05,
            "epoch": 2.107843137254902,
            "step": 31820
        },
        {
            "loss": 0.3517,
            "grad_norm": 2.0094261169433594,
            "learning_rate": 1.4847200141317788e-05,
            "epoch": 2.1091679915209327,
            "step": 31840
        },
        {
            "loss": 0.3918,
            "grad_norm": 2.193554401397705,
            "learning_rate": 1.4825119236883945e-05,
            "epoch": 2.1104928457869634,
            "step": 31860
        },
        {
            "loss": 0.3799,
            "grad_norm": 1.229418158531189,
            "learning_rate": 1.4803038332450097e-05,
            "epoch": 2.111817700052994,
            "step": 31880
        },
        {
            "loss": 0.3486,
            "grad_norm": 0.8769271969795227,
            "learning_rate": 1.4780957428016252e-05,
            "epoch": 2.1131425543190248,
            "step": 31900
        },
        {
            "loss": 0.3821,
            "grad_norm": 3.2019364833831787,
            "learning_rate": 1.4758876523582408e-05,
            "epoch": 2.1144674085850554,
            "step": 31920
        },
        {
            "loss": 0.3824,
            "grad_norm": 0.9637615084648132,
            "learning_rate": 1.473679561914856e-05,
            "epoch": 2.1157922628510866,
            "step": 31940
        },
        {
            "loss": 0.368,
            "grad_norm": 3.712082624435425,
            "learning_rate": 1.4714714714714713e-05,
            "epoch": 2.1171171171171173,
            "step": 31960
        },
        {
            "loss": 0.3812,
            "grad_norm": 3.3539483547210693,
            "learning_rate": 1.469263381028087e-05,
            "epoch": 2.118441971383148,
            "step": 31980
        },
        {
            "loss": 0.3901,
            "grad_norm": 3.176162004470825,
            "learning_rate": 1.4670552905847024e-05,
            "epoch": 2.1197668256491786,
            "step": 32000
        },
        {
            "loss": 0.3801,
            "grad_norm": 1.266837239265442,
            "learning_rate": 1.464847200141318e-05,
            "epoch": 2.1210916799152093,
            "step": 32020
        },
        {
            "loss": 0.4048,
            "grad_norm": 3.7789113521575928,
            "learning_rate": 1.4626391096979333e-05,
            "epoch": 2.12241653418124,
            "step": 32040
        },
        {
            "loss": 0.4041,
            "grad_norm": 2.8273165225982666,
            "learning_rate": 1.4604310192545487e-05,
            "epoch": 2.1237413884472707,
            "step": 32060
        },
        {
            "loss": 0.3563,
            "grad_norm": 0.9565436244010925,
            "learning_rate": 1.4582229288111643e-05,
            "epoch": 2.1250662427133014,
            "step": 32080
        },
        {
            "loss": 0.4082,
            "grad_norm": 3.978126287460327,
            "learning_rate": 1.4560148383677796e-05,
            "epoch": 2.126391096979332,
            "step": 32100
        },
        {
            "loss": 0.4086,
            "grad_norm": 1.0991790294647217,
            "learning_rate": 1.4538067479243949e-05,
            "epoch": 2.127715951245363,
            "step": 32120
        },
        {
            "loss": 0.3328,
            "grad_norm": 1.3097745180130005,
            "learning_rate": 1.4515986574810107e-05,
            "epoch": 2.129040805511394,
            "step": 32140
        },
        {
            "loss": 0.369,
            "grad_norm": 0.9469362497329712,
            "learning_rate": 1.449390567037626e-05,
            "epoch": 2.1303656597774245,
            "step": 32160
        },
        {
            "loss": 0.3924,
            "grad_norm": 3.788116216659546,
            "learning_rate": 1.4471824765942412e-05,
            "epoch": 2.131690514043455,
            "step": 32180
        },
        {
            "loss": 0.3656,
            "grad_norm": 0.7351890802383423,
            "learning_rate": 1.4449743861508568e-05,
            "epoch": 2.133015368309486,
            "step": 32200
        },
        {
            "loss": 0.3721,
            "grad_norm": 1.185826063156128,
            "learning_rate": 1.4427662957074723e-05,
            "epoch": 2.1343402225755166,
            "step": 32220
        },
        {
            "loss": 0.3096,
            "grad_norm": 3.8913912773132324,
            "learning_rate": 1.4405582052640876e-05,
            "epoch": 2.1356650768415473,
            "step": 32240
        },
        {
            "loss": 0.444,
            "grad_norm": 0.7924781441688538,
            "learning_rate": 1.4383501148207032e-05,
            "epoch": 2.1369899311075784,
            "step": 32260
        },
        {
            "loss": 0.4098,
            "grad_norm": 3.006007671356201,
            "learning_rate": 1.4361420243773186e-05,
            "epoch": 2.138314785373609,
            "step": 32280
        },
        {
            "loss": 0.3881,
            "grad_norm": 1.9935940504074097,
            "learning_rate": 1.4339339339339339e-05,
            "epoch": 2.1396396396396398,
            "step": 32300
        },
        {
            "loss": 0.3785,
            "grad_norm": 0.46906015276908875,
            "learning_rate": 1.4317258434905495e-05,
            "epoch": 2.1409644939056705,
            "step": 32320
        },
        {
            "loss": 0.3362,
            "grad_norm": 2.935389280319214,
            "learning_rate": 1.4295177530471648e-05,
            "epoch": 2.142289348171701,
            "step": 32340
        },
        {
            "loss": 0.3985,
            "grad_norm": 2.7390549182891846,
            "learning_rate": 1.4273096626037802e-05,
            "epoch": 2.143614202437732,
            "step": 32360
        },
        {
            "loss": 0.4246,
            "grad_norm": 1.6281414031982422,
            "learning_rate": 1.4251015721603958e-05,
            "epoch": 2.1449390567037625,
            "step": 32380
        },
        {
            "loss": 0.3633,
            "grad_norm": 0.8861482739448547,
            "learning_rate": 1.4228934817170111e-05,
            "epoch": 2.146263910969793,
            "step": 32400
        },
        {
            "loss": 0.4241,
            "grad_norm": 2.812314510345459,
            "learning_rate": 1.4206853912736267e-05,
            "epoch": 2.147588765235824,
            "step": 32420
        },
        {
            "loss": 0.4371,
            "grad_norm": 3.0507619380950928,
            "learning_rate": 1.4184773008302422e-05,
            "epoch": 2.148913619501855,
            "step": 32440
        },
        {
            "loss": 0.411,
            "grad_norm": 2.4654133319854736,
            "learning_rate": 1.4162692103868574e-05,
            "epoch": 2.1502384737678857,
            "step": 32460
        },
        {
            "loss": 0.3842,
            "grad_norm": 0.8188520073890686,
            "learning_rate": 1.414061119943473e-05,
            "epoch": 2.1515633280339164,
            "step": 32480
        },
        {
            "loss": 0.3384,
            "grad_norm": 0.9329509139060974,
            "learning_rate": 1.4118530295000883e-05,
            "epoch": 2.152888182299947,
            "step": 32500
        },
        {
            "loss": 0.3618,
            "grad_norm": 2.8253204822540283,
            "learning_rate": 1.4096449390567038e-05,
            "epoch": 2.1542130365659777,
            "step": 32520
        },
        {
            "loss": 0.3626,
            "grad_norm": 2.311368703842163,
            "learning_rate": 1.4074368486133194e-05,
            "epoch": 2.1555378908320084,
            "step": 32540
        },
        {
            "loss": 0.3851,
            "grad_norm": 2.1205623149871826,
            "learning_rate": 1.4052287581699347e-05,
            "epoch": 2.156862745098039,
            "step": 32560
        },
        {
            "loss": 0.3619,
            "grad_norm": 3.5256826877593994,
            "learning_rate": 1.4030206677265501e-05,
            "epoch": 2.15818759936407,
            "step": 32580
        },
        {
            "loss": 0.3278,
            "grad_norm": 3.002683639526367,
            "learning_rate": 1.4008125772831657e-05,
            "epoch": 2.159512453630101,
            "step": 32600
        },
        {
            "loss": 0.4273,
            "grad_norm": 1.692956566810608,
            "learning_rate": 1.398604486839781e-05,
            "epoch": 2.1608373078961316,
            "step": 32620
        },
        {
            "loss": 0.3523,
            "grad_norm": 1.904348373413086,
            "learning_rate": 1.3963963963963963e-05,
            "epoch": 2.1621621621621623,
            "step": 32640
        },
        {
            "loss": 0.3609,
            "grad_norm": 1.0930100679397583,
            "learning_rate": 1.394188305953012e-05,
            "epoch": 2.163487016428193,
            "step": 32660
        },
        {
            "loss": 0.391,
            "grad_norm": 0.9034479856491089,
            "learning_rate": 1.3919802155096273e-05,
            "epoch": 2.1648118706942236,
            "step": 32680
        },
        {
            "loss": 0.4051,
            "grad_norm": 0.972886860370636,
            "learning_rate": 1.3897721250662426e-05,
            "epoch": 2.1661367249602543,
            "step": 32700
        },
        {
            "loss": 0.3935,
            "grad_norm": 1.9267966747283936,
            "learning_rate": 1.3875640346228582e-05,
            "epoch": 2.167461579226285,
            "step": 32720
        },
        {
            "loss": 0.4333,
            "grad_norm": 0.9840965270996094,
            "learning_rate": 1.3853559441794737e-05,
            "epoch": 2.1687864334923157,
            "step": 32740
        },
        {
            "loss": 0.3516,
            "grad_norm": 0.5882647037506104,
            "learning_rate": 1.383147853736089e-05,
            "epoch": 2.1701112877583464,
            "step": 32760
        },
        {
            "loss": 0.3756,
            "grad_norm": 0.8321747779846191,
            "learning_rate": 1.3809397632927046e-05,
            "epoch": 2.1714361420243775,
            "step": 32780
        },
        {
            "loss": 0.3854,
            "grad_norm": 2.2353663444519043,
            "learning_rate": 1.37873167284932e-05,
            "epoch": 2.172760996290408,
            "step": 32800
        },
        {
            "loss": 0.3403,
            "grad_norm": 0.9677185416221619,
            "learning_rate": 1.3765235824059356e-05,
            "epoch": 2.174085850556439,
            "step": 32820
        },
        {
            "loss": 0.3763,
            "grad_norm": 2.0224761962890625,
            "learning_rate": 1.3743154919625509e-05,
            "epoch": 2.1754107048224696,
            "step": 32840
        },
        {
            "loss": 0.3312,
            "grad_norm": 1.1643030643463135,
            "learning_rate": 1.3721074015191662e-05,
            "epoch": 2.1767355590885002,
            "step": 32860
        },
        {
            "loss": 0.3431,
            "grad_norm": 3.0394704341888428,
            "learning_rate": 1.3698993110757818e-05,
            "epoch": 2.178060413354531,
            "step": 32880
        },
        {
            "loss": 0.412,
            "grad_norm": 2.1852426528930664,
            "learning_rate": 1.3676912206323972e-05,
            "epoch": 2.1793852676205616,
            "step": 32900
        },
        {
            "loss": 0.3591,
            "grad_norm": 1.9581332206726074,
            "learning_rate": 1.3654831301890125e-05,
            "epoch": 2.1807101218865923,
            "step": 32920
        },
        {
            "loss": 0.4179,
            "grad_norm": 1.1302733421325684,
            "learning_rate": 1.3632750397456281e-05,
            "epoch": 2.1820349761526234,
            "step": 32940
        },
        {
            "loss": 0.3818,
            "grad_norm": 1.356160044670105,
            "learning_rate": 1.3610669493022436e-05,
            "epoch": 2.183359830418654,
            "step": 32960
        },
        {
            "loss": 0.371,
            "grad_norm": 2.7131264209747314,
            "learning_rate": 1.3588588588588588e-05,
            "epoch": 2.184684684684685,
            "step": 32980
        },
        {
            "loss": 0.3851,
            "grad_norm": 1.8800913095474243,
            "learning_rate": 1.3566507684154744e-05,
            "epoch": 2.1860095389507155,
            "step": 33000
        },
        {
            "loss": 0.3689,
            "grad_norm": 1.2151069641113281,
            "learning_rate": 1.3544426779720897e-05,
            "epoch": 2.187334393216746,
            "step": 33020
        },
        {
            "loss": 0.373,
            "grad_norm": 1.9244451522827148,
            "learning_rate": 1.3522345875287052e-05,
            "epoch": 2.188659247482777,
            "step": 33040
        },
        {
            "loss": 0.4101,
            "grad_norm": 2.1406474113464355,
            "learning_rate": 1.3500264970853208e-05,
            "epoch": 2.1899841017488075,
            "step": 33060
        },
        {
            "loss": 0.3445,
            "grad_norm": 1.0876476764678955,
            "learning_rate": 1.347818406641936e-05,
            "epoch": 2.191308956014838,
            "step": 33080
        },
        {
            "loss": 0.3759,
            "grad_norm": 1.2355444431304932,
            "learning_rate": 1.3456103161985515e-05,
            "epoch": 2.192633810280869,
            "step": 33100
        },
        {
            "loss": 0.3604,
            "grad_norm": 1.0018481016159058,
            "learning_rate": 1.3434022257551671e-05,
            "epoch": 2.1939586645469,
            "step": 33120
        },
        {
            "loss": 0.3295,
            "grad_norm": 3.915137529373169,
            "learning_rate": 1.3411941353117824e-05,
            "epoch": 2.1952835188129307,
            "step": 33140
        },
        {
            "loss": 0.4136,
            "grad_norm": 0.7551550269126892,
            "learning_rate": 1.3389860448683977e-05,
            "epoch": 2.1966083730789614,
            "step": 33160
        },
        {
            "loss": 0.3601,
            "grad_norm": 4.011435031890869,
            "learning_rate": 1.3367779544250134e-05,
            "epoch": 2.197933227344992,
            "step": 33180
        },
        {
            "loss": 0.3532,
            "grad_norm": 0.9311548471450806,
            "learning_rate": 1.3345698639816287e-05,
            "epoch": 2.1992580816110228,
            "step": 33200
        },
        {
            "loss": 0.3877,
            "grad_norm": 0.6870518922805786,
            "learning_rate": 1.3323617735382443e-05,
            "epoch": 2.2005829358770534,
            "step": 33220
        },
        {
            "loss": 0.4014,
            "grad_norm": 0.8333761096000671,
            "learning_rate": 1.3301536830948596e-05,
            "epoch": 2.201907790143084,
            "step": 33240
        },
        {
            "loss": 0.3731,
            "grad_norm": 4.048952102661133,
            "learning_rate": 1.327945592651475e-05,
            "epoch": 2.203232644409115,
            "step": 33260
        },
        {
            "loss": 0.3819,
            "grad_norm": 2.137615442276001,
            "learning_rate": 1.3257375022080907e-05,
            "epoch": 2.204557498675146,
            "step": 33280
        },
        {
            "loss": 0.3872,
            "grad_norm": 1.2274091243743896,
            "learning_rate": 1.323529411764706e-05,
            "epoch": 2.2058823529411766,
            "step": 33300
        },
        {
            "loss": 0.4124,
            "grad_norm": 4.1732869148254395,
            "learning_rate": 1.3213213213213214e-05,
            "epoch": 2.2072072072072073,
            "step": 33320
        },
        {
            "loss": 0.3542,
            "grad_norm": 1.9864097833633423,
            "learning_rate": 1.319113230877937e-05,
            "epoch": 2.208532061473238,
            "step": 33340
        },
        {
            "loss": 0.3897,
            "grad_norm": 1.1846435070037842,
            "learning_rate": 1.3169051404345523e-05,
            "epoch": 2.2098569157392687,
            "step": 33360
        },
        {
            "loss": 0.3543,
            "grad_norm": 2.8107962608337402,
            "learning_rate": 1.3146970499911675e-05,
            "epoch": 2.2111817700052994,
            "step": 33380
        },
        {
            "loss": 0.4486,
            "grad_norm": 2.747248411178589,
            "learning_rate": 1.3124889595477832e-05,
            "epoch": 2.21250662427133,
            "step": 33400
        },
        {
            "loss": 0.3788,
            "grad_norm": 0.8955715298652649,
            "learning_rate": 1.3102808691043986e-05,
            "epoch": 2.2138314785373607,
            "step": 33420
        },
        {
            "loss": 0.3777,
            "grad_norm": 2.451744556427002,
            "learning_rate": 1.3080727786610139e-05,
            "epoch": 2.2151563328033914,
            "step": 33440
        },
        {
            "loss": 0.3362,
            "grad_norm": 0.48866933584213257,
            "learning_rate": 1.3058646882176295e-05,
            "epoch": 2.2164811870694225,
            "step": 33460
        },
        {
            "loss": 0.378,
            "grad_norm": 2.2585980892181396,
            "learning_rate": 1.303656597774245e-05,
            "epoch": 2.2178060413354532,
            "step": 33480
        },
        {
            "loss": 0.3715,
            "grad_norm": 4.285512924194336,
            "learning_rate": 1.3014485073308602e-05,
            "epoch": 2.219130895601484,
            "step": 33500
        },
        {
            "loss": 0.3229,
            "grad_norm": 2.942695140838623,
            "learning_rate": 1.2992404168874758e-05,
            "epoch": 2.2204557498675146,
            "step": 33520
        },
        {
            "loss": 0.35,
            "grad_norm": 0.45989012718200684,
            "learning_rate": 1.2970323264440911e-05,
            "epoch": 2.2217806041335453,
            "step": 33540
        },
        {
            "loss": 0.4177,
            "grad_norm": 1.991589069366455,
            "learning_rate": 1.2948242360007065e-05,
            "epoch": 2.223105458399576,
            "step": 33560
        },
        {
            "loss": 0.3307,
            "grad_norm": 1.1863614320755005,
            "learning_rate": 1.2926161455573222e-05,
            "epoch": 2.2244303126656066,
            "step": 33580
        },
        {
            "loss": 0.3546,
            "grad_norm": 2.7586565017700195,
            "learning_rate": 1.2904080551139374e-05,
            "epoch": 2.2257551669316373,
            "step": 33600
        },
        {
            "loss": 0.3033,
            "grad_norm": 0.9523965120315552,
            "learning_rate": 1.288199964670553e-05,
            "epoch": 2.2270800211976685,
            "step": 33620
        },
        {
            "loss": 0.381,
            "grad_norm": 1.115904450416565,
            "learning_rate": 1.2859918742271685e-05,
            "epoch": 2.228404875463699,
            "step": 33640
        },
        {
            "loss": 0.3943,
            "grad_norm": 1.5014865398406982,
            "learning_rate": 1.2837837837837838e-05,
            "epoch": 2.22972972972973,
            "step": 33660
        },
        {
            "loss": 0.3579,
            "grad_norm": 2.4814815521240234,
            "learning_rate": 1.2815756933403994e-05,
            "epoch": 2.2310545839957605,
            "step": 33680
        },
        {
            "loss": 0.4151,
            "grad_norm": 1.2062731981277466,
            "learning_rate": 1.2793676028970148e-05,
            "epoch": 2.232379438261791,
            "step": 33700
        },
        {
            "loss": 0.3366,
            "grad_norm": 1.623953938484192,
            "learning_rate": 1.2771595124536301e-05,
            "epoch": 2.233704292527822,
            "step": 33720
        },
        {
            "loss": 0.359,
            "grad_norm": 1.1878191232681274,
            "learning_rate": 1.2749514220102457e-05,
            "epoch": 2.2350291467938526,
            "step": 33740
        },
        {
            "loss": 0.324,
            "grad_norm": 2.870492696762085,
            "learning_rate": 1.272743331566861e-05,
            "epoch": 2.2363540010598832,
            "step": 33760
        },
        {
            "loss": 0.3743,
            "grad_norm": 3.751486301422119,
            "learning_rate": 1.2705352411234764e-05,
            "epoch": 2.237678855325914,
            "step": 33780
        },
        {
            "loss": 0.3825,
            "grad_norm": 1.4586127996444702,
            "learning_rate": 1.268327150680092e-05,
            "epoch": 2.239003709591945,
            "step": 33800
        },
        {
            "loss": 0.3283,
            "grad_norm": 1.4714627265930176,
            "learning_rate": 1.2661190602367073e-05,
            "epoch": 2.2403285638579757,
            "step": 33820
        },
        {
            "loss": 0.3386,
            "grad_norm": 0.9065923094749451,
            "learning_rate": 1.2639109697933228e-05,
            "epoch": 2.2416534181240064,
            "step": 33840
        },
        {
            "loss": 0.3576,
            "grad_norm": 4.779514312744141,
            "learning_rate": 1.2617028793499384e-05,
            "epoch": 2.242978272390037,
            "step": 33860
        },
        {
            "loss": 0.4144,
            "grad_norm": 1.2621878385543823,
            "learning_rate": 1.2594947889065536e-05,
            "epoch": 2.244303126656068,
            "step": 33880
        },
        {
            "loss": 0.4082,
            "grad_norm": 3.0355167388916016,
            "learning_rate": 1.257286698463169e-05,
            "epoch": 2.2456279809220985,
            "step": 33900
        },
        {
            "loss": 0.3195,
            "grad_norm": 0.828086793422699,
            "learning_rate": 1.2550786080197845e-05,
            "epoch": 2.246952835188129,
            "step": 33920
        },
        {
            "loss": 0.3934,
            "grad_norm": 2.9876346588134766,
            "learning_rate": 1.2528705175764e-05,
            "epoch": 2.24827768945416,
            "step": 33940
        },
        {
            "loss": 0.3801,
            "grad_norm": 1.0611629486083984,
            "learning_rate": 1.2506624271330153e-05,
            "epoch": 2.249602543720191,
            "step": 33960
        },
        {
            "loss": 0.4077,
            "grad_norm": 0.9506841897964478,
            "learning_rate": 1.2484543366896309e-05,
            "epoch": 2.2509273979862217,
            "step": 33980
        },
        {
            "loss": 0.3606,
            "grad_norm": 1.219243049621582,
            "learning_rate": 1.2462462462462463e-05,
            "epoch": 2.2522522522522523,
            "step": 34000
        },
        {
            "loss": 0.3881,
            "grad_norm": 2.164654016494751,
            "learning_rate": 1.2440381558028618e-05,
            "epoch": 2.253577106518283,
            "step": 34020
        },
        {
            "loss": 0.3595,
            "grad_norm": 2.244755744934082,
            "learning_rate": 1.2418300653594772e-05,
            "epoch": 2.2549019607843137,
            "step": 34040
        },
        {
            "loss": 0.4147,
            "grad_norm": 3.0198373794555664,
            "learning_rate": 1.2396219749160925e-05,
            "epoch": 2.2562268150503444,
            "step": 34060
        },
        {
            "loss": 0.4013,
            "grad_norm": 0.8575655221939087,
            "learning_rate": 1.2374138844727081e-05,
            "epoch": 2.257551669316375,
            "step": 34080
        },
        {
            "loss": 0.3435,
            "grad_norm": 1.1289094686508179,
            "learning_rate": 1.2352057940293235e-05,
            "epoch": 2.2588765235824058,
            "step": 34100
        },
        {
            "loss": 0.4146,
            "grad_norm": 0.9575690031051636,
            "learning_rate": 1.232997703585939e-05,
            "epoch": 2.2602013778484364,
            "step": 34120
        },
        {
            "loss": 0.3174,
            "grad_norm": 1.8740190267562866,
            "learning_rate": 1.2307896131425543e-05,
            "epoch": 2.2615262321144676,
            "step": 34140
        },
        {
            "loss": 0.3382,
            "grad_norm": 1.3661012649536133,
            "learning_rate": 1.2285815226991699e-05,
            "epoch": 2.2628510863804983,
            "step": 34160
        },
        {
            "loss": 0.3618,
            "grad_norm": 1.0095112323760986,
            "learning_rate": 1.2263734322557853e-05,
            "epoch": 2.264175940646529,
            "step": 34180
        },
        {
            "loss": 0.3708,
            "grad_norm": 0.41986367106437683,
            "learning_rate": 1.2241653418124006e-05,
            "epoch": 2.2655007949125596,
            "step": 34200
        },
        {
            "loss": 0.3658,
            "grad_norm": 1.2068580389022827,
            "learning_rate": 1.2219572513690162e-05,
            "epoch": 2.2668256491785903,
            "step": 34220
        },
        {
            "loss": 0.4121,
            "grad_norm": 4.718790531158447,
            "learning_rate": 1.2197491609256316e-05,
            "epoch": 2.268150503444621,
            "step": 34240
        },
        {
            "loss": 0.3969,
            "grad_norm": 3.0955374240875244,
            "learning_rate": 1.217541070482247e-05,
            "epoch": 2.2694753577106517,
            "step": 34260
        },
        {
            "loss": 0.3731,
            "grad_norm": 1.2717392444610596,
            "learning_rate": 1.2153329800388624e-05,
            "epoch": 2.270800211976683,
            "step": 34280
        },
        {
            "loss": 0.3667,
            "grad_norm": 2.443648099899292,
            "learning_rate": 1.213124889595478e-05,
            "epoch": 2.2721250662427135,
            "step": 34300
        },
        {
            "loss": 0.3938,
            "grad_norm": 2.161792755126953,
            "learning_rate": 1.2109167991520934e-05,
            "epoch": 2.273449920508744,
            "step": 34320
        },
        {
            "loss": 0.3357,
            "grad_norm": 3.910921812057495,
            "learning_rate": 1.2087087087087087e-05,
            "epoch": 2.274774774774775,
            "step": 34340
        },
        {
            "loss": 0.4088,
            "grad_norm": 4.114978790283203,
            "learning_rate": 1.2065006182653241e-05,
            "epoch": 2.2760996290408055,
            "step": 34360
        },
        {
            "loss": 0.4087,
            "grad_norm": 0.9927592277526855,
            "learning_rate": 1.2042925278219398e-05,
            "epoch": 2.2774244833068362,
            "step": 34380
        },
        {
            "loss": 0.3806,
            "grad_norm": 2.195683002471924,
            "learning_rate": 1.202084437378555e-05,
            "epoch": 2.278749337572867,
            "step": 34400
        },
        {
            "loss": 0.3551,
            "grad_norm": 1.1147840023040771,
            "learning_rate": 1.1998763469351705e-05,
            "epoch": 2.2800741918388976,
            "step": 34420
        },
        {
            "loss": 0.378,
            "grad_norm": 3.0896453857421875,
            "learning_rate": 1.197668256491786e-05,
            "epoch": 2.2813990461049283,
            "step": 34440
        },
        {
            "loss": 0.3453,
            "grad_norm": 1.287156581878662,
            "learning_rate": 1.1954601660484014e-05,
            "epoch": 2.282723900370959,
            "step": 34460
        },
        {
            "loss": 0.4062,
            "grad_norm": 0.63431715965271,
            "learning_rate": 1.1932520756050168e-05,
            "epoch": 2.28404875463699,
            "step": 34480
        },
        {
            "loss": 0.3375,
            "grad_norm": 0.4288928508758545,
            "learning_rate": 1.1910439851616323e-05,
            "epoch": 2.2853736089030208,
            "step": 34500
        },
        {
            "loss": 0.3861,
            "grad_norm": 0.8141670823097229,
            "learning_rate": 1.1888358947182477e-05,
            "epoch": 2.2866984631690515,
            "step": 34520
        },
        {
            "loss": 0.3171,
            "grad_norm": 3.922764301300049,
            "learning_rate": 1.1866278042748631e-05,
            "epoch": 2.288023317435082,
            "step": 34540
        },
        {
            "loss": 0.3724,
            "grad_norm": 0.9558922648429871,
            "learning_rate": 1.1844197138314786e-05,
            "epoch": 2.289348171701113,
            "step": 34560
        },
        {
            "loss": 0.3845,
            "grad_norm": 0.9265446066856384,
            "learning_rate": 1.182211623388094e-05,
            "epoch": 2.2906730259671435,
            "step": 34580
        },
        {
            "loss": 0.4086,
            "grad_norm": 1.028294563293457,
            "learning_rate": 1.1800035329447095e-05,
            "epoch": 2.291997880233174,
            "step": 34600
        },
        {
            "loss": 0.3465,
            "grad_norm": 2.848367929458618,
            "learning_rate": 1.177795442501325e-05,
            "epoch": 2.2933227344992053,
            "step": 34620
        },
        {
            "loss": 0.4096,
            "grad_norm": 1.8884220123291016,
            "learning_rate": 1.1755873520579404e-05,
            "epoch": 2.294647588765236,
            "step": 34640
        },
        {
            "loss": 0.3719,
            "grad_norm": 2.92533540725708,
            "learning_rate": 1.1733792616145556e-05,
            "epoch": 2.2959724430312667,
            "step": 34660
        },
        {
            "loss": 0.341,
            "grad_norm": 2.7221200466156006,
            "learning_rate": 1.1711711711711713e-05,
            "epoch": 2.2972972972972974,
            "step": 34680
        },
        {
            "loss": 0.4016,
            "grad_norm": 1.1541379690170288,
            "learning_rate": 1.1689630807277867e-05,
            "epoch": 2.298622151563328,
            "step": 34700
        },
        {
            "loss": 0.3557,
            "grad_norm": 2.1339027881622314,
            "learning_rate": 1.1667549902844021e-05,
            "epoch": 2.2999470058293587,
            "step": 34720
        },
        {
            "loss": 0.3897,
            "grad_norm": 0.8412057161331177,
            "learning_rate": 1.1645468998410176e-05,
            "epoch": 2.3012718600953894,
            "step": 34740
        },
        {
            "loss": 0.3529,
            "grad_norm": 153.6876678466797,
            "learning_rate": 1.162338809397633e-05,
            "epoch": 2.30259671436142,
            "step": 34760
        },
        {
            "loss": 0.3107,
            "grad_norm": 0.9628480076789856,
            "learning_rate": 1.1601307189542485e-05,
            "epoch": 2.303921568627451,
            "step": 34780
        },
        {
            "loss": 0.3527,
            "grad_norm": 4.658498287200928,
            "learning_rate": 1.1579226285108637e-05,
            "epoch": 2.3052464228934815,
            "step": 34800
        },
        {
            "loss": 0.3717,
            "grad_norm": 1.190053939819336,
            "learning_rate": 1.1557145380674794e-05,
            "epoch": 2.3065712771595126,
            "step": 34820
        },
        {
            "loss": 0.4097,
            "grad_norm": 2.1946210861206055,
            "learning_rate": 1.1535064476240948e-05,
            "epoch": 2.3078961314255433,
            "step": 34840
        },
        {
            "loss": 0.367,
            "grad_norm": 0.5807031393051147,
            "learning_rate": 1.1512983571807102e-05,
            "epoch": 2.309220985691574,
            "step": 34860
        },
        {
            "loss": 0.3955,
            "grad_norm": 2.817666530609131,
            "learning_rate": 1.1490902667373255e-05,
            "epoch": 2.3105458399576047,
            "step": 34880
        },
        {
            "loss": 0.3664,
            "grad_norm": 1.2472128868103027,
            "learning_rate": 1.1468821762939411e-05,
            "epoch": 2.3118706942236353,
            "step": 34900
        },
        {
            "loss": 0.4052,
            "grad_norm": 2.277970552444458,
            "learning_rate": 1.1446740858505566e-05,
            "epoch": 2.313195548489666,
            "step": 34920
        },
        {
            "loss": 0.4279,
            "grad_norm": 2.093055486679077,
            "learning_rate": 1.1424659954071719e-05,
            "epoch": 2.3145204027556967,
            "step": 34940
        },
        {
            "loss": 0.314,
            "grad_norm": 0.02386433817446232,
            "learning_rate": 1.1402579049637873e-05,
            "epoch": 2.315845257021728,
            "step": 34960
        },
        {
            "loss": 0.3497,
            "grad_norm": 2.7939293384552,
            "learning_rate": 1.1380498145204029e-05,
            "epoch": 2.3171701112877585,
            "step": 34980
        },
        {
            "loss": 0.339,
            "grad_norm": 0.9405109882354736,
            "learning_rate": 1.1358417240770182e-05,
            "epoch": 2.318494965553789,
            "step": 35000
        },
        {
            "loss": 0.3482,
            "grad_norm": 1.2109864950180054,
            "learning_rate": 1.1336336336336336e-05,
            "epoch": 2.31981981981982,
            "step": 35020
        },
        {
            "loss": 0.4238,
            "grad_norm": 1.911246418952942,
            "learning_rate": 1.131425543190249e-05,
            "epoch": 2.3211446740858506,
            "step": 35040
        },
        {
            "loss": 0.391,
            "grad_norm": 0.9482028484344482,
            "learning_rate": 1.1292174527468647e-05,
            "epoch": 2.3224695283518813,
            "step": 35060
        },
        {
            "loss": 0.357,
            "grad_norm": 1.8412202596664429,
            "learning_rate": 1.12700936230348e-05,
            "epoch": 2.323794382617912,
            "step": 35080
        },
        {
            "loss": 0.3478,
            "grad_norm": 2.0878937244415283,
            "learning_rate": 1.1248012718600954e-05,
            "epoch": 2.3251192368839426,
            "step": 35100
        },
        {
            "loss": 0.3493,
            "grad_norm": 2.098478078842163,
            "learning_rate": 1.1225931814167109e-05,
            "epoch": 2.3264440911499733,
            "step": 35120
        },
        {
            "loss": 0.4804,
            "grad_norm": 2.01448392868042,
            "learning_rate": 1.1203850909733263e-05,
            "epoch": 2.3277689454160044,
            "step": 35140
        },
        {
            "loss": 0.383,
            "grad_norm": 3.406465530395508,
            "learning_rate": 1.1181770005299417e-05,
            "epoch": 2.329093799682035,
            "step": 35160
        },
        {
            "loss": 0.4008,
            "grad_norm": 1.3070248365402222,
            "learning_rate": 1.1159689100865572e-05,
            "epoch": 2.330418653948066,
            "step": 35180
        },
        {
            "loss": 0.3585,
            "grad_norm": 0.4382569193840027,
            "learning_rate": 1.1137608196431726e-05,
            "epoch": 2.3317435082140965,
            "step": 35200
        },
        {
            "loss": 0.3707,
            "grad_norm": 0.9812231659889221,
            "learning_rate": 1.111552729199788e-05,
            "epoch": 2.333068362480127,
            "step": 35220
        },
        {
            "loss": 0.362,
            "grad_norm": 3.9185843467712402,
            "learning_rate": 1.1093446387564035e-05,
            "epoch": 2.334393216746158,
            "step": 35240
        },
        {
            "loss": 0.3451,
            "grad_norm": 1.3168941736221313,
            "learning_rate": 1.107136548313019e-05,
            "epoch": 2.3357180710121885,
            "step": 35260
        },
        {
            "loss": 0.3626,
            "grad_norm": 2.2601211071014404,
            "learning_rate": 1.1049284578696344e-05,
            "epoch": 2.337042925278219,
            "step": 35280
        },
        {
            "loss": 0.3808,
            "grad_norm": 1.1663048267364502,
            "learning_rate": 1.1027203674262499e-05,
            "epoch": 2.3383677795442503,
            "step": 35300
        },
        {
            "loss": 0.3586,
            "grad_norm": 2.0626649856567383,
            "learning_rate": 1.1005122769828653e-05,
            "epoch": 2.339692633810281,
            "step": 35320
        },
        {
            "loss": 0.4238,
            "grad_norm": 2.252084255218506,
            "learning_rate": 1.0983041865394807e-05,
            "epoch": 2.3410174880763117,
            "step": 35340
        },
        {
            "loss": 0.3595,
            "grad_norm": 1.4441285133361816,
            "learning_rate": 1.0960960960960962e-05,
            "epoch": 2.3423423423423424,
            "step": 35360
        },
        {
            "loss": 0.4042,
            "grad_norm": 0.8897571563720703,
            "learning_rate": 1.0938880056527116e-05,
            "epoch": 2.343667196608373,
            "step": 35380
        },
        {
            "loss": 0.3677,
            "grad_norm": 0.5189454555511475,
            "learning_rate": 1.0916799152093269e-05,
            "epoch": 2.3449920508744038,
            "step": 35400
        },
        {
            "loss": 0.4058,
            "grad_norm": 1.3419867753982544,
            "learning_rate": 1.0894718247659425e-05,
            "epoch": 2.3463169051404345,
            "step": 35420
        },
        {
            "loss": 0.3591,
            "grad_norm": 1.0319021940231323,
            "learning_rate": 1.087263734322558e-05,
            "epoch": 2.347641759406465,
            "step": 35440
        },
        {
            "loss": 0.3652,
            "grad_norm": 1.671454906463623,
            "learning_rate": 1.0850556438791734e-05,
            "epoch": 2.348966613672496,
            "step": 35460
        },
        {
            "loss": 0.4094,
            "grad_norm": 1.0431971549987793,
            "learning_rate": 1.0828475534357887e-05,
            "epoch": 2.350291467938527,
            "step": 35480
        },
        {
            "loss": 0.3411,
            "grad_norm": 2.069902181625366,
            "learning_rate": 1.0806394629924043e-05,
            "epoch": 2.3516163222045576,
            "step": 35500
        },
        {
            "loss": 0.3624,
            "grad_norm": 0.9301276803016663,
            "learning_rate": 1.0784313725490197e-05,
            "epoch": 2.3529411764705883,
            "step": 35520
        },
        {
            "loss": 0.3652,
            "grad_norm": 1.5062652826309204,
            "learning_rate": 1.076223282105635e-05,
            "epoch": 2.354266030736619,
            "step": 35540
        },
        {
            "loss": 0.3682,
            "grad_norm": 2.2318317890167236,
            "learning_rate": 1.0740151916622505e-05,
            "epoch": 2.3555908850026497,
            "step": 35560
        },
        {
            "loss": 0.3451,
            "grad_norm": 1.1889028549194336,
            "learning_rate": 1.071807101218866e-05,
            "epoch": 2.3569157392686804,
            "step": 35580
        },
        {
            "loss": 0.3632,
            "grad_norm": 3.2823283672332764,
            "learning_rate": 1.0695990107754813e-05,
            "epoch": 2.358240593534711,
            "step": 35600
        },
        {
            "loss": 0.3934,
            "grad_norm": 0.6795153617858887,
            "learning_rate": 1.0673909203320968e-05,
            "epoch": 2.3595654478007417,
            "step": 35620
        },
        {
            "loss": 0.392,
            "grad_norm": 2.653794288635254,
            "learning_rate": 1.0651828298887122e-05,
            "epoch": 2.360890302066773,
            "step": 35640
        },
        {
            "loss": 0.4261,
            "grad_norm": 2.4422600269317627,
            "learning_rate": 1.0629747394453278e-05,
            "epoch": 2.3622151563328035,
            "step": 35660
        },
        {
            "loss": 0.3615,
            "grad_norm": 4.671518325805664,
            "learning_rate": 1.0607666490019431e-05,
            "epoch": 2.3635400105988342,
            "step": 35680
        },
        {
            "loss": 0.3809,
            "grad_norm": 2.00557541847229,
            "learning_rate": 1.0585585585585586e-05,
            "epoch": 2.364864864864865,
            "step": 35700
        },
        {
            "loss": 0.3933,
            "grad_norm": 1.6863703727722168,
            "learning_rate": 1.0563504681151742e-05,
            "epoch": 2.3661897191308956,
            "step": 35720
        },
        {
            "loss": 0.3667,
            "grad_norm": 1.398059606552124,
            "learning_rate": 1.0541423776717895e-05,
            "epoch": 2.3675145733969263,
            "step": 35740
        },
        {
            "loss": 0.4148,
            "grad_norm": 1.2002729177474976,
            "learning_rate": 1.0519342872284049e-05,
            "epoch": 2.368839427662957,
            "step": 35760
        },
        {
            "loss": 0.3253,
            "grad_norm": 1.2973419427871704,
            "learning_rate": 1.0497261967850203e-05,
            "epoch": 2.3701642819289876,
            "step": 35780
        },
        {
            "loss": 0.3714,
            "grad_norm": 1.2639802694320679,
            "learning_rate": 1.0475181063416358e-05,
            "epoch": 2.3714891361950183,
            "step": 35800
        },
        {
            "loss": 0.4291,
            "grad_norm": 1.3492573499679565,
            "learning_rate": 1.0453100158982512e-05,
            "epoch": 2.3728139904610495,
            "step": 35820
        },
        {
            "loss": 0.4207,
            "grad_norm": 3.176668405532837,
            "learning_rate": 1.0431019254548667e-05,
            "epoch": 2.37413884472708,
            "step": 35840
        },
        {
            "loss": 0.3331,
            "grad_norm": 1.134444236755371,
            "learning_rate": 1.0408938350114821e-05,
            "epoch": 2.375463698993111,
            "step": 35860
        },
        {
            "loss": 0.3645,
            "grad_norm": 1.9766446352005005,
            "learning_rate": 1.0386857445680976e-05,
            "epoch": 2.3767885532591415,
            "step": 35880
        },
        {
            "loss": 0.3611,
            "grad_norm": 2.210064172744751,
            "learning_rate": 1.036477654124713e-05,
            "epoch": 2.378113407525172,
            "step": 35900
        },
        {
            "loss": 0.4084,
            "grad_norm": 2.6451821327209473,
            "learning_rate": 1.0342695636813285e-05,
            "epoch": 2.379438261791203,
            "step": 35920
        },
        {
            "loss": 0.4201,
            "grad_norm": 2.7505714893341064,
            "learning_rate": 1.0320614732379439e-05,
            "epoch": 2.3807631160572336,
            "step": 35940
        },
        {
            "loss": 0.3285,
            "grad_norm": 3.773003578186035,
            "learning_rate": 1.0298533827945593e-05,
            "epoch": 2.3820879703232642,
            "step": 35960
        },
        {
            "loss": 0.4186,
            "grad_norm": 2.308556079864502,
            "learning_rate": 1.0276452923511748e-05,
            "epoch": 2.3834128245892954,
            "step": 35980
        },
        {
            "loss": 0.4007,
            "grad_norm": 2.3677332401275635,
            "learning_rate": 1.02543720190779e-05,
            "epoch": 2.384737678855326,
            "step": 36000
        },
        {
            "loss": 0.3546,
            "grad_norm": 0.7065668702125549,
            "learning_rate": 1.0232291114644057e-05,
            "epoch": 2.3860625331213567,
            "step": 36020
        },
        {
            "loss": 0.3778,
            "grad_norm": 0.9698294401168823,
            "learning_rate": 1.0210210210210211e-05,
            "epoch": 2.3873873873873874,
            "step": 36040
        },
        {
            "loss": 0.3392,
            "grad_norm": 0.8694296479225159,
            "learning_rate": 1.0188129305776366e-05,
            "epoch": 2.388712241653418,
            "step": 36060
        },
        {
            "loss": 0.3393,
            "grad_norm": 1.9381986856460571,
            "learning_rate": 1.0166048401342518e-05,
            "epoch": 2.390037095919449,
            "step": 36080
        },
        {
            "loss": 0.365,
            "grad_norm": 2.9650278091430664,
            "learning_rate": 1.0143967496908675e-05,
            "epoch": 2.3913619501854795,
            "step": 36100
        },
        {
            "loss": 0.3485,
            "grad_norm": 1.7652912139892578,
            "learning_rate": 1.0121886592474829e-05,
            "epoch": 2.39268680445151,
            "step": 36120
        },
        {
            "loss": 0.4095,
            "grad_norm": 0.6435678601264954,
            "learning_rate": 1.0099805688040982e-05,
            "epoch": 2.394011658717541,
            "step": 36140
        },
        {
            "loss": 0.3498,
            "grad_norm": 0.708359956741333,
            "learning_rate": 1.0077724783607136e-05,
            "epoch": 2.395336512983572,
            "step": 36160
        },
        {
            "loss": 0.3329,
            "grad_norm": 1.6065303087234497,
            "learning_rate": 1.0055643879173292e-05,
            "epoch": 2.3966613672496027,
            "step": 36180
        },
        {
            "loss": 0.3745,
            "grad_norm": 3.8649168014526367,
            "learning_rate": 1.0033562974739445e-05,
            "epoch": 2.3979862215156333,
            "step": 36200
        },
        {
            "loss": 0.3761,
            "grad_norm": 3.8323118686676025,
            "learning_rate": 1.00114820703056e-05,
            "epoch": 2.399311075781664,
            "step": 36220
        },
        {
            "loss": 0.3522,
            "grad_norm": 1.1076911687850952,
            "learning_rate": 9.989401165871756e-06,
            "epoch": 2.4006359300476947,
            "step": 36240
        },
        {
            "loss": 0.4058,
            "grad_norm": 2.729341506958008,
            "learning_rate": 9.96732026143791e-06,
            "epoch": 2.4019607843137254,
            "step": 36260
        },
        {
            "loss": 0.3919,
            "grad_norm": 0.7776261568069458,
            "learning_rate": 9.945239357004063e-06,
            "epoch": 2.403285638579756,
            "step": 36280
        },
        {
            "loss": 0.372,
            "grad_norm": 1.3694499731063843,
            "learning_rate": 9.923158452570217e-06,
            "epoch": 2.404610492845787,
            "step": 36300
        },
        {
            "loss": 0.3991,
            "grad_norm": 1.3017239570617676,
            "learning_rate": 9.901077548136373e-06,
            "epoch": 2.405935347111818,
            "step": 36320
        },
        {
            "loss": 0.4601,
            "grad_norm": 1.1353379487991333,
            "learning_rate": 9.878996643702526e-06,
            "epoch": 2.4072602013778486,
            "step": 36340
        },
        {
            "loss": 0.3691,
            "grad_norm": 1.2198686599731445,
            "learning_rate": 9.85691573926868e-06,
            "epoch": 2.4085850556438793,
            "step": 36360
        },
        {
            "loss": 0.3629,
            "grad_norm": 0.025040673092007637,
            "learning_rate": 9.834834834834835e-06,
            "epoch": 2.40990990990991,
            "step": 36380
        },
        {
            "loss": 0.4512,
            "grad_norm": 2.9794297218322754,
            "learning_rate": 9.81275393040099e-06,
            "epoch": 2.4112347641759406,
            "step": 36400
        },
        {
            "loss": 0.3871,
            "grad_norm": 1.0352087020874023,
            "learning_rate": 9.790673025967144e-06,
            "epoch": 2.4125596184419713,
            "step": 36420
        },
        {
            "loss": 0.3769,
            "grad_norm": 1.7613364458084106,
            "learning_rate": 9.768592121533298e-06,
            "epoch": 2.413884472708002,
            "step": 36440
        },
        {
            "loss": 0.3665,
            "grad_norm": 1.1679432392120361,
            "learning_rate": 9.746511217099453e-06,
            "epoch": 2.4152093269740327,
            "step": 36460
        },
        {
            "loss": 0.3666,
            "grad_norm": 0.9157164692878723,
            "learning_rate": 9.724430312665607e-06,
            "epoch": 2.4165341812400634,
            "step": 36480
        },
        {
            "loss": 0.3688,
            "grad_norm": 1.947416067123413,
            "learning_rate": 9.702349408231762e-06,
            "epoch": 2.4178590355060945,
            "step": 36500
        },
        {
            "loss": 0.3177,
            "grad_norm": 0.9433644413948059,
            "learning_rate": 9.680268503797916e-06,
            "epoch": 2.419183889772125,
            "step": 36520
        },
        {
            "loss": 0.3736,
            "grad_norm": 2.3151910305023193,
            "learning_rate": 9.65818759936407e-06,
            "epoch": 2.420508744038156,
            "step": 36540
        },
        {
            "loss": 0.3873,
            "grad_norm": 3.167387008666992,
            "learning_rate": 9.636106694930225e-06,
            "epoch": 2.4218335983041865,
            "step": 36560
        },
        {
            "loss": 0.4271,
            "grad_norm": 0.4886067509651184,
            "learning_rate": 9.61402579049638e-06,
            "epoch": 2.4231584525702172,
            "step": 36580
        },
        {
            "loss": 0.3708,
            "grad_norm": 0.6631662845611572,
            "learning_rate": 9.591944886062532e-06,
            "epoch": 2.424483306836248,
            "step": 36600
        },
        {
            "loss": 0.3176,
            "grad_norm": 1.8867462873458862,
            "learning_rate": 9.569863981628688e-06,
            "epoch": 2.4258081611022786,
            "step": 36620
        },
        {
            "loss": 0.3778,
            "grad_norm": 1.4396816492080688,
            "learning_rate": 9.547783077194843e-06,
            "epoch": 2.4271330153683097,
            "step": 36640
        },
        {
            "loss": 0.453,
            "grad_norm": 12.683476448059082,
            "learning_rate": 9.525702172760997e-06,
            "epoch": 2.4284578696343404,
            "step": 36660
        },
        {
            "loss": 0.366,
            "grad_norm": 3.075042963027954,
            "learning_rate": 9.503621268327152e-06,
            "epoch": 2.429782723900371,
            "step": 36680
        },
        {
            "loss": 0.3507,
            "grad_norm": 1.065619945526123,
            "learning_rate": 9.481540363893306e-06,
            "epoch": 2.4311075781664018,
            "step": 36700
        },
        {
            "loss": 0.3953,
            "grad_norm": 1.9947222471237183,
            "learning_rate": 9.45945945945946e-06,
            "epoch": 2.4324324324324325,
            "step": 36720
        },
        {
            "loss": 0.4208,
            "grad_norm": 1.2094407081604004,
            "learning_rate": 9.437378555025613e-06,
            "epoch": 2.433757286698463,
            "step": 36740
        },
        {
            "loss": 0.3966,
            "grad_norm": 0.7090889811515808,
            "learning_rate": 9.41529765059177e-06,
            "epoch": 2.435082140964494,
            "step": 36760
        },
        {
            "loss": 0.334,
            "grad_norm": 2.174414873123169,
            "learning_rate": 9.393216746157924e-06,
            "epoch": 2.4364069952305245,
            "step": 36780
        },
        {
            "loss": 0.3421,
            "grad_norm": 2.911857843399048,
            "learning_rate": 9.371135841724077e-06,
            "epoch": 2.437731849496555,
            "step": 36800
        },
        {
            "loss": 0.4387,
            "grad_norm": 0.6488746404647827,
            "learning_rate": 9.349054937290231e-06,
            "epoch": 2.439056703762586,
            "step": 36820
        },
        {
            "loss": 0.358,
            "grad_norm": 2.4724223613739014,
            "learning_rate": 9.326974032856387e-06,
            "epoch": 2.440381558028617,
            "step": 36840
        },
        {
            "loss": 0.3999,
            "grad_norm": 3.6518266201019287,
            "learning_rate": 9.304893128422542e-06,
            "epoch": 2.4417064122946477,
            "step": 36860
        },
        {
            "loss": 0.3701,
            "grad_norm": 0.669918417930603,
            "learning_rate": 9.282812223988694e-06,
            "epoch": 2.4430312665606784,
            "step": 36880
        },
        {
            "loss": 0.4674,
            "grad_norm": 3.4599263668060303,
            "learning_rate": 9.260731319554849e-06,
            "epoch": 2.444356120826709,
            "step": 36900
        },
        {
            "loss": 0.4086,
            "grad_norm": 1.0341988801956177,
            "learning_rate": 9.238650415121005e-06,
            "epoch": 2.4456809750927397,
            "step": 36920
        },
        {
            "loss": 0.4267,
            "grad_norm": 0.8332504630088806,
            "learning_rate": 9.216569510687158e-06,
            "epoch": 2.4470058293587704,
            "step": 36940
        },
        {
            "loss": 0.3286,
            "grad_norm": 1.0718119144439697,
            "learning_rate": 9.194488606253312e-06,
            "epoch": 2.448330683624801,
            "step": 36960
        },
        {
            "loss": 0.3866,
            "grad_norm": 0.8983803391456604,
            "learning_rate": 9.172407701819467e-06,
            "epoch": 2.4496555378908322,
            "step": 36980
        },
        {
            "loss": 0.3416,
            "grad_norm": 1.4645572900772095,
            "learning_rate": 9.150326797385621e-06,
            "epoch": 2.450980392156863,
            "step": 37000
        },
        {
            "loss": 0.4445,
            "grad_norm": 1.1672322750091553,
            "learning_rate": 9.128245892951775e-06,
            "epoch": 2.4523052464228936,
            "step": 37020
        },
        {
            "loss": 0.396,
            "grad_norm": 1.8198376893997192,
            "learning_rate": 9.10616498851793e-06,
            "epoch": 2.4536301006889243,
            "step": 37040
        },
        {
            "loss": 0.3921,
            "grad_norm": 2.3703761100769043,
            "learning_rate": 9.084084084084084e-06,
            "epoch": 2.454954954954955,
            "step": 37060
        },
        {
            "loss": 0.3726,
            "grad_norm": 1.2294366359710693,
            "learning_rate": 9.062003179650239e-06,
            "epoch": 2.4562798092209857,
            "step": 37080
        },
        {
            "loss": 0.371,
            "grad_norm": 1.2106988430023193,
            "learning_rate": 9.039922275216393e-06,
            "epoch": 2.4576046634870163,
            "step": 37100
        },
        {
            "loss": 0.394,
            "grad_norm": 0.7191904783248901,
            "learning_rate": 9.017841370782548e-06,
            "epoch": 2.458929517753047,
            "step": 37120
        },
        {
            "loss": 0.3595,
            "grad_norm": 1.1011683940887451,
            "learning_rate": 8.995760466348702e-06,
            "epoch": 2.4602543720190777,
            "step": 37140
        },
        {
            "loss": 0.3707,
            "grad_norm": 1.1844663619995117,
            "learning_rate": 8.973679561914857e-06,
            "epoch": 2.4615792262851084,
            "step": 37160
        },
        {
            "loss": 0.3766,
            "grad_norm": 1.070637822151184,
            "learning_rate": 8.951598657481011e-06,
            "epoch": 2.4629040805511395,
            "step": 37180
        },
        {
            "loss": 0.3532,
            "grad_norm": 1.314764380455017,
            "learning_rate": 8.929517753047165e-06,
            "epoch": 2.46422893481717,
            "step": 37200
        },
        {
            "loss": 0.3489,
            "grad_norm": 66.44425964355469,
            "learning_rate": 8.90743684861332e-06,
            "epoch": 2.465553789083201,
            "step": 37220
        },
        {
            "loss": 0.336,
            "grad_norm": 2.116576910018921,
            "learning_rate": 8.885355944179474e-06,
            "epoch": 2.4668786433492316,
            "step": 37240
        },
        {
            "loss": 0.38,
            "grad_norm": 2.0546398162841797,
            "learning_rate": 8.863275039745629e-06,
            "epoch": 2.4682034976152623,
            "step": 37260
        },
        {
            "loss": 0.397,
            "grad_norm": 1.223075032234192,
            "learning_rate": 8.841194135311783e-06,
            "epoch": 2.469528351881293,
            "step": 37280
        },
        {
            "loss": 0.376,
            "grad_norm": 1.088605523109436,
            "learning_rate": 8.819113230877938e-06,
            "epoch": 2.4708532061473236,
            "step": 37300
        },
        {
            "loss": 0.3554,
            "grad_norm": 1.176992654800415,
            "learning_rate": 8.797032326444092e-06,
            "epoch": 2.4721780604133547,
            "step": 37320
        },
        {
            "loss": 0.3735,
            "grad_norm": 1.0576512813568115,
            "learning_rate": 8.774951422010245e-06,
            "epoch": 2.4735029146793854,
            "step": 37340
        },
        {
            "loss": 0.4329,
            "grad_norm": 1.1698837280273438,
            "learning_rate": 8.752870517576401e-06,
            "epoch": 2.474827768945416,
            "step": 37360
        },
        {
            "loss": 0.3569,
            "grad_norm": 1.8420945405960083,
            "learning_rate": 8.730789613142555e-06,
            "epoch": 2.476152623211447,
            "step": 37380
        },
        {
            "loss": 0.3991,
            "grad_norm": 1.0158876180648804,
            "learning_rate": 8.708708708708708e-06,
            "epoch": 2.4774774774774775,
            "step": 37400
        },
        {
            "loss": 0.3499,
            "grad_norm": 1.9508942365646362,
            "learning_rate": 8.686627804274863e-06,
            "epoch": 2.478802331743508,
            "step": 37420
        },
        {
            "loss": 0.357,
            "grad_norm": 1.910879373550415,
            "learning_rate": 8.664546899841019e-06,
            "epoch": 2.480127186009539,
            "step": 37440
        },
        {
            "loss": 0.3717,
            "grad_norm": 0.780805230140686,
            "learning_rate": 8.642465995407173e-06,
            "epoch": 2.4814520402755695,
            "step": 37460
        },
        {
            "loss": 0.3407,
            "grad_norm": 1.8622909784317017,
            "learning_rate": 8.620385090973326e-06,
            "epoch": 2.4827768945416,
            "step": 37480
        },
        {
            "loss": 0.4094,
            "grad_norm": 4.007370471954346,
            "learning_rate": 8.59830418653948e-06,
            "epoch": 2.4841017488076313,
            "step": 37500
        },
        {
            "loss": 0.3636,
            "grad_norm": 3.759223699569702,
            "learning_rate": 8.576223282105637e-06,
            "epoch": 2.485426603073662,
            "step": 37520
        },
        {
            "loss": 0.3579,
            "grad_norm": 1.1666311025619507,
            "learning_rate": 8.55414237767179e-06,
            "epoch": 2.4867514573396927,
            "step": 37540
        },
        {
            "loss": 0.3453,
            "grad_norm": 2.861325740814209,
            "learning_rate": 8.532061473237944e-06,
            "epoch": 2.4880763116057234,
            "step": 37560
        },
        {
            "loss": 0.3477,
            "grad_norm": 2.7005152702331543,
            "learning_rate": 8.509980568804098e-06,
            "epoch": 2.489401165871754,
            "step": 37580
        },
        {
            "loss": 0.3795,
            "grad_norm": 1.2813199758529663,
            "learning_rate": 8.487899664370253e-06,
            "epoch": 2.4907260201377848,
            "step": 37600
        },
        {
            "loss": 0.3804,
            "grad_norm": 1.1055186986923218,
            "learning_rate": 8.465818759936407e-06,
            "epoch": 2.4920508744038155,
            "step": 37620
        },
        {
            "loss": 0.4199,
            "grad_norm": 3.367125988006592,
            "learning_rate": 8.443737855502562e-06,
            "epoch": 2.493375728669846,
            "step": 37640
        },
        {
            "loss": 0.3235,
            "grad_norm": 0.8648329973220825,
            "learning_rate": 8.421656951068718e-06,
            "epoch": 2.4947005829358773,
            "step": 37660
        },
        {
            "loss": 0.408,
            "grad_norm": 0.49499887228012085,
            "learning_rate": 8.39957604663487e-06,
            "epoch": 2.496025437201908,
            "step": 37680
        },
        {
            "loss": 0.3706,
            "grad_norm": 1.1480571031570435,
            "learning_rate": 8.377495142201025e-06,
            "epoch": 2.4973502914679386,
            "step": 37700
        },
        {
            "loss": 0.3546,
            "grad_norm": 0.6895565390586853,
            "learning_rate": 8.35541423776718e-06,
            "epoch": 2.4986751457339693,
            "step": 37720
        },
        {
            "loss": 0.386,
            "grad_norm": 1.067759394645691,
            "learning_rate": 8.333333333333334e-06,
            "epoch": 2.5,
            "step": 37740
        },
        {
            "loss": 0.3757,
            "grad_norm": 3.9506936073303223,
            "learning_rate": 8.311252428899488e-06,
            "epoch": 2.5013248542660307,
            "step": 37760
        },
        {
            "loss": 0.4175,
            "grad_norm": 22.122356414794922,
            "learning_rate": 8.289171524465643e-06,
            "epoch": 2.5026497085320614,
            "step": 37780
        },
        {
            "loss": 0.4196,
            "grad_norm": 2.143998146057129,
            "learning_rate": 8.267090620031797e-06,
            "epoch": 2.503974562798092,
            "step": 37800
        },
        {
            "loss": 0.3488,
            "grad_norm": 1.1198081970214844,
            "learning_rate": 8.245009715597952e-06,
            "epoch": 2.5052994170641227,
            "step": 37820
        },
        {
            "loss": 0.3915,
            "grad_norm": 1.671466588973999,
            "learning_rate": 8.222928811164106e-06,
            "epoch": 2.5066242713301534,
            "step": 37840
        },
        {
            "loss": 0.3965,
            "grad_norm": 0.767859160900116,
            "learning_rate": 8.20084790673026e-06,
            "epoch": 2.5079491255961845,
            "step": 37860
        },
        {
            "loss": 0.2967,
            "grad_norm": 2.935131549835205,
            "learning_rate": 8.178767002296415e-06,
            "epoch": 2.5092739798622152,
            "step": 37880
        },
        {
            "loss": 0.3629,
            "grad_norm": 2.0998761653900146,
            "learning_rate": 8.15668609786257e-06,
            "epoch": 2.510598834128246,
            "step": 37900
        },
        {
            "loss": 0.3362,
            "grad_norm": 2.1165482997894287,
            "learning_rate": 8.134605193428724e-06,
            "epoch": 2.5119236883942766,
            "step": 37920
        },
        {
            "loss": 0.3435,
            "grad_norm": 0.6878560781478882,
            "learning_rate": 8.112524288994876e-06,
            "epoch": 2.5132485426603073,
            "step": 37940
        },
        {
            "loss": 0.4342,
            "grad_norm": 1.1554640531539917,
            "learning_rate": 8.090443384561033e-06,
            "epoch": 2.514573396926338,
            "step": 37960
        },
        {
            "loss": 0.414,
            "grad_norm": 1.8979148864746094,
            "learning_rate": 8.068362480127187e-06,
            "epoch": 2.515898251192369,
            "step": 37980
        },
        {
            "loss": 0.3868,
            "grad_norm": 1.9525610208511353,
            "learning_rate": 8.04628157569334e-06,
            "epoch": 2.5172231054584,
            "step": 38000
        },
        {
            "loss": 0.3931,
            "grad_norm": 1.9914650917053223,
            "learning_rate": 8.024200671259494e-06,
            "epoch": 2.5185479597244305,
            "step": 38020
        },
        {
            "loss": 0.3593,
            "grad_norm": 3.288703680038452,
            "learning_rate": 8.00211976682565e-06,
            "epoch": 2.519872813990461,
            "step": 38040
        },
        {
            "loss": 0.3692,
            "grad_norm": 1.5779719352722168,
            "learning_rate": 7.980038862391805e-06,
            "epoch": 2.521197668256492,
            "step": 38060
        },
        {
            "loss": 0.3626,
            "grad_norm": 1.9867857694625854,
            "learning_rate": 7.957957957957958e-06,
            "epoch": 2.5225225225225225,
            "step": 38080
        },
        {
            "loss": 0.3816,
            "grad_norm": 1.3191027641296387,
            "learning_rate": 7.935877053524112e-06,
            "epoch": 2.523847376788553,
            "step": 38100
        },
        {
            "loss": 0.3832,
            "grad_norm": 1.8401981592178345,
            "learning_rate": 7.913796149090268e-06,
            "epoch": 2.525172231054584,
            "step": 38120
        },
        {
            "loss": 0.4298,
            "grad_norm": 1.7950009107589722,
            "learning_rate": 7.891715244656421e-06,
            "epoch": 2.5264970853206146,
            "step": 38140
        },
        {
            "loss": 0.4406,
            "grad_norm": 1.3559616804122925,
            "learning_rate": 7.869634340222575e-06,
            "epoch": 2.5278219395866453,
            "step": 38160
        },
        {
            "loss": 0.3275,
            "grad_norm": 1.1965429782867432,
            "learning_rate": 7.847553435788731e-06,
            "epoch": 2.529146793852676,
            "step": 38180
        },
        {
            "loss": 0.3843,
            "grad_norm": 1.5116554498672485,
            "learning_rate": 7.825472531354884e-06,
            "epoch": 2.530471648118707,
            "step": 38200
        },
        {
            "loss": 0.32,
            "grad_norm": 1.183367371559143,
            "learning_rate": 7.803391626921039e-06,
            "epoch": 2.5317965023847377,
            "step": 38220
        },
        {
            "loss": 0.3573,
            "grad_norm": 2.7256906032562256,
            "learning_rate": 7.781310722487193e-06,
            "epoch": 2.5331213566507684,
            "step": 38240
        },
        {
            "loss": 0.4022,
            "grad_norm": 4.190271854400635,
            "learning_rate": 7.75922981805335e-06,
            "epoch": 2.534446210916799,
            "step": 38260
        },
        {
            "loss": 0.3537,
            "grad_norm": 0.8091658353805542,
            "learning_rate": 7.737148913619502e-06,
            "epoch": 2.53577106518283,
            "step": 38280
        },
        {
            "loss": 0.3443,
            "grad_norm": 2.2125182151794434,
            "learning_rate": 7.715068009185656e-06,
            "epoch": 2.5370959194488605,
            "step": 38300
        },
        {
            "loss": 0.3885,
            "grad_norm": 2.0129730701446533,
            "learning_rate": 7.692987104751811e-06,
            "epoch": 2.5384207737148916,
            "step": 38320
        },
        {
            "loss": 0.3967,
            "grad_norm": 2.843324899673462,
            "learning_rate": 7.670906200317965e-06,
            "epoch": 2.5397456279809223,
            "step": 38340
        },
        {
            "loss": 0.4431,
            "grad_norm": 0.8684397339820862,
            "learning_rate": 7.64882529588412e-06,
            "epoch": 2.541070482246953,
            "step": 38360
        },
        {
            "loss": 0.3252,
            "grad_norm": 0.9517287611961365,
            "learning_rate": 7.626744391450274e-06,
            "epoch": 2.5423953365129837,
            "step": 38380
        },
        {
            "loss": 0.3656,
            "grad_norm": 1.1270941495895386,
            "learning_rate": 7.604663487016428e-06,
            "epoch": 2.5437201907790143,
            "step": 38400
        },
        {
            "loss": 0.3695,
            "grad_norm": 0.6991212368011475,
            "learning_rate": 7.582582582582583e-06,
            "epoch": 2.545045045045045,
            "step": 38420
        },
        {
            "loss": 0.3652,
            "grad_norm": 1.3159408569335938,
            "learning_rate": 7.5605016781487375e-06,
            "epoch": 2.5463698993110757,
            "step": 38440
        },
        {
            "loss": 0.4094,
            "grad_norm": 0.8422456979751587,
            "learning_rate": 7.538420773714892e-06,
            "epoch": 2.5476947535771064,
            "step": 38460
        },
        {
            "loss": 0.3154,
            "grad_norm": 1.7597512006759644,
            "learning_rate": 7.5163398692810456e-06,
            "epoch": 2.549019607843137,
            "step": 38480
        },
        {
            "loss": 0.4023,
            "grad_norm": 6.269291877746582,
            "learning_rate": 7.494258964847201e-06,
            "epoch": 2.5503444621091678,
            "step": 38500
        },
        {
            "loss": 0.3188,
            "grad_norm": 2.017737627029419,
            "learning_rate": 7.472178060413355e-06,
            "epoch": 2.551669316375199,
            "step": 38520
        },
        {
            "loss": 0.364,
            "grad_norm": 0.7355696558952332,
            "learning_rate": 7.450097155979509e-06,
            "epoch": 2.5529941706412296,
            "step": 38540
        },
        {
            "loss": 0.3582,
            "grad_norm": 1.0534576177597046,
            "learning_rate": 7.428016251545663e-06,
            "epoch": 2.5543190249072603,
            "step": 38560
        },
        {
            "loss": 0.4191,
            "grad_norm": 2.075516939163208,
            "learning_rate": 7.405935347111819e-06,
            "epoch": 2.555643879173291,
            "step": 38580
        },
        {
            "loss": 0.4133,
            "grad_norm": 3.2839715480804443,
            "learning_rate": 7.383854442677972e-06,
            "epoch": 2.5569687334393216,
            "step": 38600
        },
        {
            "loss": 0.3566,
            "grad_norm": 2.0592806339263916,
            "learning_rate": 7.361773538244127e-06,
            "epoch": 2.5582935877053523,
            "step": 38620
        },
        {
            "loss": 0.3575,
            "grad_norm": 1.1529873609542847,
            "learning_rate": 7.339692633810281e-06,
            "epoch": 2.559618441971383,
            "step": 38640
        },
        {
            "loss": 0.3616,
            "grad_norm": 4.242574691772461,
            "learning_rate": 7.317611729376436e-06,
            "epoch": 2.560943296237414,
            "step": 38660
        },
        {
            "loss": 0.3682,
            "grad_norm": 2.064316511154175,
            "learning_rate": 7.29553082494259e-06,
            "epoch": 2.562268150503445,
            "step": 38680
        },
        {
            "loss": 0.4655,
            "grad_norm": 0.9615970253944397,
            "learning_rate": 7.2734499205087444e-06,
            "epoch": 2.5635930047694755,
            "step": 38700
        },
        {
            "loss": 0.3636,
            "grad_norm": 0.7385151982307434,
            "learning_rate": 7.251369016074899e-06,
            "epoch": 2.564917859035506,
            "step": 38720
        },
        {
            "loss": 0.4027,
            "grad_norm": 1.085775375366211,
            "learning_rate": 7.2292881116410525e-06,
            "epoch": 2.566242713301537,
            "step": 38740
        },
        {
            "loss": 0.3825,
            "grad_norm": 2.298788070678711,
            "learning_rate": 7.207207207207208e-06,
            "epoch": 2.5675675675675675,
            "step": 38760
        },
        {
            "loss": 0.3595,
            "grad_norm": 1.016255497932434,
            "learning_rate": 7.185126302773362e-06,
            "epoch": 2.5688924218335982,
            "step": 38780
        },
        {
            "loss": 0.3721,
            "grad_norm": 0.7643128037452698,
            "learning_rate": 7.163045398339516e-06,
            "epoch": 2.570217276099629,
            "step": 38800
        },
        {
            "loss": 0.4192,
            "grad_norm": 2.1835076808929443,
            "learning_rate": 7.14096449390567e-06,
            "epoch": 2.5715421303656596,
            "step": 38820
        },
        {
            "loss": 0.3181,
            "grad_norm": 0.9056530594825745,
            "learning_rate": 7.1188835894718255e-06,
            "epoch": 2.5728669846316903,
            "step": 38840
        },
        {
            "loss": 0.3633,
            "grad_norm": 0.6779099106788635,
            "learning_rate": 7.09680268503798e-06,
            "epoch": 2.5741918388977214,
            "step": 38860
        },
        {
            "loss": 0.3883,
            "grad_norm": 1.2110213041305542,
            "learning_rate": 7.0747217806041336e-06,
            "epoch": 2.575516693163752,
            "step": 38880
        },
        {
            "loss": 0.3433,
            "grad_norm": 1.2429624795913696,
            "learning_rate": 7.052640876170288e-06,
            "epoch": 2.5768415474297828,
            "step": 38900
        },
        {
            "loss": 0.4117,
            "grad_norm": 3.0507020950317383,
            "learning_rate": 7.030559971736443e-06,
            "epoch": 2.5781664016958135,
            "step": 38920
        },
        {
            "loss": 0.3809,
            "grad_norm": 1.2378252744674683,
            "learning_rate": 7.008479067302597e-06,
            "epoch": 2.579491255961844,
            "step": 38940
        },
        {
            "loss": 0.3512,
            "grad_norm": 3.0137040615081787,
            "learning_rate": 6.986398162868751e-06,
            "epoch": 2.580816110227875,
            "step": 38960
        },
        {
            "loss": 0.3971,
            "grad_norm": 1.2964211702346802,
            "learning_rate": 6.964317258434906e-06,
            "epoch": 2.5821409644939055,
            "step": 38980
        },
        {
            "loss": 0.4097,
            "grad_norm": 3.840913772583008,
            "learning_rate": 6.942236354001059e-06,
            "epoch": 2.5834658187599366,
            "step": 39000
        },
        {
            "loss": 0.3777,
            "grad_norm": 1.1706048250198364,
            "learning_rate": 6.920155449567215e-06,
            "epoch": 2.5847906730259673,
            "step": 39020
        },
        {
            "loss": 0.4512,
            "grad_norm": 0.8227631449699402,
            "learning_rate": 6.898074545133369e-06,
            "epoch": 2.586115527291998,
            "step": 39040
        },
        {
            "loss": 0.4228,
            "grad_norm": 5.116398334503174,
            "learning_rate": 6.875993640699524e-06,
            "epoch": 2.5874403815580287,
            "step": 39060
        },
        {
            "loss": 0.3737,
            "grad_norm": 1.3199787139892578,
            "learning_rate": 6.853912736265677e-06,
            "epoch": 2.5887652358240594,
            "step": 39080
        },
        {
            "loss": 0.3448,
            "grad_norm": 0.7885183095932007,
            "learning_rate": 6.8318318318318324e-06,
            "epoch": 2.59009009009009,
            "step": 39100
        },
        {
            "loss": 0.391,
            "grad_norm": 3.0002267360687256,
            "learning_rate": 6.809750927397987e-06,
            "epoch": 2.5914149443561207,
            "step": 39120
        },
        {
            "loss": 0.333,
            "grad_norm": 1.2867088317871094,
            "learning_rate": 6.7876700229641405e-06,
            "epoch": 2.5927397986221514,
            "step": 39140
        },
        {
            "loss": 0.3532,
            "grad_norm": 1.721226453781128,
            "learning_rate": 6.765589118530295e-06,
            "epoch": 2.594064652888182,
            "step": 39160
        },
        {
            "loss": 0.3235,
            "grad_norm": 1.2139699459075928,
            "learning_rate": 6.74350821409645e-06,
            "epoch": 2.595389507154213,
            "step": 39180
        },
        {
            "loss": 0.3576,
            "grad_norm": 0.8690405488014221,
            "learning_rate": 6.721427309662604e-06,
            "epoch": 2.596714361420244,
            "step": 39200
        },
        {
            "loss": 0.3961,
            "grad_norm": 1.8979681730270386,
            "learning_rate": 6.699346405228758e-06,
            "epoch": 2.5980392156862746,
            "step": 39220
        },
        {
            "loss": 0.3768,
            "grad_norm": 1.0378636121749878,
            "learning_rate": 6.677265500794913e-06,
            "epoch": 2.5993640699523053,
            "step": 39240
        },
        {
            "loss": 0.3243,
            "grad_norm": 1.1783041954040527,
            "learning_rate": 6.655184596361068e-06,
            "epoch": 2.600688924218336,
            "step": 39260
        },
        {
            "loss": 0.3514,
            "grad_norm": 2.8787214756011963,
            "learning_rate": 6.6331036919272216e-06,
            "epoch": 2.6020137784843667,
            "step": 39280
        },
        {
            "loss": 0.3543,
            "grad_norm": 1.7919155359268188,
            "learning_rate": 6.611022787493376e-06,
            "epoch": 2.6033386327503973,
            "step": 39300
        },
        {
            "loss": 0.3488,
            "grad_norm": 3.563159942626953,
            "learning_rate": 6.588941883059531e-06,
            "epoch": 2.6046634870164285,
            "step": 39320
        },
        {
            "loss": 0.3302,
            "grad_norm": 0.7071315050125122,
            "learning_rate": 6.566860978625684e-06,
            "epoch": 2.605988341282459,
            "step": 39340
        },
        {
            "loss": 0.4086,
            "grad_norm": 3.009398937225342,
            "learning_rate": 6.544780074191839e-06,
            "epoch": 2.60731319554849,
            "step": 39360
        },
        {
            "loss": 0.3789,
            "grad_norm": 4.023491382598877,
            "learning_rate": 6.522699169757994e-06,
            "epoch": 2.6086380498145205,
            "step": 39380
        },
        {
            "loss": 0.3363,
            "grad_norm": 4.735151290893555,
            "learning_rate": 6.500618265324147e-06,
            "epoch": 2.609962904080551,
            "step": 39400
        },
        {
            "loss": 0.3682,
            "grad_norm": 1.9859893321990967,
            "learning_rate": 6.478537360890302e-06,
            "epoch": 2.611287758346582,
            "step": 39420
        },
        {
            "loss": 0.3573,
            "grad_norm": 0.5648049712181091,
            "learning_rate": 6.456456456456457e-06,
            "epoch": 2.6126126126126126,
            "step": 39440
        },
        {
            "loss": 0.3868,
            "grad_norm": 23.64848518371582,
            "learning_rate": 6.4343755520226116e-06,
            "epoch": 2.6139374668786433,
            "step": 39460
        },
        {
            "loss": 0.439,
            "grad_norm": 1.5351885557174683,
            "learning_rate": 6.412294647588765e-06,
            "epoch": 2.615262321144674,
            "step": 39480
        },
        {
            "loss": 0.4268,
            "grad_norm": 1.9388947486877441,
            "learning_rate": 6.39021374315492e-06,
            "epoch": 2.6165871754107046,
            "step": 39500
        },
        {
            "loss": 0.3862,
            "grad_norm": 2.8082656860351562,
            "learning_rate": 6.368132838721075e-06,
            "epoch": 2.6179120296767353,
            "step": 39520
        },
        {
            "loss": 0.4299,
            "grad_norm": 1.213794469833374,
            "learning_rate": 6.3460519342872285e-06,
            "epoch": 2.6192368839427664,
            "step": 39540
        },
        {
            "loss": 0.3259,
            "grad_norm": 0.9918519854545593,
            "learning_rate": 6.323971029853383e-06,
            "epoch": 2.620561738208797,
            "step": 39560
        },
        {
            "loss": 0.4027,
            "grad_norm": 1.1821234226226807,
            "learning_rate": 6.301890125419538e-06,
            "epoch": 2.621886592474828,
            "step": 39580
        },
        {
            "loss": 0.4215,
            "grad_norm": 1.4962776899337769,
            "learning_rate": 6.279809220985691e-06,
            "epoch": 2.6232114467408585,
            "step": 39600
        },
        {
            "loss": 0.4596,
            "grad_norm": 2.3303139209747314,
            "learning_rate": 6.257728316551846e-06,
            "epoch": 2.624536301006889,
            "step": 39620
        },
        {
            "loss": 0.393,
            "grad_norm": 1.2149934768676758,
            "learning_rate": 6.235647412118001e-06,
            "epoch": 2.62586115527292,
            "step": 39640
        },
        {
            "loss": 0.4136,
            "grad_norm": 2.0797886848449707,
            "learning_rate": 6.213566507684155e-06,
            "epoch": 2.627186009538951,
            "step": 39660
        },
        {
            "loss": 0.3536,
            "grad_norm": 8.187514305114746,
            "learning_rate": 6.1914856032503096e-06,
            "epoch": 2.6285108638049817,
            "step": 39680
        },
        {
            "loss": 0.4154,
            "grad_norm": 1.308457851409912,
            "learning_rate": 6.169404698816464e-06,
            "epoch": 2.6298357180710124,
            "step": 39700
        },
        {
            "loss": 0.3572,
            "grad_norm": 1.1994976997375488,
            "learning_rate": 6.147323794382618e-06,
            "epoch": 2.631160572337043,
            "step": 39720
        },
        {
            "loss": 0.3945,
            "grad_norm": 1.2597181797027588,
            "learning_rate": 6.125242889948773e-06,
            "epoch": 2.6324854266030737,
            "step": 39740
        },
        {
            "loss": 0.3231,
            "grad_norm": 1.9962172508239746,
            "learning_rate": 6.1031619855149265e-06,
            "epoch": 2.6338102808691044,
            "step": 39760
        },
        {
            "loss": 0.4026,
            "grad_norm": 1.2480028867721558,
            "learning_rate": 6.081081081081082e-06,
            "epoch": 2.635135135135135,
            "step": 39780
        },
        {
            "loss": 0.3758,
            "grad_norm": 2.2699766159057617,
            "learning_rate": 6.059000176647236e-06,
            "epoch": 2.6364599894011658,
            "step": 39800
        },
        {
            "loss": 0.3472,
            "grad_norm": 1.9271001815795898,
            "learning_rate": 6.03691927221339e-06,
            "epoch": 2.6377848436671965,
            "step": 39820
        },
        {
            "loss": 0.3727,
            "grad_norm": 1.6224302053451538,
            "learning_rate": 6.014838367779545e-06,
            "epoch": 2.639109697933227,
            "step": 39840
        },
        {
            "loss": 0.3861,
            "grad_norm": 2.9527292251586914,
            "learning_rate": 5.992757463345699e-06,
            "epoch": 2.640434552199258,
            "step": 39860
        },
        {
            "loss": 0.4028,
            "grad_norm": 0.7010284066200256,
            "learning_rate": 5.970676558911854e-06,
            "epoch": 2.641759406465289,
            "step": 39880
        },
        {
            "loss": 0.4089,
            "grad_norm": 0.7245003581047058,
            "learning_rate": 5.948595654478008e-06,
            "epoch": 2.6430842607313196,
            "step": 39900
        },
        {
            "loss": 0.4408,
            "grad_norm": 1.2599130868911743,
            "learning_rate": 5.926514750044162e-06,
            "epoch": 2.6444091149973503,
            "step": 39920
        },
        {
            "loss": 0.399,
            "grad_norm": 4.215077877044678,
            "learning_rate": 5.9044338456103165e-06,
            "epoch": 2.645733969263381,
            "step": 39940
        },
        {
            "loss": 0.3615,
            "grad_norm": 2.215277910232544,
            "learning_rate": 5.882352941176471e-06,
            "epoch": 2.6470588235294117,
            "step": 39960
        },
        {
            "loss": 0.4427,
            "grad_norm": 2.5324008464813232,
            "learning_rate": 5.860272036742625e-06,
            "epoch": 2.6483836777954424,
            "step": 39980
        },
        {
            "loss": 0.3889,
            "grad_norm": 1.2982332706451416,
            "learning_rate": 5.83819113230878e-06,
            "epoch": 2.6497085320614735,
            "step": 40000
        },
        {
            "loss": 0.3566,
            "grad_norm": 0.9672826528549194,
            "learning_rate": 5.816110227874933e-06,
            "epoch": 2.651033386327504,
            "step": 40020
        },
        {
            "loss": 0.3882,
            "grad_norm": 0.7167983055114746,
            "learning_rate": 5.794029323441089e-06,
            "epoch": 2.652358240593535,
            "step": 40040
        },
        {
            "loss": 0.3583,
            "grad_norm": 2.231595039367676,
            "learning_rate": 5.771948419007243e-06,
            "epoch": 2.6536830948595655,
            "step": 40060
        },
        {
            "loss": 0.3649,
            "grad_norm": 2.209534168243408,
            "learning_rate": 5.7498675145733976e-06,
            "epoch": 2.6550079491255962,
            "step": 40080
        },
        {
            "loss": 0.3802,
            "grad_norm": 2.059382915496826,
            "learning_rate": 5.727786610139552e-06,
            "epoch": 2.656332803391627,
            "step": 40100
        },
        {
            "loss": 0.3705,
            "grad_norm": 1.3088661432266235,
            "learning_rate": 5.705705705705706e-06,
            "epoch": 2.6576576576576576,
            "step": 40120
        },
        {
            "loss": 0.3429,
            "grad_norm": 1.0046517848968506,
            "learning_rate": 5.683624801271861e-06,
            "epoch": 2.6589825119236883,
            "step": 40140
        },
        {
            "loss": 0.3859,
            "grad_norm": 2.8869705200195312,
            "learning_rate": 5.6615438968380145e-06,
            "epoch": 2.660307366189719,
            "step": 40160
        },
        {
            "loss": 0.3794,
            "grad_norm": 2.839679718017578,
            "learning_rate": 5.63946299240417e-06,
            "epoch": 2.6616322204557497,
            "step": 40180
        },
        {
            "loss": 0.3991,
            "grad_norm": 3.6533925533294678,
            "learning_rate": 5.617382087970323e-06,
            "epoch": 2.6629570747217803,
            "step": 40200
        },
        {
            "loss": 0.3757,
            "grad_norm": 1.9932483434677124,
            "learning_rate": 5.595301183536478e-06,
            "epoch": 2.6642819289878115,
            "step": 40220
        },
        {
            "loss": 0.3765,
            "grad_norm": 3.1821539402008057,
            "learning_rate": 5.573220279102632e-06,
            "epoch": 2.665606783253842,
            "step": 40240
        },
        {
            "loss": 0.3688,
            "grad_norm": 1.012242317199707,
            "learning_rate": 5.551139374668787e-06,
            "epoch": 2.666931637519873,
            "step": 40260
        },
        {
            "loss": 0.3501,
            "grad_norm": 3.931391954421997,
            "learning_rate": 5.529058470234941e-06,
            "epoch": 2.6682564917859035,
            "step": 40280
        },
        {
            "loss": 0.3455,
            "grad_norm": 1.9040653705596924,
            "learning_rate": 5.506977565801096e-06,
            "epoch": 2.669581346051934,
            "step": 40300
        },
        {
            "loss": 0.3726,
            "grad_norm": 1.0810726881027222,
            "learning_rate": 5.48489666136725e-06,
            "epoch": 2.670906200317965,
            "step": 40320
        },
        {
            "loss": 0.3786,
            "grad_norm": 0.5760772824287415,
            "learning_rate": 5.4628157569334045e-06,
            "epoch": 2.672231054583996,
            "step": 40340
        },
        {
            "loss": 0.3726,
            "grad_norm": 0.726839542388916,
            "learning_rate": 5.440734852499559e-06,
            "epoch": 2.6735559088500267,
            "step": 40360
        },
        {
            "loss": 0.3861,
            "grad_norm": 1.402491807937622,
            "learning_rate": 5.418653948065713e-06,
            "epoch": 2.6748807631160574,
            "step": 40380
        },
        {
            "loss": 0.3815,
            "grad_norm": 1.8660634756088257,
            "learning_rate": 5.396573043631868e-06,
            "epoch": 2.676205617382088,
            "step": 40400
        },
        {
            "loss": 0.4198,
            "grad_norm": 1.18471360206604,
            "learning_rate": 5.374492139198021e-06,
            "epoch": 2.6775304716481187,
            "step": 40420
        },
        {
            "loss": 0.4355,
            "grad_norm": 0.4615648090839386,
            "learning_rate": 5.352411234764177e-06,
            "epoch": 2.6788553259141494,
            "step": 40440
        },
        {
            "loss": 0.3748,
            "grad_norm": 1.208048701286316,
            "learning_rate": 5.33033033033033e-06,
            "epoch": 2.68018018018018,
            "step": 40460
        },
        {
            "loss": 0.3601,
            "grad_norm": 1.7223775386810303,
            "learning_rate": 5.308249425896486e-06,
            "epoch": 2.681505034446211,
            "step": 40480
        },
        {
            "loss": 0.3274,
            "grad_norm": 0.6384931802749634,
            "learning_rate": 5.286168521462639e-06,
            "epoch": 2.6828298887122415,
            "step": 40500
        },
        {
            "loss": 0.4048,
            "grad_norm": 1.2668601274490356,
            "learning_rate": 5.264087617028794e-06,
            "epoch": 2.684154742978272,
            "step": 40520
        },
        {
            "loss": 0.353,
            "grad_norm": 0.5549191832542419,
            "learning_rate": 5.242006712594948e-06,
            "epoch": 2.685479597244303,
            "step": 40540
        },
        {
            "loss": 0.3779,
            "grad_norm": 0.7129034399986267,
            "learning_rate": 5.2199258081611025e-06,
            "epoch": 2.686804451510334,
            "step": 40560
        },
        {
            "loss": 0.3602,
            "grad_norm": 1.2983119487762451,
            "learning_rate": 5.197844903727257e-06,
            "epoch": 2.6881293057763647,
            "step": 40580
        },
        {
            "loss": 0.4596,
            "grad_norm": 1.0346438884735107,
            "learning_rate": 5.175763999293411e-06,
            "epoch": 2.6894541600423953,
            "step": 40600
        },
        {
            "loss": 0.4572,
            "grad_norm": 1.0091537237167358,
            "learning_rate": 5.153683094859566e-06,
            "epoch": 2.690779014308426,
            "step": 40620
        },
        {
            "loss": 0.3929,
            "grad_norm": 2.705207347869873,
            "learning_rate": 5.13160219042572e-06,
            "epoch": 2.6921038685744567,
            "step": 40640
        },
        {
            "loss": 0.4012,
            "grad_norm": 2.094625949859619,
            "learning_rate": 5.109521285991875e-06,
            "epoch": 2.6934287228404874,
            "step": 40660
        },
        {
            "loss": 0.3614,
            "grad_norm": 2.9157252311706543,
            "learning_rate": 5.087440381558029e-06,
            "epoch": 2.6947535771065185,
            "step": 40680
        },
        {
            "loss": 0.4075,
            "grad_norm": 10.23774242401123,
            "learning_rate": 5.065359477124184e-06,
            "epoch": 2.696078431372549,
            "step": 40700
        },
        {
            "loss": 0.4323,
            "grad_norm": 0.8719637393951416,
            "learning_rate": 5.043278572690337e-06,
            "epoch": 2.69740328563858,
            "step": 40720
        },
        {
            "loss": 0.4176,
            "grad_norm": 2.7956430912017822,
            "learning_rate": 5.0211976682564925e-06,
            "epoch": 2.6987281399046106,
            "step": 40740
        },
        {
            "loss": 0.3931,
            "grad_norm": 1.9534647464752197,
            "learning_rate": 4.999116763822646e-06,
            "epoch": 2.7000529941706413,
            "step": 40760
        },
        {
            "loss": 0.3369,
            "grad_norm": 1.0999222993850708,
            "learning_rate": 4.977035859388801e-06,
            "epoch": 2.701377848436672,
            "step": 40780
        },
        {
            "loss": 0.4265,
            "grad_norm": 4.028334617614746,
            "learning_rate": 4.954954954954955e-06,
            "epoch": 2.7027027027027026,
            "step": 40800
        },
        {
            "loss": 0.4249,
            "grad_norm": 3.449617385864258,
            "learning_rate": 4.932874050521109e-06,
            "epoch": 2.7040275569687333,
            "step": 40820
        },
        {
            "loss": 0.3728,
            "grad_norm": 1.273979902267456,
            "learning_rate": 4.910793146087264e-06,
            "epoch": 2.705352411234764,
            "step": 40840
        },
        {
            "loss": 0.4205,
            "grad_norm": 0.6600516438484192,
            "learning_rate": 4.888712241653418e-06,
            "epoch": 2.7066772655007947,
            "step": 40860
        },
        {
            "loss": 0.3825,
            "grad_norm": 1.4961909055709839,
            "learning_rate": 4.866631337219573e-06,
            "epoch": 2.708002119766826,
            "step": 40880
        },
        {
            "loss": 0.4066,
            "grad_norm": 1.3589781522750854,
            "learning_rate": 4.844550432785727e-06,
            "epoch": 2.7093269740328565,
            "step": 40900
        },
        {
            "loss": 0.348,
            "grad_norm": 2.9188051223754883,
            "learning_rate": 4.822469528351882e-06,
            "epoch": 2.710651828298887,
            "step": 40920
        },
        {
            "loss": 0.4586,
            "grad_norm": 2.010030508041382,
            "learning_rate": 4.800388623918036e-06,
            "epoch": 2.711976682564918,
            "step": 40940
        },
        {
            "loss": 0.3738,
            "grad_norm": 2.274294137954712,
            "learning_rate": 4.7783077194841905e-06,
            "epoch": 2.7133015368309485,
            "step": 40960
        },
        {
            "loss": 0.396,
            "grad_norm": 4.709593772888184,
            "learning_rate": 4.756226815050345e-06,
            "epoch": 2.7146263910969792,
            "step": 40980
        },
        {
            "loss": 0.4319,
            "grad_norm": 0.9927971959114075,
            "learning_rate": 4.734145910616499e-06,
            "epoch": 2.71595124536301,
            "step": 41000
        },
        {
            "loss": 0.3603,
            "grad_norm": 1.9678378105163574,
            "learning_rate": 4.712065006182653e-06,
            "epoch": 2.717276099629041,
            "step": 41020
        },
        {
            "loss": 0.3571,
            "grad_norm": 1.9072237014770508,
            "learning_rate": 4.689984101748808e-06,
            "epoch": 2.7186009538950717,
            "step": 41040
        },
        {
            "loss": 0.3577,
            "grad_norm": 0.5712074637413025,
            "learning_rate": 4.667903197314962e-06,
            "epoch": 2.7199258081611024,
            "step": 41060
        },
        {
            "loss": 0.3591,
            "grad_norm": 1.058832049369812,
            "learning_rate": 4.645822292881117e-06,
            "epoch": 2.721250662427133,
            "step": 41080
        },
        {
            "loss": 0.3802,
            "grad_norm": 1.946595311164856,
            "learning_rate": 4.623741388447271e-06,
            "epoch": 2.7225755166931638,
            "step": 41100
        },
        {
            "loss": 0.421,
            "grad_norm": 3.199375629425049,
            "learning_rate": 4.601660484013425e-06,
            "epoch": 2.7239003709591945,
            "step": 41120
        },
        {
            "loss": 0.407,
            "grad_norm": 1.1739226579666138,
            "learning_rate": 4.57957957957958e-06,
            "epoch": 2.725225225225225,
            "step": 41140
        },
        {
            "loss": 0.3327,
            "grad_norm": 0.9769628643989563,
            "learning_rate": 4.557498675145734e-06,
            "epoch": 2.726550079491256,
            "step": 41160
        },
        {
            "loss": 0.3815,
            "grad_norm": 1.1830321550369263,
            "learning_rate": 4.5354177707118885e-06,
            "epoch": 2.7278749337572865,
            "step": 41180
        },
        {
            "loss": 0.3935,
            "grad_norm": 1.0897899866104126,
            "learning_rate": 4.513336866278043e-06,
            "epoch": 2.729199788023317,
            "step": 41200
        },
        {
            "loss": 0.3937,
            "grad_norm": 0.5864170789718628,
            "learning_rate": 4.491255961844197e-06,
            "epoch": 2.7305246422893483,
            "step": 41220
        },
        {
            "loss": 0.3334,
            "grad_norm": 1.926561713218689,
            "learning_rate": 4.469175057410352e-06,
            "epoch": 2.731849496555379,
            "step": 41240
        },
        {
            "loss": 0.3354,
            "grad_norm": 0.9796844124794006,
            "learning_rate": 4.447094152976506e-06,
            "epoch": 2.7331743508214097,
            "step": 41260
        },
        {
            "loss": 0.3383,
            "grad_norm": 1.9427350759506226,
            "learning_rate": 4.425013248542661e-06,
            "epoch": 2.7344992050874404,
            "step": 41280
        },
        {
            "loss": 0.364,
            "grad_norm": 1.9517704248428345,
            "learning_rate": 4.402932344108815e-06,
            "epoch": 2.735824059353471,
            "step": 41300
        },
        {
            "loss": 0.3753,
            "grad_norm": 1.0416343212127686,
            "learning_rate": 4.380851439674969e-06,
            "epoch": 2.7371489136195017,
            "step": 41320
        },
        {
            "loss": 0.3669,
            "grad_norm": 1.8468612432479858,
            "learning_rate": 4.358770535241124e-06,
            "epoch": 2.7384737678855324,
            "step": 41340
        },
        {
            "loss": 0.3587,
            "grad_norm": 3.9203975200653076,
            "learning_rate": 4.336689630807278e-06,
            "epoch": 2.7397986221515636,
            "step": 41360
        },
        {
            "loss": 0.4406,
            "grad_norm": 2.849748373031616,
            "learning_rate": 4.314608726373433e-06,
            "epoch": 2.7411234764175942,
            "step": 41380
        },
        {
            "loss": 0.3904,
            "grad_norm": 1.8243842124938965,
            "learning_rate": 4.2925278219395865e-06,
            "epoch": 2.742448330683625,
            "step": 41400
        },
        {
            "loss": 0.3887,
            "grad_norm": 35.7227897644043,
            "learning_rate": 4.270446917505741e-06,
            "epoch": 2.7437731849496556,
            "step": 41420
        },
        {
            "loss": 0.3471,
            "grad_norm": 1.968557357788086,
            "learning_rate": 4.2483660130718954e-06,
            "epoch": 2.7450980392156863,
            "step": 41440
        },
        {
            "loss": 0.3858,
            "grad_norm": 0.7773336172103882,
            "learning_rate": 4.22628510863805e-06,
            "epoch": 2.746422893481717,
            "step": 41460
        },
        {
            "loss": 0.3749,
            "grad_norm": 1.3096425533294678,
            "learning_rate": 4.204204204204204e-06,
            "epoch": 2.7477477477477477,
            "step": 41480
        },
        {
            "loss": 0.3915,
            "grad_norm": 2.930051326751709,
            "learning_rate": 4.182123299770359e-06,
            "epoch": 2.7490726020137783,
            "step": 41500
        },
        {
            "loss": 0.4011,
            "grad_norm": 0.7192823886871338,
            "learning_rate": 4.160042395336513e-06,
            "epoch": 2.750397456279809,
            "step": 41520
        },
        {
            "loss": 0.3827,
            "grad_norm": 1.9961618185043335,
            "learning_rate": 4.137961490902668e-06,
            "epoch": 2.7517223105458397,
            "step": 41540
        },
        {
            "loss": 0.3869,
            "grad_norm": 1.2162283658981323,
            "learning_rate": 4.115880586468822e-06,
            "epoch": 2.753047164811871,
            "step": 41560
        },
        {
            "loss": 0.3631,
            "grad_norm": 3.5939862728118896,
            "learning_rate": 4.0937996820349765e-06,
            "epoch": 2.7543720190779015,
            "step": 41580
        },
        {
            "loss": 0.4016,
            "grad_norm": 1.2740458250045776,
            "learning_rate": 4.071718777601131e-06,
            "epoch": 2.755696873343932,
            "step": 41600
        },
        {
            "loss": 0.425,
            "grad_norm": 0.6661608219146729,
            "learning_rate": 4.0496378731672846e-06,
            "epoch": 2.757021727609963,
            "step": 41620
        },
        {
            "loss": 0.3462,
            "grad_norm": 1.5173455476760864,
            "learning_rate": 4.02755696873344e-06,
            "epoch": 2.7583465818759936,
            "step": 41640
        },
        {
            "loss": 0.4105,
            "grad_norm": 1.551439642906189,
            "learning_rate": 4.0054760642995934e-06,
            "epoch": 2.7596714361420243,
            "step": 41660
        },
        {
            "loss": 0.3216,
            "grad_norm": 0.9954062104225159,
            "learning_rate": 3.983395159865749e-06,
            "epoch": 2.7609962904080554,
            "step": 41680
        },
        {
            "loss": 0.3822,
            "grad_norm": 1.152620553970337,
            "learning_rate": 3.961314255431902e-06,
            "epoch": 2.762321144674086,
            "step": 41700
        },
        {
            "loss": 0.3277,
            "grad_norm": 2.139679431915283,
            "learning_rate": 3.939233350998057e-06,
            "epoch": 2.7636459989401168,
            "step": 41720
        },
        {
            "loss": 0.3902,
            "grad_norm": 1.2277251482009888,
            "learning_rate": 3.917152446564211e-06,
            "epoch": 2.7649708532061474,
            "step": 41740
        },
        {
            "loss": 0.3723,
            "grad_norm": 1.8101507425308228,
            "learning_rate": 3.895071542130366e-06,
            "epoch": 2.766295707472178,
            "step": 41760
        },
        {
            "loss": 0.3902,
            "grad_norm": 1.2831898927688599,
            "learning_rate": 3.872990637696521e-06,
            "epoch": 2.767620561738209,
            "step": 41780
        },
        {
            "loss": 0.3821,
            "grad_norm": 17.161855697631836,
            "learning_rate": 3.8509097332626745e-06,
            "epoch": 2.7689454160042395,
            "step": 41800
        },
        {
            "loss": 0.3701,
            "grad_norm": 2.085993528366089,
            "learning_rate": 3.828828828828829e-06,
            "epoch": 2.77027027027027,
            "step": 41820
        },
        {
            "loss": 0.3999,
            "grad_norm": 2.6557462215423584,
            "learning_rate": 3.8067479243949834e-06,
            "epoch": 2.771595124536301,
            "step": 41840
        },
        {
            "loss": 0.3588,
            "grad_norm": 0.9772506952285767,
            "learning_rate": 3.784667019961138e-06,
            "epoch": 2.7729199788023315,
            "step": 41860
        },
        {
            "loss": 0.3858,
            "grad_norm": 4.073451995849609,
            "learning_rate": 3.7625861155272923e-06,
            "epoch": 2.7742448330683622,
            "step": 41880
        },
        {
            "loss": 0.3291,
            "grad_norm": 0.8111209273338318,
            "learning_rate": 3.7405052110934468e-06,
            "epoch": 2.7755696873343934,
            "step": 41900
        },
        {
            "loss": 0.3759,
            "grad_norm": 1.2886569499969482,
            "learning_rate": 3.7184243066596008e-06,
            "epoch": 2.776894541600424,
            "step": 41920
        },
        {
            "loss": 0.3786,
            "grad_norm": 1.2678532600402832,
            "learning_rate": 3.6963434022257556e-06,
            "epoch": 2.7782193958664547,
            "step": 41940
        },
        {
            "loss": 0.3399,
            "grad_norm": 2.8726935386657715,
            "learning_rate": 3.6742624977919097e-06,
            "epoch": 2.7795442501324854,
            "step": 41960
        },
        {
            "loss": 0.3558,
            "grad_norm": 2.763259172439575,
            "learning_rate": 3.6521815933580645e-06,
            "epoch": 2.780869104398516,
            "step": 41980
        },
        {
            "loss": 0.3946,
            "grad_norm": 3.082836627960205,
            "learning_rate": 3.6301006889242185e-06,
            "epoch": 2.7821939586645468,
            "step": 42000
        },
        {
            "loss": 0.362,
            "grad_norm": 0.7212980389595032,
            "learning_rate": 3.6080197844903726e-06,
            "epoch": 2.783518812930578,
            "step": 42020
        },
        {
            "loss": 0.3628,
            "grad_norm": 1.9363744258880615,
            "learning_rate": 3.5859388800565274e-06,
            "epoch": 2.7848436671966086,
            "step": 42040
        },
        {
            "loss": 0.3587,
            "grad_norm": 2.0332627296447754,
            "learning_rate": 3.5638579756226815e-06,
            "epoch": 2.7861685214626393,
            "step": 42060
        },
        {
            "loss": 0.4266,
            "grad_norm": 1.8442485332489014,
            "learning_rate": 3.5417770711888363e-06,
            "epoch": 2.78749337572867,
            "step": 42080
        },
        {
            "loss": 0.3748,
            "grad_norm": 2.0015933513641357,
            "learning_rate": 3.5196961667549903e-06,
            "epoch": 2.7888182299947006,
            "step": 42100
        },
        {
            "loss": 0.4359,
            "grad_norm": 0.49522265791893005,
            "learning_rate": 3.4976152623211448e-06,
            "epoch": 2.7901430842607313,
            "step": 42120
        },
        {
            "loss": 0.3985,
            "grad_norm": 1.3359222412109375,
            "learning_rate": 3.4755343578872992e-06,
            "epoch": 2.791467938526762,
            "step": 42140
        },
        {
            "loss": 0.3983,
            "grad_norm": 4.666720390319824,
            "learning_rate": 3.4534534534534537e-06,
            "epoch": 2.7927927927927927,
            "step": 42160
        },
        {
            "loss": 0.3999,
            "grad_norm": 7.491530418395996,
            "learning_rate": 3.431372549019608e-06,
            "epoch": 2.7941176470588234,
            "step": 42180
        },
        {
            "loss": 0.3506,
            "grad_norm": 1.4892231225967407,
            "learning_rate": 3.4092916445857625e-06,
            "epoch": 2.795442501324854,
            "step": 42200
        },
        {
            "loss": 0.3443,
            "grad_norm": 1.2188493013381958,
            "learning_rate": 3.3872107401519166e-06,
            "epoch": 2.7967673555908847,
            "step": 42220
        },
        {
            "loss": 0.3893,
            "grad_norm": 2.7190475463867188,
            "learning_rate": 3.3651298357180714e-06,
            "epoch": 2.798092209856916,
            "step": 42240
        },
        {
            "loss": 0.3604,
            "grad_norm": 1.4495364427566528,
            "learning_rate": 3.3430489312842255e-06,
            "epoch": 2.7994170641229466,
            "step": 42260
        },
        {
            "loss": 0.4011,
            "grad_norm": 1.9660757780075073,
            "learning_rate": 3.3209680268503803e-06,
            "epoch": 2.8007419183889772,
            "step": 42280
        },
        {
            "loss": 0.3546,
            "grad_norm": 1.998426914215088,
            "learning_rate": 3.2988871224165343e-06,
            "epoch": 2.802066772655008,
            "step": 42300
        },
        {
            "loss": 0.4074,
            "grad_norm": 4.7439165115356445,
            "learning_rate": 3.2768062179826884e-06,
            "epoch": 2.8033916269210386,
            "step": 42320
        },
        {
            "loss": 0.3323,
            "grad_norm": 1.2122373580932617,
            "learning_rate": 3.2547253135488432e-06,
            "epoch": 2.8047164811870693,
            "step": 42340
        },
        {
            "loss": 0.3706,
            "grad_norm": 0.8932215571403503,
            "learning_rate": 3.2326444091149972e-06,
            "epoch": 2.8060413354531004,
            "step": 42360
        },
        {
            "loss": 0.3515,
            "grad_norm": 1.0290032625198364,
            "learning_rate": 3.210563504681152e-06,
            "epoch": 2.807366189719131,
            "step": 42380
        },
        {
            "loss": 0.3318,
            "grad_norm": 2.293790817260742,
            "learning_rate": 3.188482600247306e-06,
            "epoch": 2.808691043985162,
            "step": 42400
        },
        {
            "loss": 0.3908,
            "grad_norm": 2.9583120346069336,
            "learning_rate": 3.1664016958134606e-06,
            "epoch": 2.8100158982511925,
            "step": 42420
        },
        {
            "loss": 0.4182,
            "grad_norm": 0.657244086265564,
            "learning_rate": 3.144320791379615e-06,
            "epoch": 2.811340752517223,
            "step": 42440
        },
        {
            "loss": 0.4422,
            "grad_norm": 2.2423391342163086,
            "learning_rate": 3.1222398869457695e-06,
            "epoch": 2.812665606783254,
            "step": 42460
        },
        {
            "loss": 0.3336,
            "grad_norm": 2.3157007694244385,
            "learning_rate": 3.100158982511924e-06,
            "epoch": 2.8139904610492845,
            "step": 42480
        },
        {
            "loss": 0.378,
            "grad_norm": 3.210068464279175,
            "learning_rate": 3.0780780780780783e-06,
            "epoch": 2.815315315315315,
            "step": 42500
        },
        {
            "loss": 0.3043,
            "grad_norm": 0.4970707595348358,
            "learning_rate": 3.0559971736442328e-06,
            "epoch": 2.816640169581346,
            "step": 42520
        },
        {
            "loss": 0.3392,
            "grad_norm": 5.7149834632873535,
            "learning_rate": 3.0339162692103872e-06,
            "epoch": 2.8179650238473766,
            "step": 42540
        },
        {
            "loss": 0.3705,
            "grad_norm": 3.8283233642578125,
            "learning_rate": 3.0118353647765412e-06,
            "epoch": 2.8192898781134073,
            "step": 42560
        },
        {
            "loss": 0.3734,
            "grad_norm": 3.0149919986724854,
            "learning_rate": 2.9897544603426957e-06,
            "epoch": 2.8206147323794384,
            "step": 42580
        },
        {
            "loss": 0.4102,
            "grad_norm": 4.806857109069824,
            "learning_rate": 2.96767355590885e-06,
            "epoch": 2.821939586645469,
            "step": 42600
        },
        {
            "loss": 0.3818,
            "grad_norm": 4.119080543518066,
            "learning_rate": 2.9455926514750046e-06,
            "epoch": 2.8232644409114998,
            "step": 42620
        },
        {
            "loss": 0.3801,
            "grad_norm": 1.0167375802993774,
            "learning_rate": 2.923511747041159e-06,
            "epoch": 2.8245892951775304,
            "step": 42640
        },
        {
            "loss": 0.3382,
            "grad_norm": 3.1354265213012695,
            "learning_rate": 2.901430842607313e-06,
            "epoch": 2.825914149443561,
            "step": 42660
        },
        {
            "loss": 0.3871,
            "grad_norm": 14.837688446044922,
            "learning_rate": 2.8793499381734675e-06,
            "epoch": 2.827239003709592,
            "step": 42680
        },
        {
            "loss": 0.3625,
            "grad_norm": 2.305422067642212,
            "learning_rate": 2.857269033739622e-06,
            "epoch": 2.828563857975623,
            "step": 42700
        },
        {
            "loss": 0.3524,
            "grad_norm": 1.2290880680084229,
            "learning_rate": 2.8351881293057768e-06,
            "epoch": 2.8298887122416536,
            "step": 42720
        },
        {
            "loss": 0.3442,
            "grad_norm": 1.2366328239440918,
            "learning_rate": 2.8131072248719312e-06,
            "epoch": 2.8312135665076843,
            "step": 42740
        },
        {
            "loss": 0.3294,
            "grad_norm": 0.5174928903579712,
            "learning_rate": 2.7910263204380852e-06,
            "epoch": 2.832538420773715,
            "step": 42760
        },
        {
            "loss": 0.3969,
            "grad_norm": 1.0613377094268799,
            "learning_rate": 2.7689454160042397e-06,
            "epoch": 2.8338632750397457,
            "step": 42780
        },
        {
            "loss": 0.3251,
            "grad_norm": 2.1731107234954834,
            "learning_rate": 2.746864511570394e-06,
            "epoch": 2.8351881293057764,
            "step": 42800
        },
        {
            "loss": 0.3857,
            "grad_norm": 3.7288239002227783,
            "learning_rate": 2.7247836071365486e-06,
            "epoch": 2.836512983571807,
            "step": 42820
        },
        {
            "loss": 0.4128,
            "grad_norm": 0.458107054233551,
            "learning_rate": 2.702702702702703e-06,
            "epoch": 2.8378378378378377,
            "step": 42840
        },
        {
            "loss": 0.37,
            "grad_norm": 2.8344669342041016,
            "learning_rate": 2.680621798268857e-06,
            "epoch": 2.8391626921038684,
            "step": 42860
        },
        {
            "loss": 0.3907,
            "grad_norm": 1.8264992237091064,
            "learning_rate": 2.6585408938350115e-06,
            "epoch": 2.840487546369899,
            "step": 42880
        },
        {
            "loss": 0.3733,
            "grad_norm": 3.993715286254883,
            "learning_rate": 2.636459989401166e-06,
            "epoch": 2.84181240063593,
            "step": 42900
        },
        {
            "loss": 0.5072,
            "grad_norm": 1.9679975509643555,
            "learning_rate": 2.6143790849673204e-06,
            "epoch": 2.843137254901961,
            "step": 42920
        },
        {
            "loss": 0.4001,
            "grad_norm": 1.3408544063568115,
            "learning_rate": 2.592298180533475e-06,
            "epoch": 2.8444621091679916,
            "step": 42940
        },
        {
            "loss": 0.3401,
            "grad_norm": 1.8731385469436646,
            "learning_rate": 2.570217276099629e-06,
            "epoch": 2.8457869634340223,
            "step": 42960
        },
        {
            "loss": 0.3227,
            "grad_norm": 3.976047992706299,
            "learning_rate": 2.5481363716657837e-06,
            "epoch": 2.847111817700053,
            "step": 42980
        },
        {
            "loss": 0.3645,
            "grad_norm": 1.3529345989227295,
            "learning_rate": 2.526055467231938e-06,
            "epoch": 2.8484366719660836,
            "step": 43000
        },
        {
            "loss": 0.3472,
            "grad_norm": 2.0686442852020264,
            "learning_rate": 2.5039745627980926e-06,
            "epoch": 2.8497615262321143,
            "step": 43020
        },
        {
            "loss": 0.4158,
            "grad_norm": 1.8696268796920776,
            "learning_rate": 2.481893658364247e-06,
            "epoch": 2.8510863804981454,
            "step": 43040
        },
        {
            "loss": 0.3892,
            "grad_norm": 2.1594185829162598,
            "learning_rate": 2.459812753930401e-06,
            "epoch": 2.852411234764176,
            "step": 43060
        },
        {
            "loss": 0.3259,
            "grad_norm": 2.3544745445251465,
            "learning_rate": 2.4377318494965555e-06,
            "epoch": 2.853736089030207,
            "step": 43080
        },
        {
            "loss": 0.3362,
            "grad_norm": 1.7801754474639893,
            "learning_rate": 2.41565094506271e-06,
            "epoch": 2.8550609432962375,
            "step": 43100
        },
        {
            "loss": 0.3661,
            "grad_norm": 1.1275193691253662,
            "learning_rate": 2.3935700406288644e-06,
            "epoch": 2.856385797562268,
            "step": 43120
        },
        {
            "loss": 0.3515,
            "grad_norm": 0.6136958599090576,
            "learning_rate": 2.371489136195019e-06,
            "epoch": 2.857710651828299,
            "step": 43140
        },
        {
            "loss": 0.4432,
            "grad_norm": 2.125840187072754,
            "learning_rate": 2.349408231761173e-06,
            "epoch": 2.8590355060943295,
            "step": 43160
        },
        {
            "loss": 0.3695,
            "grad_norm": 0.9599829316139221,
            "learning_rate": 2.3273273273273273e-06,
            "epoch": 2.8603603603603602,
            "step": 43180
        },
        {
            "loss": 0.3812,
            "grad_norm": 4.831051349639893,
            "learning_rate": 2.3052464228934817e-06,
            "epoch": 2.861685214626391,
            "step": 43200
        },
        {
            "loss": 0.3953,
            "grad_norm": 2.809694528579712,
            "learning_rate": 2.283165518459636e-06,
            "epoch": 2.8630100688924216,
            "step": 43220
        },
        {
            "loss": 0.3577,
            "grad_norm": 3.864722490310669,
            "learning_rate": 2.2610846140257906e-06,
            "epoch": 2.8643349231584527,
            "step": 43240
        },
        {
            "loss": 0.4257,
            "grad_norm": 1.480186939239502,
            "learning_rate": 2.239003709591945e-06,
            "epoch": 2.8656597774244834,
            "step": 43260
        },
        {
            "loss": 0.4187,
            "grad_norm": 2.980008602142334,
            "learning_rate": 2.2169228051580995e-06,
            "epoch": 2.866984631690514,
            "step": 43280
        },
        {
            "loss": 0.3757,
            "grad_norm": 1.0281250476837158,
            "learning_rate": 2.194841900724254e-06,
            "epoch": 2.868309485956545,
            "step": 43300
        },
        {
            "loss": 0.4105,
            "grad_norm": 1.1330595016479492,
            "learning_rate": 2.1727609962904084e-06,
            "epoch": 2.8696343402225755,
            "step": 43320
        },
        {
            "loss": 0.3729,
            "grad_norm": 3.3422281742095947,
            "learning_rate": 2.150680091856563e-06,
            "epoch": 2.870959194488606,
            "step": 43340
        },
        {
            "loss": 0.3721,
            "grad_norm": 0.9817187190055847,
            "learning_rate": 2.128599187422717e-06,
            "epoch": 2.872284048754637,
            "step": 43360
        },
        {
            "loss": 0.476,
            "grad_norm": 4.874119758605957,
            "learning_rate": 2.1065182829888713e-06,
            "epoch": 2.873608903020668,
            "step": 43380
        },
        {
            "loss": 0.3615,
            "grad_norm": 1.9615799188613892,
            "learning_rate": 2.0844373785550257e-06,
            "epoch": 2.8749337572866986,
            "step": 43400
        },
        {
            "loss": 0.392,
            "grad_norm": 0.8929204344749451,
            "learning_rate": 2.06235647412118e-06,
            "epoch": 2.8762586115527293,
            "step": 43420
        },
        {
            "loss": 0.3589,
            "grad_norm": 2.7858009338378906,
            "learning_rate": 2.0402755696873346e-06,
            "epoch": 2.87758346581876,
            "step": 43440
        },
        {
            "loss": 0.4435,
            "grad_norm": 2.739051103591919,
            "learning_rate": 2.0181946652534886e-06,
            "epoch": 2.8789083200847907,
            "step": 43460
        },
        {
            "loss": 0.3922,
            "grad_norm": 1.3784279823303223,
            "learning_rate": 1.996113760819643e-06,
            "epoch": 2.8802331743508214,
            "step": 43480
        },
        {
            "loss": 0.3148,
            "grad_norm": 1.1460152864456177,
            "learning_rate": 1.9740328563857975e-06,
            "epoch": 2.881558028616852,
            "step": 43500
        },
        {
            "loss": 0.4087,
            "grad_norm": 2.898685932159424,
            "learning_rate": 1.951951951951952e-06,
            "epoch": 2.8828828828828827,
            "step": 43520
        },
        {
            "loss": 0.3874,
            "grad_norm": 1.4911075830459595,
            "learning_rate": 1.9298710475181064e-06,
            "epoch": 2.8842077371489134,
            "step": 43540
        },
        {
            "loss": 0.3973,
            "grad_norm": 2.281660795211792,
            "learning_rate": 1.907790143084261e-06,
            "epoch": 2.885532591414944,
            "step": 43560
        },
        {
            "loss": 0.375,
            "grad_norm": 0.6335055828094482,
            "learning_rate": 1.885709238650415e-06,
            "epoch": 2.8868574456809752,
            "step": 43580
        },
        {
            "loss": 0.3994,
            "grad_norm": 0.9884740710258484,
            "learning_rate": 1.8636283342165695e-06,
            "epoch": 2.888182299947006,
            "step": 43600
        },
        {
            "loss": 0.3889,
            "grad_norm": 1.2596330642700195,
            "learning_rate": 1.8415474297827242e-06,
            "epoch": 2.8895071542130366,
            "step": 43620
        },
        {
            "loss": 0.4014,
            "grad_norm": 1.643917441368103,
            "learning_rate": 1.8194665253488786e-06,
            "epoch": 2.8908320084790673,
            "step": 43640
        },
        {
            "loss": 0.3495,
            "grad_norm": 1.8657933473587036,
            "learning_rate": 1.7973856209150326e-06,
            "epoch": 2.892156862745098,
            "step": 43660
        },
        {
            "loss": 0.4216,
            "grad_norm": 2.363007068634033,
            "learning_rate": 1.775304716481187e-06,
            "epoch": 2.8934817170111287,
            "step": 43680
        },
        {
            "loss": 0.4049,
            "grad_norm": 1.2674461603164673,
            "learning_rate": 1.7532238120473415e-06,
            "epoch": 2.8948065712771593,
            "step": 43700
        },
        {
            "loss": 0.351,
            "grad_norm": 3.747089147567749,
            "learning_rate": 1.731142907613496e-06,
            "epoch": 2.8961314255431905,
            "step": 43720
        },
        {
            "loss": 0.3499,
            "grad_norm": 3.086442470550537,
            "learning_rate": 1.7090620031796504e-06,
            "epoch": 2.897456279809221,
            "step": 43740
        },
        {
            "loss": 0.3755,
            "grad_norm": 2.0735251903533936,
            "learning_rate": 1.6869810987458046e-06,
            "epoch": 2.898781134075252,
            "step": 43760
        },
        {
            "loss": 0.3866,
            "grad_norm": 0.9582091569900513,
            "learning_rate": 1.664900194311959e-06,
            "epoch": 2.9001059883412825,
            "step": 43780
        },
        {
            "loss": 0.3917,
            "grad_norm": 3.1640472412109375,
            "learning_rate": 1.6428192898781135e-06,
            "epoch": 2.901430842607313,
            "step": 43800
        },
        {
            "loss": 0.3457,
            "grad_norm": 1.053274393081665,
            "learning_rate": 1.620738385444268e-06,
            "epoch": 2.902755696873344,
            "step": 43820
        },
        {
            "loss": 0.3125,
            "grad_norm": 1.0652824640274048,
            "learning_rate": 1.5986574810104224e-06,
            "epoch": 2.9040805511393746,
            "step": 43840
        },
        {
            "loss": 0.3649,
            "grad_norm": 1.5808684825897217,
            "learning_rate": 1.5765765765765764e-06,
            "epoch": 2.9054054054054053,
            "step": 43860
        },
        {
            "loss": 0.3948,
            "grad_norm": 1.066170334815979,
            "learning_rate": 1.554495672142731e-06,
            "epoch": 2.906730259671436,
            "step": 43880
        },
        {
            "loss": 0.4218,
            "grad_norm": 2.2320637702941895,
            "learning_rate": 1.5324147677088855e-06,
            "epoch": 2.9080551139374666,
            "step": 43900
        },
        {
            "loss": 0.3756,
            "grad_norm": 0.6598910093307495,
            "learning_rate": 1.51033386327504e-06,
            "epoch": 2.9093799682034978,
            "step": 43920
        },
        {
            "loss": 0.4022,
            "grad_norm": 1.2177131175994873,
            "learning_rate": 1.4882529588411942e-06,
            "epoch": 2.9107048224695284,
            "step": 43940
        },
        {
            "loss": 0.3592,
            "grad_norm": 1.9606492519378662,
            "learning_rate": 1.4661720544073486e-06,
            "epoch": 2.912029676735559,
            "step": 43960
        },
        {
            "loss": 0.3516,
            "grad_norm": 1.1909635066986084,
            "learning_rate": 1.4440911499735028e-06,
            "epoch": 2.91335453100159,
            "step": 43980
        },
        {
            "loss": 0.4167,
            "grad_norm": 3.2592031955718994,
            "learning_rate": 1.4220102455396573e-06,
            "epoch": 2.9146793852676205,
            "step": 44000
        },
        {
            "loss": 0.3797,
            "grad_norm": 1.249097228050232,
            "learning_rate": 1.3999293411058117e-06,
            "epoch": 2.916004239533651,
            "step": 44020
        },
        {
            "loss": 0.3973,
            "grad_norm": 1.2037765979766846,
            "learning_rate": 1.3778484366719662e-06,
            "epoch": 2.9173290937996823,
            "step": 44040
        },
        {
            "loss": 0.3939,
            "grad_norm": 1.4213382005691528,
            "learning_rate": 1.3557675322381206e-06,
            "epoch": 2.918653948065713,
            "step": 44060
        },
        {
            "loss": 0.334,
            "grad_norm": 2.856400728225708,
            "learning_rate": 1.3336866278042748e-06,
            "epoch": 2.9199788023317437,
            "step": 44080
        },
        {
            "loss": 0.3729,
            "grad_norm": 1.2362940311431885,
            "learning_rate": 1.3116057233704293e-06,
            "epoch": 2.9213036565977744,
            "step": 44100
        },
        {
            "loss": 0.3581,
            "grad_norm": 1.2471853494644165,
            "learning_rate": 1.2895248189365837e-06,
            "epoch": 2.922628510863805,
            "step": 44120
        },
        {
            "loss": 0.5111,
            "grad_norm": 0.7231181859970093,
            "learning_rate": 1.267443914502738e-06,
            "epoch": 2.9239533651298357,
            "step": 44140
        },
        {
            "loss": 0.3405,
            "grad_norm": 1.094079613685608,
            "learning_rate": 1.2453630100688924e-06,
            "epoch": 2.9252782193958664,
            "step": 44160
        },
        {
            "loss": 0.3572,
            "grad_norm": 1.3571120500564575,
            "learning_rate": 1.2232821056350469e-06,
            "epoch": 2.926603073661897,
            "step": 44180
        },
        {
            "loss": 0.3487,
            "grad_norm": 2.9521310329437256,
            "learning_rate": 1.2012012012012013e-06,
            "epoch": 2.9279279279279278,
            "step": 44200
        },
        {
            "loss": 0.3513,
            "grad_norm": 0.465667724609375,
            "learning_rate": 1.1791202967673557e-06,
            "epoch": 2.9292527821939585,
            "step": 44220
        },
        {
            "loss": 0.3746,
            "grad_norm": 1.329465627670288,
            "learning_rate": 1.15703939233351e-06,
            "epoch": 2.930577636459989,
            "step": 44240
        },
        {
            "loss": 0.3695,
            "grad_norm": 2.2841029167175293,
            "learning_rate": 1.1349584878996644e-06,
            "epoch": 2.9319024907260203,
            "step": 44260
        },
        {
            "loss": 0.3832,
            "grad_norm": 1.9214545488357544,
            "learning_rate": 1.1128775834658186e-06,
            "epoch": 2.933227344992051,
            "step": 44280
        },
        {
            "loss": 0.4279,
            "grad_norm": 1.085825800895691,
            "learning_rate": 1.0907966790319733e-06,
            "epoch": 2.9345521992580816,
            "step": 44300
        },
        {
            "loss": 0.3172,
            "grad_norm": 0.7733929753303528,
            "learning_rate": 1.0687157745981277e-06,
            "epoch": 2.9358770535241123,
            "step": 44320
        },
        {
            "loss": 0.3769,
            "grad_norm": 1.1765055656433105,
            "learning_rate": 1.046634870164282e-06,
            "epoch": 2.937201907790143,
            "step": 44340
        },
        {
            "loss": 0.4356,
            "grad_norm": 0.49501559138298035,
            "learning_rate": 1.0245539657304364e-06,
            "epoch": 2.9385267620561737,
            "step": 44360
        },
        {
            "loss": 0.4059,
            "grad_norm": 2.9722578525543213,
            "learning_rate": 1.0024730612965906e-06,
            "epoch": 2.939851616322205,
            "step": 44380
        },
        {
            "loss": 0.3363,
            "grad_norm": 4.498284339904785,
            "learning_rate": 9.80392156862745e-07,
            "epoch": 2.9411764705882355,
            "step": 44400
        },
        {
            "loss": 0.4068,
            "grad_norm": 1.378943681716919,
            "learning_rate": 9.583112524288995e-07,
            "epoch": 2.942501324854266,
            "step": 44420
        },
        {
            "loss": 0.3463,
            "grad_norm": 1.1669083833694458,
            "learning_rate": 9.362303479950539e-07,
            "epoch": 2.943826179120297,
            "step": 44440
        },
        {
            "loss": 0.379,
            "grad_norm": 1.9452967643737793,
            "learning_rate": 9.141494435612083e-07,
            "epoch": 2.9451510333863276,
            "step": 44460
        },
        {
            "loss": 0.3944,
            "grad_norm": 1.9406373500823975,
            "learning_rate": 8.920685391273626e-07,
            "epoch": 2.9464758876523582,
            "step": 44480
        },
        {
            "loss": 0.4004,
            "grad_norm": 2.2143752574920654,
            "learning_rate": 8.699876346935171e-07,
            "epoch": 2.947800741918389,
            "step": 44500
        },
        {
            "loss": 0.3314,
            "grad_norm": 3.8273727893829346,
            "learning_rate": 8.479067302596715e-07,
            "epoch": 2.9491255961844196,
            "step": 44520
        },
        {
            "loss": 0.3409,
            "grad_norm": 0.7526511549949646,
            "learning_rate": 8.258258258258259e-07,
            "epoch": 2.9504504504504503,
            "step": 44540
        },
        {
            "loss": 0.4058,
            "grad_norm": 0.9339808225631714,
            "learning_rate": 8.037449213919803e-07,
            "epoch": 2.951775304716481,
            "step": 44560
        },
        {
            "loss": 0.373,
            "grad_norm": 1.8852612972259521,
            "learning_rate": 7.816640169581345e-07,
            "epoch": 2.9531001589825117,
            "step": 44580
        },
        {
            "loss": 0.3922,
            "grad_norm": 0.7375102639198303,
            "learning_rate": 7.59583112524289e-07,
            "epoch": 2.954425013248543,
            "step": 44600
        },
        {
            "loss": 0.332,
            "grad_norm": 1.268333911895752,
            "learning_rate": 7.375022080904434e-07,
            "epoch": 2.9557498675145735,
            "step": 44620
        },
        {
            "loss": 0.4553,
            "grad_norm": 0.9631320834159851,
            "learning_rate": 7.154213036565979e-07,
            "epoch": 2.957074721780604,
            "step": 44640
        },
        {
            "loss": 0.3484,
            "grad_norm": 1.4414571523666382,
            "learning_rate": 6.933403992227522e-07,
            "epoch": 2.958399576046635,
            "step": 44660
        },
        {
            "loss": 0.3242,
            "grad_norm": 1.238742709159851,
            "learning_rate": 6.712594947889065e-07,
            "epoch": 2.9597244303126655,
            "step": 44680
        },
        {
            "loss": 0.4161,
            "grad_norm": 1.1676734685897827,
            "learning_rate": 6.49178590355061e-07,
            "epoch": 2.961049284578696,
            "step": 44700
        },
        {
            "loss": 0.3775,
            "grad_norm": 0.6471456289291382,
            "learning_rate": 6.270976859212153e-07,
            "epoch": 2.9623741388447273,
            "step": 44720
        },
        {
            "loss": 0.433,
            "grad_norm": 1.2260096073150635,
            "learning_rate": 6.050167814873698e-07,
            "epoch": 2.963698993110758,
            "step": 44740
        },
        {
            "loss": 0.3549,
            "grad_norm": 1.9549072980880737,
            "learning_rate": 5.829358770535242e-07,
            "epoch": 2.9650238473767887,
            "step": 44760
        },
        {
            "loss": 0.4155,
            "grad_norm": 1.371275544166565,
            "learning_rate": 5.608549726196785e-07,
            "epoch": 2.9663487016428194,
            "step": 44780
        },
        {
            "loss": 0.343,
            "grad_norm": 1.018923282623291,
            "learning_rate": 5.387740681858329e-07,
            "epoch": 2.96767355590885,
            "step": 44800
        },
        {
            "loss": 0.3142,
            "grad_norm": 2.7180395126342773,
            "learning_rate": 5.166931637519873e-07,
            "epoch": 2.9689984101748808,
            "step": 44820
        },
        {
            "loss": 0.395,
            "grad_norm": 1.1268885135650635,
            "learning_rate": 4.946122593181418e-07,
            "epoch": 2.9703232644409114,
            "step": 44840
        },
        {
            "loss": 0.3291,
            "grad_norm": 1.1670295000076294,
            "learning_rate": 4.725313548842961e-07,
            "epoch": 2.971648118706942,
            "step": 44860
        },
        {
            "loss": 0.3488,
            "grad_norm": 3.408799648284912,
            "learning_rate": 4.504504504504505e-07,
            "epoch": 2.972972972972973,
            "step": 44880
        },
        {
            "loss": 0.4022,
            "grad_norm": 4.435642719268799,
            "learning_rate": 4.283695460166048e-07,
            "epoch": 2.9742978272390035,
            "step": 44900
        },
        {
            "loss": 0.377,
            "grad_norm": 1.139129638671875,
            "learning_rate": 4.062886415827592e-07,
            "epoch": 2.975622681505034,
            "step": 44920
        },
        {
            "loss": 0.3359,
            "grad_norm": 0.9286513328552246,
            "learning_rate": 3.842077371489136e-07,
            "epoch": 2.9769475357710653,
            "step": 44940
        },
        {
            "loss": 0.3695,
            "grad_norm": 2.154202699661255,
            "learning_rate": 3.6212683271506804e-07,
            "epoch": 2.978272390037096,
            "step": 44960
        },
        {
            "loss": 0.3648,
            "grad_norm": 0.6443266868591309,
            "learning_rate": 3.4004592828122243e-07,
            "epoch": 2.9795972443031267,
            "step": 44980
        },
        {
            "loss": 0.4309,
            "grad_norm": 1.2813917398452759,
            "learning_rate": 3.179650238473768e-07,
            "epoch": 2.9809220985691574,
            "step": 45000
        },
        {
            "loss": 0.3669,
            "grad_norm": 1.5018306970596313,
            "learning_rate": 2.958841194135312e-07,
            "epoch": 2.982246952835188,
            "step": 45020
        },
        {
            "loss": 0.3759,
            "grad_norm": 1.0130729675292969,
            "learning_rate": 2.7380321497968555e-07,
            "epoch": 2.9835718071012187,
            "step": 45040
        },
        {
            "loss": 0.4134,
            "grad_norm": 3.776482105255127,
            "learning_rate": 2.5172231054584e-07,
            "epoch": 2.98489666136725,
            "step": 45060
        },
        {
            "loss": 0.3605,
            "grad_norm": 3.9848451614379883,
            "learning_rate": 2.2964140611199435e-07,
            "epoch": 2.9862215156332805,
            "step": 45080
        },
        {
            "loss": 0.3736,
            "grad_norm": 2.0179874897003174,
            "learning_rate": 2.0756050167814877e-07,
            "epoch": 2.987546369899311,
            "step": 45100
        },
        {
            "loss": 0.3665,
            "grad_norm": 1.2613329887390137,
            "learning_rate": 1.8547959724430313e-07,
            "epoch": 2.988871224165342,
            "step": 45120
        },
        {
            "loss": 0.4129,
            "grad_norm": 3.1850461959838867,
            "learning_rate": 1.6339869281045752e-07,
            "epoch": 2.9901960784313726,
            "step": 45140
        },
        {
            "loss": 0.3529,
            "grad_norm": 0.9851053357124329,
            "learning_rate": 1.413177883766119e-07,
            "epoch": 2.9915209326974033,
            "step": 45160
        },
        {
            "loss": 0.4412,
            "grad_norm": 3.004941463470459,
            "learning_rate": 1.192368839427663e-07,
            "epoch": 2.992845786963434,
            "step": 45180
        },
        {
            "loss": 0.3838,
            "grad_norm": 1.1882305145263672,
            "learning_rate": 9.715597950892069e-08,
            "epoch": 2.9941706412294646,
            "step": 45200
        },
        {
            "loss": 0.3605,
            "grad_norm": 3.128603935241699,
            "learning_rate": 7.507507507507508e-08,
            "epoch": 2.9954954954954953,
            "step": 45220
        },
        {
            "loss": 0.4219,
            "grad_norm": 1.9832041263580322,
            "learning_rate": 5.299417064122947e-08,
            "epoch": 2.996820349761526,
            "step": 45240
        },
        {
            "loss": 0.4735,
            "grad_norm": 2.283942461013794,
            "learning_rate": 3.091326620738386e-08,
            "epoch": 2.998145204027557,
            "step": 45260
        },
        {
            "loss": 0.3803,
            "grad_norm": 2.943786859512329,
            "learning_rate": 8.832361773538245e-09,
            "epoch": 2.999470058293588,
            "step": 45280
        },
        {
            "eval_loss": 0.38576897978782654,
            "eval_runtime": 111.5104,
            "eval_samples_per_second": 270.755,
            "eval_steps_per_second": 33.844,
            "epoch": 3.0,
            "step": 45288
        },
        {
            "train_runtime": 5924.0071,
            "train_samples_per_second": 61.158,
            "train_steps_per_second": 7.645,
            "total_flos": 460294913617920.0,
            "train_loss": 0.3955863413088862,
            "epoch": 3.0,
            "step": 45288
        }
    ]
}